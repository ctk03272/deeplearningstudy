{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMmXePAZ1H6HfYoH1qGK5fL",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ctk03272/deeplearningstudy/blob/main/chapter5.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Chapter5\n",
        "- 수치 미분은 단순하고 구현도 쉽지만 계산이 오래 걸린다는 게 단점이다. 가중치 매개변수의 기울기를 효율적으로 계산하는 '오차역전파법'을 배워 본다.\n",
        "- 오차역전파법은 수식과 그래프 두가지로 이해할 수 있다.\n",
        "\n",
        "## 5.1 계산 그래프\n",
        "- 계산과정을 그래프로 나타낸 것\n",
        "\n",
        "### 5.1.1 계산 그래프로 풀다.\n",
        "- 계산을 왼쪽에서 오른쪽으로 진행하는 단계를 순전파\n",
        "- 오른쪽에서 왼쪽으로 전파가 역전파\n",
        "\n",
        "### 5.1.2 국소적 계산\n",
        "- 계산그래프의 특징은 국소적 계산을 전파함으로 최종 결과를 얻는다는 것이다.\n",
        "- 전체 계산이 아무리 복잡해져도 각 단계에서 하는 일은 해당 노드의 국소적 계산이다.\n",
        "\n",
        "### 5.1.3 왜 계산 그래프로 푸는가?\n",
        "- 실제 계산그래프를 사용하는 가장 큰 이유는 역전파를 통해 미분을 효율적으로 계산 할 수 있다\n",
        "- 계산 그래프의 이점은 순전파와 역전파를 활용해서 각 변수의 미분을 효율적으로 구할 수 있다.\n",
        "\n",
        "## 5.2. 연쇄법칙\n",
        "- 국소적 미분을 전달하는 원리는 연쇄법칙에 따른 것이다.\n",
        "\n",
        "### 5.2.1 계산 그래프의 역전파\n",
        "- 신호의 노드에 국소적 미분을 곱한 후 다음 노드로 전달하는 것이다.\n",
        "\n",
        "### 5.3.1 덧셈 노드의 역전파\n",
        "- 덧셈 노드의 역전파는 입력 값을 그대로 흘려보낸다.\n",
        "\n",
        "### 5.3.2 곱셈 노드의 역전파\n",
        "- 곱셈 노드 역전파는 상류의 값에 순전파 떄의 입력 신호들을 서로 바꾼 값을 곱해서 하류로 보낸다.\n",
        "- 따라서 곱셈의 역전파는 순방향 입력 신호의 값이 필요하다. 그래서 곱셉 노드를 구현할 떄는 순전파의 입력 신호를 변수에 저장해 둔다.\n",
        "\n",
        "## 5.4 단순한 계층 구현하기\n",
        "\n",
        "### 5.4.1 곱셈 계층\n",
        "- 모든 계층은 forward()와 backward()라는 공통 메서드를 갖도록 한다.\n"
      ],
      "metadata": {
        "id": "nPAt5ox7s6n8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MulLayer:\n",
        "    def __init__(self):\n",
        "        self.x = None\n",
        "        self.y = None\n",
        "\n",
        "    def forward(self, x, y):\n",
        "        self.x = x\n",
        "        self.y = y\n",
        "        out = x * y\n",
        "\n",
        "        return out\n",
        "\n",
        "    def backward(self, dout):\n",
        "        dx = dout * self.y  # x와 y를 바꾼다.\n",
        "        dy = dout * self.x\n",
        "\n",
        "        return dx, dy\n",
        "\n",
        "\n",
        "class AddLayer:\n",
        "    def __init__(self):\n",
        "        pass\n",
        "\n",
        "    def forward(self, x, y):\n",
        "        out = x + y\n",
        "\n",
        "        return out\n",
        "\n",
        "    def backward(self, dout):\n",
        "        dx = dout * 1\n",
        "        dy = dout * 1\n",
        "\n",
        "        return dx, dy"
      ],
      "metadata": {
        "id": "7LtOb5q6yf8N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "\n",
        "apple = 100\n",
        "apple_num = 2\n",
        "tax = 1.1\n",
        "\n",
        "mul_apple_layer = MulLayer()\n",
        "mul_tax_layer = MulLayer()\n",
        "\n",
        "# forward\n",
        "apple_price = mul_apple_layer.forward(apple, apple_num)\n",
        "price = mul_tax_layer.forward(apple_price, tax)\n",
        "\n",
        "# backward\n",
        "dprice = 1\n",
        "dapple_price, dtax = mul_tax_layer.backward(dprice)\n",
        "dapple, dapple_num = mul_apple_layer.backward(dapple_price)\n",
        "\n",
        "print(\"price:\", int(price))\n",
        "print(\"dApple:\", dapple)\n",
        "print(\"dApple_num:\", int(dapple_num))\n",
        "print(\"dTax:\", dtax)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YvAGRkARy2Hc",
        "outputId": "4dddc9b5-c762-4154-d1b4-e16f67a0213e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "price: 220\n",
            "dApple: 2.2\n",
            "dApple_num: 110\n",
            "dTax: 200\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "apple = 100\n",
        "apple_num = 2\n",
        "orange = 150\n",
        "orange_num = 3\n",
        "tax = 1.1\n",
        "\n",
        "mul_apple_layer = MulLayer()\n",
        "mul_orange_layer = MulLayer()\n",
        "add_apple_orange_layer = AddLayer()\n",
        "mul_tax_layer = MulLayer()\n",
        "\n",
        "apple_price = mul_apple_layer.forward(apple, apple_num)\n",
        "orange_price = mul_orange_layer.forward(orange, orange_num)\n",
        "all_price = add_apple_orange_layer.forward(apple_price, orange_price)\n",
        "price = mul_tax_layer.forward(all_price, tax)\n",
        "\n",
        "dprice = 1\n",
        "dall_price, dtax = mul_tax_layer.backward(dprice)\n",
        "dapple_price, dorange_price = add_apple_orange_layer.backward(dall_price)\n",
        "dorange, dorange_num = mul_orange_layer.backward(dorange_price)\n",
        "dapple, dapple_num = mul_apple_layer.backward(dapple_price)\n",
        "\n",
        "print(\"price:\", int(price))\n",
        "print(\"dApple:\", dapple)\n",
        "print(\"dApple_num:\", int(dapple_num))\n",
        "print(\"dOrange:\", dorange)\n",
        "print(\"dOrange_num:\", int(dorange_num))\n",
        "print(\"dTax:\", dtax)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GZRuhBKxzJgU",
        "outputId": "28ece3b6-d4b9-4066-d7e1-a2fd5bf23c9f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "price: 715\n",
            "dApple: 2.2\n",
            "dApple_num: 110\n",
            "dOrange: 3.3000000000000003\n",
            "dOrange_num: 165\n",
            "dTax: 650\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 5.5 활성화 함수 계층 구현하기\n",
        "\n",
        "## 5.5.1 ReLU 계층\n",
        "- 0보다 클때는 선형 함수 0보다 작을떄는 0인 함수이다\n",
        "- ReLU 계층은 전기회로의 스위치에 비유할 수 있다."
      ],
      "metadata": {
        "id": "9N6kiqjDzXyX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "class Relu:\n",
        "    def __init__(self):\n",
        "        self.mask = None\n",
        "\n",
        "    def forward(self, x):\n",
        "        self.mask = (x <= 0)\n",
        "        out = x.copy()\n",
        "        out[self.mask] = 0\n",
        "\n",
        "        return out\n",
        "\n",
        "    def backward(self, dout):\n",
        "        dout[self.mask] = 0\n",
        "        dx = dout\n",
        "\n",
        "        return dx"
      ],
      "metadata": {
        "id": "GVRZX1lu15TD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5.5.2 Sigmoid 계층\n",
        "- 시그모이드 함수를 사용한다."
      ],
      "metadata": {
        "id": "ruenN35r2PSf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Sigmoid:\n",
        "    def __init__(self):\n",
        "        self.out = None\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = sigmoid(x)\n",
        "        self.out = out\n",
        "        return out\n",
        "\n",
        "    def backward(self, dout):\n",
        "        dx = dout * (1.0 - self.out) * self.out\n",
        "\n",
        "        return dx"
      ],
      "metadata": {
        "id": "XY0VIgmy2kCF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5.6 Affine/Softmax 계측 구현하기\n",
        "\n",
        "### 5.6.1 Affine 계층\n",
        "- 신경파의 순전파 때 수행하는 행렬의 곱은 기하학에서는 어파인변환 이라고 한다. 그래서 이 책에서는 어파인 변환을 수행하는 처리를 Affine 계층 이라는 이름으로 구현한다."
      ],
      "metadata": {
        "id": "56ntcbBZ27De"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "class Affine:\n",
        "    def __init__(self, W, b):\n",
        "        self.W = W\n",
        "        self.b = b\n",
        "\n",
        "        self.x = None\n",
        "        self.original_x_shape = None\n",
        "        self.dW = None\n",
        "        self.db = None\n",
        "\n",
        "    def forward(self, x):\n",
        "        self.original_x_shape = x.shape\n",
        "        x = x.reshape(x.shape[0], -1)\n",
        "        self.x = x\n",
        "\n",
        "        out = np.dot(self.x, self.W) + self.b\n",
        "\n",
        "        return out\n",
        "\n",
        "    def backward(self, dout):\n",
        "        dx = np.dot(dout, self.W.T)\n",
        "        self.dW = np.dot(self.x.T, dout)\n",
        "        self.db = np.sum(dout, axis=0)\n",
        "\n",
        "        dx = dx.reshape(*self.original_x_shape)\n",
        "        return dx"
      ],
      "metadata": {
        "id": "wl3VWv_d3puA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5.6.3 Softmax-with-Loss 계층\n",
        "- 소프트맥스 함수는 입력 값을 정규화하여 출력\n",
        "- 신경망에서 수행하는 작업은 학습과 추론 두가지이다. 앞장에서 본거같이 추론시에는 일반적으로 SoftMax 계층을 사용하지 않는다."
      ],
      "metadata": {
        "id": "AkN1c83D3wv8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SoftmaxWithLoss:\n",
        "    def __init__(self):\n",
        "        self.loss = None\n",
        "        self.y = None\n",
        "        self.t = None\n",
        "\n",
        "    def forward(self, x, t):\n",
        "        self.t = t\n",
        "        self.y = softmax(x)\n",
        "        self.loss = cross_entropy_error(self.y, self.t)\n",
        "\n",
        "        return self.loss\n",
        "\n",
        "    def backward(self, dout=1):\n",
        "        batch_size = self.t.shape[0]\n",
        "        if self.t.size == self.y.size:\n",
        "            dx = (self.y - self.t) / batch_size\n",
        "        else:\n",
        "            dx = self.y.copy()\n",
        "            dx[np.arange(batch_size), self.t] -= 1\n",
        "            dx = dx / batch_size\n",
        "\n",
        "        return dx"
      ],
      "metadata": {
        "id": "laoUPW-36KS4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 5.7 오차역전파법 *구현하기*"
      ],
      "metadata": {
        "id": "j9nSpR3B6PUM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# coding: utf-8\n",
        "import sys, os\n",
        "sys.path.append(os.pardir)  # 부모 디렉터리의 파일을 가져올 수 있도록 설정\n",
        "import numpy as np\n",
        "from common.layers import *\n",
        "from common.gradient import numerical_gradient\n",
        "from collections import OrderedDict\n",
        "\n",
        "\n",
        "class TwoLayerNet:\n",
        "\n",
        "    def __init__(self, input_size, hidden_size, output_size, weight_init_std = 0.01):\n",
        "        # 가중치 초기화\n",
        "        self.params = {}\n",
        "        self.params['W1'] = weight_init_std * np.random.randn(input_size, hidden_size)\n",
        "        self.params['b1'] = np.zeros(hidden_size)\n",
        "        self.params['W2'] = weight_init_std * np.random.randn(hidden_size, output_size)\n",
        "        self.params['b2'] = np.zeros(output_size)\n",
        "\n",
        "        # 계층 생성\n",
        "        self.layers = OrderedDict()\n",
        "        self.layers['Affine1'] = Affine(self.params['W1'], self.params['b1'])\n",
        "        self.layers['Relu1'] = Relu()\n",
        "        self.layers['Affine2'] = Affine(self.params['W2'], self.params['b2'])\n",
        "\n",
        "        self.lastLayer = SoftmaxWithLoss()\n",
        "\n",
        "    def predict(self, x):\n",
        "        for layer in self.layers.values():\n",
        "            x = layer.forward(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "    # x : 입력 데이터, t : 정답 레이블\n",
        "    def loss(self, x, t):\n",
        "        y = self.predict(x)\n",
        "        return self.lastLayer.forward(y, t)\n",
        "\n",
        "    def accuracy(self, x, t):\n",
        "        y = self.predict(x)\n",
        "        y = np.argmax(y, axis=1)\n",
        "        if t.ndim != 1 : t = np.argmax(t, axis=1)\n",
        "\n",
        "        accuracy = np.sum(y == t) / float(x.shape[0])\n",
        "        return accuracy\n",
        "\n",
        "    # x : 입력 데이터, t : 정답 레이블\n",
        "    def numerical_gradient(self, x, t):\n",
        "        loss_W = lambda W: self.loss(x, t)\n",
        "\n",
        "        grads = {}\n",
        "        grads['W1'] = numerical_gradient(loss_W, self.params['W1'])\n",
        "        grads['b1'] = numerical_gradient(loss_W, self.params['b1'])\n",
        "        grads['W2'] = numerical_gradient(loss_W, self.params['W2'])\n",
        "        grads['b2'] = numerical_gradient(loss_W, self.params['b2'])\n",
        "\n",
        "        return grads\n",
        "\n",
        "    def gradient(self, x, t):\n",
        "        # forward\n",
        "        self.loss(x, t)\n",
        "\n",
        "        # backward\n",
        "        dout = 1\n",
        "        dout = self.lastLayer.backward(dout)\n",
        "\n",
        "        layers = list(self.layers.values())\n",
        "        layers.reverse()\n",
        "        for layer in layers:\n",
        "            dout = layer.backward(dout)\n",
        "\n",
        "        # 결과 저장\n",
        "        grads = {}\n",
        "        grads['W1'], grads['b1'] = self.layers['Affine1'].dW, self.layers['Affine1'].db\n",
        "        grads['W2'], grads['b2'] = self.layers['Affine2'].dW, self.layers['Affine2'].db\n",
        "\n",
        "        return grads"
      ],
      "metadata": {
        "id": "ptfI4hznJ1PU"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# coding: utf-8\n",
        "import sys, os\n",
        "\n",
        "\n",
        "import numpy as np\n",
        "from mnist import load_mnist\n",
        "from layers import *\n",
        "from gradient import numerical_gradient\n",
        "\n",
        "(x_train, t_train), (x_test, t_test) = load_mnist(normalize=True, one_hot_label=True)\n",
        "\n",
        "network = TwoLayerNet(input_size=784, hidden_size=50, output_size=10)\n",
        "\n",
        "iters_num = 10000\n",
        "train_size = x_train.shape[0]\n",
        "batch_size = 100\n",
        "learning_rate = 0.1\n",
        "\n",
        "train_loss_list = []\n",
        "train_acc_list = []\n",
        "test_acc_list = []\n",
        "\n",
        "iter_per_epoch = max(train_size / batch_size, 1)\n",
        "\n",
        "for i in range(iters_num):\n",
        "    batch_mask = np.random.choice(train_size, batch_size)\n",
        "    x_batch = x_train[batch_mask]\n",
        "    t_batch = t_train[batch_mask]\n",
        "\n",
        "    # 기울기 계산\n",
        "    #grad = network.numerical_gradient(x_batch, t_batch) # 수치 미분 방식\n",
        "    grad = network.gradient(x_batch, t_batch) # 오차역전파법 방식(훨씬 빠르다)\n",
        "\n",
        "    # 갱신\n",
        "    for key in ('W1', 'b1', 'W2', 'b2'):\n",
        "        network.params[key] -= learning_rate * grad[key]\n",
        "\n",
        "    loss = network.loss(x_batch, t_batch)\n",
        "    train_loss_list.append(loss)\n",
        "\n",
        "    if i % iter_per_epoch == 0:\n",
        "        train_acc = network.accuracy(x_train, t_train)\n",
        "        test_acc = network.accuracy(x_test, t_test)\n",
        "        train_acc_list.append(train_acc)\n",
        "        test_acc_list.append(test_acc)\n",
        "        print(train_acc, test_acc)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Oa8WtEK06Wcz",
        "outputId": "7d2657cd-45f1-4c8e-bc87-d89cf95fe420"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.10911666666666667 0.1119\n",
            "0.9057166666666666 0.9093\n",
            "0.91795 0.9206\n",
            "0.9308166666666666 0.9325\n",
            "0.94205 0.942\n",
            "0.95065 0.9498\n",
            "0.9561666666666667 0.9537\n",
            "0.9613166666666667 0.9584\n",
            "0.9642166666666667 0.9595\n",
            "0.9670166666666666 0.9627\n",
            "0.9684333333333334 0.9625\n",
            "0.9705166666666667 0.9639\n",
            "0.97195 0.9664\n",
            "0.9747166666666667 0.9664\n",
            "0.9763333333333334 0.9681\n",
            "0.9774833333333334 0.9704\n",
            "0.978 0.9707\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# coding: utf-8\n",
        "import sys, os\n",
        "sys.path.append(os.pardir)  # 부모 디렉터리의 파일을 가져올 수 있도록 설정\n",
        "import numpy as np\n",
        "from mnist import load_mnist\n",
        "\n",
        "# 데이터 읽기\n",
        "(x_train, t_train), (x_test, t_test) = load_mnist(normalize=True, one_hot_label=True)\n",
        "\n",
        "network = TwoLayerNet(input_size=784, hidden_size=50, output_size=10)\n",
        "\n",
        "x_batch = x_train[:3]\n",
        "t_batch = t_train[:3]\n",
        "\n",
        "grad_numerical = network.numerical_gradient(x_batch, t_batch)\n",
        "grad_backprop = network.gradient(x_batch, t_batch)\n",
        "\n",
        "# 각 가중치의 절대 오차의 평균을 구한다.\n",
        "for key in grad_numerical.keys():\n",
        "    diff = np.average( np.abs(grad_backprop[key] - grad_numerical[key]) )\n",
        "    print(key + \":\" + str(diff))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2JMyv2_fNm53",
        "outputId": "5cbadbc0-146f-46e2-a702-84d2fc671f34"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "W1:4.277259140813999e-10\n",
            "b1:2.342659291295181e-09\n",
            "W2:5.713298156757819e-09\n",
            "b2:1.3990672419750271e-07\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# coding: utf-8\n",
        "import sys, os\n",
        "sys.path.append(os.pardir)\n",
        "\n",
        "import numpy as np\n",
        "from mnist import load_mnist\n",
        "\n",
        "# 데이터 읽기\n",
        "(x_train, t_train), (x_test, t_test) = load_mnist(normalize=True, one_hot_label=True)\n",
        "\n",
        "network = TwoLayerNet(input_size=784, hidden_size=50, output_size=10)\n",
        "\n",
        "iters_num = 10000\n",
        "train_size = x_train.shape[0]\n",
        "batch_size = 100\n",
        "learning_rate = 0.1\n",
        "\n",
        "train_loss_list = []\n",
        "train_acc_list = []\n",
        "test_acc_list = []\n",
        "\n",
        "iter_per_epoch = max(train_size / batch_size, 1)\n",
        "\n",
        "for i in range(iters_num):\n",
        "    batch_mask = np.random.choice(train_size, batch_size)\n",
        "    x_batch = x_train[batch_mask]\n",
        "    t_batch = t_train[batch_mask]\n",
        "\n",
        "    # 기울기 계산\n",
        "    #grad = network.numerical_gradient(x_batch, t_batch) # 수치 미분 방식\n",
        "    grad = network.gradient(x_batch, t_batch) # 오차역전파법 방식(훨씬 빠르다)\n",
        "\n",
        "    # 갱신\n",
        "    for key in ('W1', 'b1', 'W2', 'b2'):\n",
        "        network.params[key] -= learning_rate * grad[key]\n",
        "\n",
        "    loss = network.loss(x_batch, t_batch)\n",
        "    train_loss_list.append(loss)\n",
        "\n",
        "    if i % iter_per_epoch == 0:\n",
        "        train_acc = network.accuracy(x_train, t_train)\n",
        "        test_acc = network.accuracy(x_test, t_test)\n",
        "        train_acc_list.append(train_acc)\n",
        "        test_acc_list.append(test_acc)\n",
        "        print(train_acc, test_acc)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QPctsIJvNw-I",
        "outputId": "a25e6e51-76ad-4084-8d1a-332dd79d9c37"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.14445 0.1464\n",
            "0.9036666666666666 0.9079\n",
            "0.9239166666666667 0.9273\n",
            "0.9366666666666666 0.9389\n",
            "0.9468333333333333 0.9465\n",
            "0.95185 0.9513\n",
            "0.95575 0.9531\n",
            "0.9594666666666667 0.9563\n",
            "0.9635833333333333 0.9588\n",
            "0.9670333333333333 0.9617\n",
            "0.9664833333333334 0.9621\n",
            "0.9711666666666666 0.9637\n",
            "0.97325 0.9648\n",
            "0.9752833333333333 0.9674\n",
            "0.9748 0.9661\n",
            "0.9765166666666667 0.9685\n",
            "0.97865 0.97\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "PLzmD-RA7-yO"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}