{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOlKUXlz+XtczpMG0oOVoYt",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ctk03272/deeplearningstudy/blob/main/Chapter7.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VIpRXIoeRiW_"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 7. 합성곱 신경망\n",
        "- 이번장의 주제는 합성만 신경곱(CNN) -> 이미지 인식과 음성 인식 등 다양한 곳에서 사용\n",
        "\n",
        "## 7.1 전체 구조\n",
        "- CNN도 레고 블록처럼 계층을 조합하여 사용\n",
        "- 합성곱 계층과 롤링 계층이 새롭게 등장\n",
        "- 지금까지의 신경망은 모든 뉴런이 결합되어 있었고 이를 완전연결이라고 하고, 완전히 연결된 계층을 Affine 계층이라고 했음\n",
        "- CNN의 경우 Conv - ReLU - (Pooling) 흐름으로 연결됨 (풀링 계층은 생략 가능)\n",
        "- 출력에 가까운 층에서는 Affine - ReLU 구성 사용 가능하며 마지막 출력에서는 Affine - Softmax 조합을 그대로 사용\n",
        "\n",
        "## 7.2 합성곱 계층\n",
        "- CNN에서는 패딩 스트라이등 고유 용어가 등장\n",
        "\n",
        "### 7.2.1 완전연결 계층의 문제점\n",
        "- 완전연결 계층의 문제점은 데이터의 형상이 무시된다는 점이다\n",
        "- 예를들어 이미지는 3차원 형상이며, 형상에 공간적 정보가 담겨 있다. 그러나 완전연결 계층은 형상을 무시하고 모든 입력 데이터를 동등한 뉴런으로 취급하여 형상에 담긴 정보를 살릴 수 없음\n",
        "- 합성곱 계층은 형상을 유지한다. 이미지 데이터를 3차원으로 입력받으며, 다음 계층에도 3차원으로 데이터를 전달\n",
        "- 추ㅜ에서는 입력 데이터를 입력 특징 맵, 출력 데이터를 출력 특징 맵이라고 한다.\n",
        "\n",
        "\n",
        "### 7.2.2 합성곱 연산\n",
        "- 합성곱 연산은 이미지 처리에서 말하는 필터연산\n",
        "- 문헌에 따라 필터를 커널이라 칭하기도 한다\n",
        "- 합성곱 연산은 필터의 윈도를 일정 간격으로 이동해가며 입력 데이터에 적용한다\n",
        "- 완전연결 신경망에는 가중치 매개변수와 편향이 존재하는데, CNN에서는 필터의 매개변수가 그동안의 가중치에 해당하며, 편향도 존재한다\n",
        "\n",
        "### 7.2.3 패딩\n",
        "- 합성곱 연산 수행 전 입력 데이터 주변을 특정 값으로 채우는 것을 패딩이라고 한다.\n",
        "- 패딩은 주로 출력 크기를 조정할 목적으로 사요오딤\n",
        "\n",
        "### 7.2.4 스트라이드\n",
        "- 필터를 적용하는 위치의 간격을 스트라이드\n",
        "- 스트라이드를 키우면 출력이 작아지고, 패딩을 크게 하면 출력 크기가 커진다\n",
        "- 출력 크기는 정수여야 한다\n",
        "\n",
        "### 7.2.5 3차원 데이터의 합성곱 연산\n",
        "- 입력에티어의 채널 수와 필터의 채널 수가 같아야 한다.\n",
        "\n",
        "### 7.2.6 블록으로 생각하기\n",
        "- 합성곱 연산의 출력으로 다수의 채널을 내보내려면, 필터를 다수 사용하면 된다.\n",
        "\n",
        "### 7.2.7 배치 처리\n",
        "- 합성곱 연산도 마찬가지로 배치 처리를 지원합니다.\n",
        "\n",
        "\n",
        "## 7.3 풀링 계층\n",
        "- 가로 세로 방향의 공간을 줄이는 연산\n",
        "- 최대 풀링 평균 풀링 등이 있음\n",
        "\n",
        "### 7.3.1 풀링 계층의 특징\n",
        "- 학습해야 할 매개변수가 없다\n",
        "- 채널 수가 변하지 않는다.\n",
        "- 입력의 변화에 영향을 적게 받는다\n",
        "\n",
        "## 7.4 합성곱/풀링 계층 구현하기\n",
        "### 7.4.1 4차원 배열\n"
      ],
      "metadata": {
        "id": "o_BVkBxeSEH5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "x = np.random.rand(10,1,28,28)\n",
        "x.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZtZF_9ktijbq",
        "outputId": "10cc8710-ff70-4096-b5c9-52d1af3b5717"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(10, 1, 28, 28)"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x[0].shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nmxl5mUcixkf",
        "outputId": "a93450f4-638d-4590-f0a4-edf066c07e50"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1, 28, 28)"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x[1].shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yoCCn5mgiy8k",
        "outputId": "2abc2dad-584b-4612-e729-fdc767a9c000"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1, 28, 28)"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x[0][0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mp4VAN1ri0AJ",
        "outputId": "d517d7af-d07a-40d4-a04a-adc493f99ca9"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[4.52110216e-01, 7.53486298e-01, 9.11470862e-01, 9.29518695e-01,\n",
              "        7.15554807e-02, 1.16063371e-01, 7.39244197e-01, 2.52493765e-01,\n",
              "        1.58951385e-01, 3.69587362e-01, 8.89370368e-01, 6.54740681e-01,\n",
              "        3.87621650e-03, 1.73923480e-01, 3.83655521e-01, 9.06880238e-01,\n",
              "        5.45442876e-02, 3.74258799e-01, 9.56134065e-01, 3.78407116e-01,\n",
              "        5.67078618e-01, 9.65050230e-01, 8.34473770e-01, 5.75703495e-01,\n",
              "        7.43637451e-01, 2.32321314e-01, 4.13432747e-01, 5.50914134e-01],\n",
              "       [6.62761838e-01, 6.33313567e-01, 1.34455202e-01, 9.51410223e-01,\n",
              "        4.59248711e-01, 7.78226405e-01, 8.30329922e-01, 8.89985892e-01,\n",
              "        8.53397417e-01, 2.81345394e-02, 2.63225203e-01, 6.06065484e-01,\n",
              "        5.88757458e-01, 5.87750678e-01, 7.04108741e-01, 6.57536203e-01,\n",
              "        3.27474928e-01, 7.46982564e-01, 7.03773145e-01, 1.13970156e-01,\n",
              "        3.32033339e-01, 5.80205458e-01, 6.86404923e-01, 1.24787914e-01,\n",
              "        1.47486386e-02, 4.24814425e-01, 8.06953277e-01, 8.03104376e-01],\n",
              "       [3.94502521e-01, 6.14312748e-01, 9.95708859e-01, 7.98646502e-01,\n",
              "        6.74759498e-01, 9.42042849e-02, 1.87205179e-01, 6.02524554e-01,\n",
              "        2.56164772e-02, 1.74962330e-01, 7.02195165e-01, 7.44952064e-01,\n",
              "        7.48427563e-01, 8.40745625e-01, 8.30990828e-01, 3.85807267e-03,\n",
              "        9.12137517e-01, 8.04348488e-01, 6.17317504e-01, 9.59374664e-01,\n",
              "        8.14313349e-01, 2.32026645e-01, 3.90467381e-01, 6.20818610e-01,\n",
              "        6.28056342e-01, 2.08162416e-01, 1.01687975e-01, 4.22674470e-01],\n",
              "       [5.17446928e-01, 9.36685678e-02, 1.27210939e-01, 6.42736723e-02,\n",
              "        6.35787807e-02, 6.43119244e-01, 4.36413407e-01, 3.46226894e-01,\n",
              "        5.29699126e-01, 7.97779326e-03, 7.04525269e-01, 7.71637672e-01,\n",
              "        4.84138732e-01, 2.35995940e-01, 2.36139752e-02, 2.12529721e-01,\n",
              "        7.34163141e-01, 7.22541687e-01, 1.05961957e-01, 9.67252915e-01,\n",
              "        7.14146658e-01, 4.62040244e-01, 8.23108651e-01, 2.43358471e-01,\n",
              "        8.47818994e-03, 3.51984148e-01, 6.61512971e-01, 9.50132421e-01],\n",
              "       [9.52635897e-01, 5.89166601e-02, 6.91443227e-01, 6.39751143e-01,\n",
              "        8.71826325e-01, 4.26832967e-01, 3.43147140e-02, 9.54571504e-01,\n",
              "        9.40784325e-01, 8.03205788e-01, 9.48690277e-01, 9.81475477e-01,\n",
              "        9.46619496e-01, 1.51523354e-01, 3.60122736e-01, 9.10249806e-01,\n",
              "        8.07342750e-01, 9.38549367e-01, 6.91324276e-01, 2.84406462e-01,\n",
              "        9.48179825e-02, 6.92558998e-01, 6.71443086e-01, 7.74460550e-01,\n",
              "        5.37520530e-02, 3.05950803e-01, 2.33154712e-01, 6.78875290e-01],\n",
              "       [6.04894071e-01, 5.72828029e-01, 7.33748907e-01, 1.11840180e-01,\n",
              "        1.91974240e-01, 3.02186089e-01, 8.97011913e-01, 7.22239753e-01,\n",
              "        1.62650127e-01, 6.09510686e-01, 8.64362154e-04, 7.23844504e-01,\n",
              "        6.98870886e-01, 6.02510007e-01, 6.16523482e-01, 9.89017664e-01,\n",
              "        8.24253323e-01, 7.22866198e-01, 9.33255789e-01, 7.31456700e-01,\n",
              "        6.92955505e-01, 1.63414046e-01, 9.03128075e-01, 7.59286706e-01,\n",
              "        7.18437038e-01, 7.81380198e-01, 3.83815581e-01, 3.52224746e-01],\n",
              "       [3.02842262e-01, 8.53296483e-01, 1.62774833e-01, 1.31633736e-01,\n",
              "        9.92809965e-01, 5.29860742e-01, 3.68335984e-01, 4.54573593e-02,\n",
              "        4.36957745e-02, 3.63407597e-01, 5.02514932e-01, 2.49055943e-01,\n",
              "        3.56799724e-01, 3.86412505e-01, 6.66839155e-01, 3.85946066e-01,\n",
              "        3.21240485e-02, 2.60615878e-01, 9.20907366e-01, 6.42691974e-01,\n",
              "        8.51020571e-01, 5.26692575e-01, 4.86629530e-01, 9.06477154e-01,\n",
              "        6.19263541e-01, 8.71429930e-02, 3.15468689e-01, 5.63304972e-01],\n",
              "       [4.49856368e-01, 7.75241271e-01, 3.81966393e-01, 1.63688408e-01,\n",
              "        9.12584438e-01, 3.77902988e-01, 1.00264554e-01, 5.50688047e-02,\n",
              "        7.92584494e-01, 2.64206059e-02, 6.68984183e-02, 9.81473200e-01,\n",
              "        4.22523482e-01, 1.51683748e-01, 9.66260662e-01, 9.41073239e-01,\n",
              "        1.47437700e-01, 7.27532135e-01, 9.11952482e-02, 7.68974896e-02,\n",
              "        6.29191746e-01, 7.63331168e-01, 8.72780878e-01, 1.47641187e-01,\n",
              "        1.89135084e-01, 9.04775456e-01, 2.31630118e-01, 2.23922784e-01],\n",
              "       [2.37732986e-01, 7.21703732e-01, 6.06911947e-01, 9.51331845e-01,\n",
              "        7.38471937e-03, 6.56449138e-01, 5.34428186e-01, 4.06130185e-01,\n",
              "        5.92981888e-01, 2.08774057e-01, 9.10300522e-01, 3.11341339e-02,\n",
              "        6.69486231e-02, 7.55026680e-01, 1.19871005e-01, 6.00403625e-01,\n",
              "        4.96187361e-02, 4.85959849e-01, 9.85304461e-01, 5.72999485e-01,\n",
              "        8.62341307e-01, 5.63671190e-01, 8.23427923e-01, 8.40865428e-01,\n",
              "        6.73410427e-01, 4.06773661e-01, 9.21281822e-01, 7.87824593e-01],\n",
              "       [6.04706383e-01, 7.45334449e-01, 6.00090104e-01, 8.74650542e-01,\n",
              "        5.57190197e-01, 7.58771985e-01, 6.64240223e-01, 7.53602142e-01,\n",
              "        8.40580492e-03, 6.90198677e-01, 6.32962786e-01, 1.17824567e-01,\n",
              "        7.62921206e-01, 5.71183323e-01, 3.51381327e-01, 6.96668230e-01,\n",
              "        5.15844117e-01, 8.45380512e-01, 5.23431262e-01, 9.44427605e-01,\n",
              "        8.04871568e-01, 2.20982904e-01, 1.66385291e-01, 3.93327181e-01,\n",
              "        1.94554709e-01, 3.09589357e-01, 6.33909621e-01, 7.12803798e-01],\n",
              "       [6.17850806e-01, 7.89130926e-01, 9.89224876e-01, 7.75703511e-01,\n",
              "        3.06552052e-01, 1.85534862e-01, 3.61598842e-01, 5.32353619e-02,\n",
              "        2.89041865e-01, 5.07634305e-01, 5.78913550e-01, 5.04896008e-01,\n",
              "        7.60178108e-01, 6.31581140e-03, 4.33610827e-02, 1.89680993e-01,\n",
              "        3.16895953e-01, 6.42074252e-01, 9.82047106e-01, 1.61606260e-01,\n",
              "        8.52022519e-01, 6.45339097e-02, 6.13869072e-01, 3.97597864e-01,\n",
              "        4.78583427e-01, 4.92643797e-01, 3.55603714e-01, 2.88945994e-01],\n",
              "       [9.17471872e-01, 2.76218764e-01, 7.26285892e-01, 2.44859533e-01,\n",
              "        4.84506074e-01, 7.67104750e-01, 1.07313782e-01, 3.73799778e-02,\n",
              "        5.35601955e-01, 2.17940585e-02, 1.08186928e-01, 4.27236503e-01,\n",
              "        5.20761736e-01, 6.15051793e-01, 5.37416015e-01, 2.57095054e-02,\n",
              "        3.39854049e-01, 9.15245475e-01, 8.34282524e-01, 9.20812993e-03,\n",
              "        6.86356028e-01, 5.81159466e-01, 2.59614893e-01, 4.96083177e-01,\n",
              "        2.11082425e-01, 7.19995452e-01, 2.15710672e-01, 7.90716473e-01],\n",
              "       [6.38027797e-01, 6.63672202e-01, 3.00010785e-01, 7.71715786e-02,\n",
              "        7.86746899e-01, 1.21386874e-01, 8.85116889e-01, 7.16164036e-01,\n",
              "        2.03022450e-01, 5.95962037e-01, 8.64091806e-01, 7.47907598e-01,\n",
              "        2.18702449e-03, 9.33813212e-01, 2.09927175e-01, 4.35903444e-01,\n",
              "        6.47038139e-01, 4.67125043e-01, 6.47128331e-01, 3.59788499e-01,\n",
              "        7.23004318e-01, 3.12408825e-01, 5.99217547e-02, 1.31838327e-01,\n",
              "        6.97506165e-01, 7.04700571e-01, 5.73583807e-01, 1.18869912e-01],\n",
              "       [2.49882240e-01, 5.18219495e-01, 5.37645679e-01, 1.30720175e-01,\n",
              "        3.01669458e-01, 9.54529829e-01, 7.86514914e-01, 6.07102030e-01,\n",
              "        7.98557730e-01, 7.90016178e-01, 6.76148829e-02, 6.75502713e-01,\n",
              "        3.36322303e-01, 7.89484416e-01, 4.47495604e-01, 3.62286556e-01,\n",
              "        6.83062474e-01, 8.94045674e-01, 7.24178367e-01, 9.96868457e-01,\n",
              "        3.76257360e-01, 5.52332017e-01, 4.81736555e-01, 9.53147710e-01,\n",
              "        1.78862459e-01, 3.47510120e-01, 3.85837584e-01, 7.45030419e-01],\n",
              "       [3.42898699e-01, 4.76461214e-02, 3.25020536e-01, 2.63393441e-01,\n",
              "        4.69065599e-01, 4.93092698e-01, 2.15573484e-02, 4.68457434e-01,\n",
              "        6.88597919e-01, 9.79323431e-02, 9.05652222e-01, 3.35788594e-01,\n",
              "        2.70952773e-01, 4.72246218e-01, 6.04612826e-01, 5.98439245e-01,\n",
              "        6.92173068e-01, 6.75699431e-01, 2.06117100e-01, 3.68975612e-01,\n",
              "        7.00617507e-01, 4.61668033e-01, 8.96091604e-01, 5.66244181e-01,\n",
              "        1.66195858e-02, 9.10561980e-01, 6.66489379e-02, 6.01730495e-01],\n",
              "       [5.65876972e-01, 3.41148716e-02, 2.88555852e-01, 9.95387615e-01,\n",
              "        4.40180030e-01, 4.08773749e-01, 7.07815129e-01, 9.15823399e-01,\n",
              "        9.40909416e-01, 9.26697877e-01, 1.76118204e-01, 7.38605505e-01,\n",
              "        9.42068539e-01, 5.28079466e-01, 2.11994564e-01, 2.97445383e-01,\n",
              "        7.12401934e-01, 5.60727842e-01, 4.58775008e-03, 4.88040775e-01,\n",
              "        2.77650448e-01, 5.98170095e-01, 1.58145343e-02, 9.25007578e-01,\n",
              "        2.50745207e-02, 8.03606072e-01, 1.42664283e-01, 7.98584035e-01],\n",
              "       [6.04530413e-01, 7.91640476e-01, 1.17263016e-01, 5.01921303e-01,\n",
              "        6.16628862e-01, 4.08191793e-01, 4.99254792e-01, 8.29311162e-01,\n",
              "        3.65350403e-01, 4.75118074e-01, 9.80469984e-02, 1.08405622e-01,\n",
              "        8.72209426e-01, 2.05157384e-01, 6.10024372e-01, 2.05437354e-01,\n",
              "        1.02750009e-01, 5.64939307e-01, 1.29165101e-01, 8.66347583e-01,\n",
              "        5.79367417e-01, 4.39201956e-01, 9.73362342e-01, 9.53642471e-01,\n",
              "        4.61491391e-01, 7.98485292e-01, 7.63386211e-01, 3.81255952e-01],\n",
              "       [4.47869552e-01, 1.42617508e-01, 2.05889020e-01, 1.12118557e-01,\n",
              "        6.60210305e-01, 4.49158115e-01, 6.60209895e-01, 6.92896919e-02,\n",
              "        9.88775981e-01, 8.53493882e-03, 8.31458091e-01, 9.56783556e-01,\n",
              "        7.50687341e-01, 8.57298557e-01, 9.73375536e-01, 3.50823647e-01,\n",
              "        5.22935188e-02, 7.29280393e-01, 4.62668856e-01, 8.75985144e-02,\n",
              "        5.05933608e-01, 7.06450312e-01, 6.05391634e-01, 6.09454972e-01,\n",
              "        9.26610825e-01, 9.19319161e-01, 7.86225206e-01, 2.32492961e-01],\n",
              "       [2.48225074e-01, 2.86183351e-01, 7.50609368e-01, 7.92912447e-01,\n",
              "        9.71992588e-01, 6.20631592e-02, 9.34009493e-03, 1.70454128e-01,\n",
              "        2.72772818e-01, 4.40988830e-01, 3.66892181e-01, 9.29585964e-01,\n",
              "        5.10524857e-01, 5.78259740e-01, 2.17163299e-01, 8.94708838e-01,\n",
              "        2.45891319e-01, 6.12673307e-01, 2.38411021e-01, 5.01267011e-01,\n",
              "        8.07123086e-01, 2.65296980e-01, 8.37977742e-01, 1.72971331e-01,\n",
              "        1.40983883e-01, 5.76551682e-01, 8.04990030e-01, 4.78069191e-01],\n",
              "       [1.11363814e-01, 3.48801180e-01, 9.08992774e-01, 4.92317216e-02,\n",
              "        7.67925153e-02, 7.73643305e-01, 8.33408567e-01, 4.75219141e-02,\n",
              "        4.58056544e-01, 8.62226792e-02, 7.55334070e-01, 4.25659600e-01,\n",
              "        1.81373553e-01, 3.75168567e-01, 1.66654855e-01, 2.06507253e-01,\n",
              "        5.87689714e-01, 9.82020651e-01, 6.40379918e-01, 6.92315141e-01,\n",
              "        1.66218376e-04, 3.85624000e-01, 3.12864050e-01, 1.40605452e-01,\n",
              "        4.66498492e-02, 9.75965383e-01, 3.39478610e-01, 9.78419131e-01],\n",
              "       [9.05239486e-01, 6.48524090e-01, 8.86424891e-01, 5.95524902e-01,\n",
              "        1.20993030e-01, 5.77186767e-01, 6.02858026e-01, 9.78317171e-01,\n",
              "        5.99851000e-01, 2.20256678e-01, 4.46627598e-01, 5.91512594e-01,\n",
              "        7.61081732e-01, 9.05991102e-01, 9.27531910e-01, 8.38192539e-02,\n",
              "        6.66965365e-02, 7.27088959e-01, 2.22585192e-01, 9.62359783e-01,\n",
              "        3.57664081e-01, 1.53891956e-01, 2.44715490e-01, 3.06103242e-01,\n",
              "        2.66097503e-01, 2.10246975e-01, 8.19669267e-01, 8.63477678e-01],\n",
              "       [8.14608271e-01, 7.57548367e-01, 7.76598890e-01, 2.29792902e-01,\n",
              "        6.49008583e-02, 9.41623604e-01, 6.72668861e-01, 6.99307392e-01,\n",
              "        4.85565013e-01, 3.50649990e-01, 8.97875358e-01, 9.27450408e-01,\n",
              "        9.50816702e-01, 3.06256146e-01, 9.11489997e-01, 3.12780574e-02,\n",
              "        2.87335718e-01, 4.04588601e-01, 6.22645068e-01, 8.92107642e-01,\n",
              "        4.74149294e-01, 2.09399444e-01, 5.46047530e-02, 7.22066966e-01,\n",
              "        2.28289900e-02, 4.94918417e-01, 4.98279964e-01, 5.87061987e-01],\n",
              "       [2.95883486e-01, 6.57600920e-01, 8.17314662e-01, 8.10420032e-03,\n",
              "        7.34922670e-01, 2.49966658e-01, 4.57882236e-01, 7.71059635e-02,\n",
              "        9.48377731e-01, 9.22385601e-01, 1.64713443e-01, 8.29138051e-01,\n",
              "        5.12411461e-01, 9.04905196e-01, 5.81898701e-01, 1.40741973e-01,\n",
              "        4.62783379e-01, 9.73530720e-01, 9.02627588e-01, 8.33777392e-02,\n",
              "        6.05128772e-01, 7.27683872e-01, 1.44378189e-02, 9.81954385e-01,\n",
              "        5.71507503e-01, 7.16591070e-01, 1.85952741e-01, 3.72840533e-01],\n",
              "       [3.61897402e-01, 8.40870479e-01, 1.49305057e-02, 6.72726546e-01,\n",
              "        4.64634550e-01, 7.21216066e-01, 5.50064213e-01, 1.99135784e-01,\n",
              "        7.08201790e-01, 3.32566112e-01, 1.99080963e-01, 2.36103479e-01,\n",
              "        7.87059204e-01, 1.88509668e-01, 5.42486153e-01, 7.14462196e-02,\n",
              "        3.61952598e-01, 7.97356116e-01, 3.57996578e-01, 2.64895644e-01,\n",
              "        1.57902341e-01, 1.91179775e-01, 7.04503041e-01, 9.75245134e-01,\n",
              "        4.14353378e-02, 6.11062652e-01, 9.01157614e-01, 3.37090188e-01],\n",
              "       [2.86954755e-01, 5.93858100e-01, 1.86033960e-01, 1.80832392e-01,\n",
              "        2.21553449e-01, 4.64143795e-01, 1.60775823e-01, 3.09170489e-01,\n",
              "        2.16961102e-02, 3.44662132e-01, 2.10644713e-01, 5.36581290e-01,\n",
              "        7.99521325e-02, 6.20155940e-01, 8.58089634e-01, 5.33340072e-02,\n",
              "        7.31685908e-01, 9.45358037e-02, 9.28096597e-01, 4.73131775e-01,\n",
              "        5.88822150e-01, 4.16529451e-02, 4.60863530e-01, 4.21183117e-01,\n",
              "        1.05845334e-01, 4.68864562e-01, 6.34163639e-01, 6.93851883e-01],\n",
              "       [4.63995181e-01, 3.92856618e-01, 3.29010589e-01, 8.34856708e-01,\n",
              "        1.75407792e-01, 9.36578669e-01, 3.86041370e-01, 5.06180193e-01,\n",
              "        8.44346814e-01, 5.86500847e-01, 6.49128252e-02, 7.18233088e-01,\n",
              "        8.19811753e-01, 4.65038187e-01, 9.01051619e-01, 9.69685407e-01,\n",
              "        4.52666147e-01, 7.98253598e-01, 9.17129278e-01, 4.32539602e-01,\n",
              "        7.65950171e-01, 7.13739466e-01, 2.71347973e-01, 2.72794815e-01,\n",
              "        5.97010764e-01, 7.74547429e-01, 9.34968458e-01, 9.41269971e-01],\n",
              "       [7.83828032e-02, 1.23698791e-01, 2.18361479e-01, 7.20919304e-01,\n",
              "        3.96910669e-01, 5.61527422e-01, 9.77590574e-01, 2.71504738e-02,\n",
              "        4.34220693e-01, 4.54561044e-01, 2.83669657e-01, 2.47596134e-01,\n",
              "        6.80697297e-01, 3.64263487e-01, 3.72792395e-01, 4.85706272e-01,\n",
              "        1.55366475e-01, 1.21366283e-01, 4.19600537e-01, 7.17484788e-02,\n",
              "        5.40633157e-01, 5.83472066e-01, 8.84305393e-01, 3.49293264e-02,\n",
              "        2.16119937e-01, 4.70926568e-01, 2.26706901e-01, 2.20275004e-01],\n",
              "       [1.45201495e-01, 5.23802511e-01, 8.46307256e-01, 7.52843567e-01,\n",
              "        1.02134430e-01, 1.95094031e-01, 1.81760550e-01, 1.33524363e-01,\n",
              "        7.16408113e-01, 2.43126012e-01, 6.79235693e-01, 4.85683242e-02,\n",
              "        7.94580382e-01, 1.03780478e-01, 6.93729432e-01, 8.15961797e-01,\n",
              "        2.25475305e-01, 9.78533597e-01, 8.70547188e-01, 2.13461126e-01,\n",
              "        6.01718526e-01, 5.47214121e-01, 8.26452421e-02, 5.15138141e-01,\n",
              "        5.94659591e-01, 2.70908767e-02, 9.23800728e-01, 6.29729365e-01]])"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 7.4.2 im2col로 데이터 전개하기\n",
        "- 합성곱 연산을 곧이곧대로 구현하려면 for문이 여러개 필요\n",
        "- for문 대신 im2col이라는 함수를 사용해 간단히 구현"
      ],
      "metadata": {
        "id": "5Ikv-wSVjM8k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from util import im2col\n",
        "\n",
        "x1 = np.random.rand(1,3,7,7)\n",
        "col1 = im2col(x1,5,5,stride=1,pad=0)\n",
        "print(col1.shape)\n",
        "\n",
        "x2 = np.random.rand(10,3,7,7)\n",
        "col2 = im2col(x2,5,5,stride=1,pad=0)\n",
        "print(col2.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pc8Ji59AjpSJ",
        "outputId": "cbe3faf8-da0c-4b5b-acfb-24ecfbf39195"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(9, 75)\n",
            "(90, 75)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Convolution:\n",
        "    def __inti__(self, w, b, stride=1, pad=0):\n",
        "        self.w=w\n",
        "        self.b=b\n",
        "        self.stride=stride\n",
        "        self.pad\n",
        "\n",
        "    def forward(self, x):\n",
        "        fn, c, fh, fw=self.w.shape\n",
        "        n,c,h,w=x.shape\n",
        "        out_h=int(1+(h+2*self.pad-fh)/self.stride)\n",
        "        out_w=int(1+(w+2*self.pad-fw)/self.stride)\n",
        "\n",
        "        col=im2col(x, fh, fw, self.stride, self.pad)\n",
        "        col_w=self.w.reshape(fn, -1).T\n",
        "        out=np.dot(col,col_w)+self.b\n",
        "\n",
        "        out=out.reshpae(n, out_h, out_w, -1).transpose(0,3,1,2)\n",
        "\n",
        "        return out"
      ],
      "metadata": {
        "id": "WgBWYTyWkiJf"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Pooling:\n",
        "  def __init__(self, pool_h, pool_w, stride=1, pad=0):\n",
        "    self.pool_h = pool_h\n",
        "    self.pool_w = pool_w\n",
        "    self.stride = stride\n",
        "    self.pad = pad\n",
        "\n",
        "  def forward(self, x):\n",
        "    N, C, H, W = x.shape\n",
        "    out_h = int(1 + (H - self.pool_h) / self.stride)\n",
        "    out_w = int(1 + (W - self.pool_w) / self.stride)\n",
        "\n",
        "    # 전개 (1)\n",
        "    col = im2col(x, self.pool_h, self.pool_w, self.stride, self.pad)\n",
        "    col = col.reshape(-1, self.pool_h*self.pool_w)\n",
        "\n",
        "    # 최댓값 (2)\n",
        "    out = np.max(col, axis=1)\n",
        "\n",
        "    # 성형 (3)\n",
        "    out = out.reshape(N, out_h, out_w, C).transpose(0, 3, 1, 2)\n",
        "\n",
        "    return out"
      ],
      "metadata": {
        "id": "CFPatE6mkwTw"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "풀링 계층은 다음의 세단계이다\n",
        "1. 입력 데이터를 전개한다.\n",
        "2. 행별 최대값을 구한다.\n",
        "3. 적절한 모양으로 성형한다."
      ],
      "metadata": {
        "id": "RnhaT164k5X-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# coding: utf-8\n",
        "import sys, os\n",
        "import pickle\n",
        "import numpy as np\n",
        "from collections import OrderedDict\n",
        "from layers import *\n",
        "from gradient import numerical_gradient\n",
        "\n",
        "\n",
        "class SimpleConvNet:\n",
        "    \"\"\"단순한 합성곱 신경망\n",
        "\n",
        "    conv - relu - pool - affine - relu - affine - softmax\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    input_size : 입력 크기（MNIST의 경우엔 784）\n",
        "    hidden_size_list : 각 은닉층의 뉴런 수를 담은 리스트（e.g. [100, 100, 100]）\n",
        "    output_size : 출력 크기（MNIST의 경우엔 10）\n",
        "    activation : 활성화 함수 - 'relu' 혹은 'sigmoid'\n",
        "    weight_init_std : 가중치의 표준편차 지정（e.g. 0.01）\n",
        "        'relu'나 'he'로 지정하면 'He 초깃값'으로 설정\n",
        "        'sigmoid'나 'xavier'로 지정하면 'Xavier 초깃값'으로 설정\n",
        "    \"\"\"\n",
        "    def __init__(self, input_dim=(1, 28, 28),\n",
        "                 conv_param={'filter_num':30, 'filter_size':5, 'pad':0, 'stride':1},\n",
        "                 hidden_size=100, output_size=10, weight_init_std=0.01):\n",
        "        filter_num = conv_param['filter_num']\n",
        "        filter_size = conv_param['filter_size']\n",
        "        filter_pad = conv_param['pad']\n",
        "        filter_stride = conv_param['stride']\n",
        "        input_size = input_dim[1]\n",
        "        conv_output_size = (input_size - filter_size + 2*filter_pad) / filter_stride + 1\n",
        "        pool_output_size = int(filter_num * (conv_output_size/2) * (conv_output_size/2))\n",
        "\n",
        "        # 가중치 초기화\n",
        "        self.params = {}\n",
        "        self.params['W1'] = weight_init_std * \\\n",
        "                            np.random.randn(filter_num, input_dim[0], filter_size, filter_size)\n",
        "        self.params['b1'] = np.zeros(filter_num)\n",
        "        self.params['W2'] = weight_init_std * \\\n",
        "                            np.random.randn(pool_output_size, hidden_size)\n",
        "        self.params['b2'] = np.zeros(hidden_size)\n",
        "        self.params['W3'] = weight_init_std * \\\n",
        "                            np.random.randn(hidden_size, output_size)\n",
        "        self.params['b3'] = np.zeros(output_size)\n",
        "\n",
        "        # 계층 생성\n",
        "        self.layers = OrderedDict()\n",
        "        self.layers['Conv1'] = Convolution(self.params['W1'], self.params['b1'],\n",
        "                                           conv_param['stride'], conv_param['pad'])\n",
        "        self.layers['Relu1'] = Relu()\n",
        "        self.layers['Pool1'] = Pooling(pool_h=2, pool_w=2, stride=2)\n",
        "        self.layers['Affine1'] = Affine(self.params['W2'], self.params['b2'])\n",
        "        self.layers['Relu2'] = Relu()\n",
        "        self.layers['Affine2'] = Affine(self.params['W3'], self.params['b3'])\n",
        "\n",
        "        self.last_layer = SoftmaxWithLoss()\n",
        "\n",
        "    def predict(self, x):\n",
        "        for layer in self.layers.values():\n",
        "            x = layer.forward(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "    def loss(self, x, t):\n",
        "        \"\"\"손실 함수를 구한다.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        x : 입력 데이터\n",
        "        t : 정답 레이블\n",
        "        \"\"\"\n",
        "        y = self.predict(x)\n",
        "        return self.last_layer.forward(y, t)\n",
        "\n",
        "    def accuracy(self, x, t, batch_size=100):\n",
        "        if t.ndim != 1 : t = np.argmax(t, axis=1)\n",
        "\n",
        "        acc = 0.0\n",
        "\n",
        "        for i in range(int(x.shape[0] / batch_size)):\n",
        "            tx = x[i*batch_size:(i+1)*batch_size]\n",
        "            tt = t[i*batch_size:(i+1)*batch_size]\n",
        "            y = self.predict(tx)\n",
        "            y = np.argmax(y, axis=1)\n",
        "            acc += np.sum(y == tt)\n",
        "\n",
        "        return acc / x.shape[0]\n",
        "\n",
        "    def numerical_gradient(self, x, t):\n",
        "        \"\"\"기울기를 구한다（수치미분）.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        x : 입력 데이터\n",
        "        t : 정답 레이블\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        각 층의 기울기를 담은 사전(dictionary) 변수\n",
        "            grads['W1']、grads['W2']、... 각 층의 가중치\n",
        "            grads['b1']、grads['b2']、... 각 층의 편향\n",
        "        \"\"\"\n",
        "        loss_w = lambda w: self.loss(x, t)\n",
        "\n",
        "        grads = {}\n",
        "        for idx in (1, 2, 3):\n",
        "            grads['W' + str(idx)] = numerical_gradient(loss_w, self.params['W' + str(idx)])\n",
        "            grads['b' + str(idx)] = numerical_gradient(loss_w, self.params['b' + str(idx)])\n",
        "\n",
        "        return grads\n",
        "\n",
        "    def gradient(self, x, t):\n",
        "        \"\"\"기울기를 구한다(오차역전파법).\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        x : 입력 데이터\n",
        "        t : 정답 레이블\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        각 층의 기울기를 담은 사전(dictionary) 변수\n",
        "            grads['W1']、grads['W2']、... 각 층의 가중치\n",
        "            grads['b1']、grads['b2']、... 각 층의 편향\n",
        "        \"\"\"\n",
        "        # forward\n",
        "        self.loss(x, t)\n",
        "\n",
        "        # backward\n",
        "        dout = 1\n",
        "        dout = self.last_layer.backward(dout)\n",
        "\n",
        "        layers = list(self.layers.values())\n",
        "        layers.reverse()\n",
        "        for layer in layers:\n",
        "            dout = layer.backward(dout)\n",
        "\n",
        "        # 결과 저장\n",
        "        grads = {}\n",
        "        grads['W1'], grads['b1'] = self.layers['Conv1'].dW, self.layers['Conv1'].db\n",
        "        grads['W2'], grads['b2'] = self.layers['Affine1'].dW, self.layers['Affine1'].db\n",
        "        grads['W3'], grads['b3'] = self.layers['Affine2'].dW, self.layers['Affine2'].db\n",
        "\n",
        "        return grads\n",
        "\n",
        "    def save_params(self, file_name=\"params.pkl\"):\n",
        "        params = {}\n",
        "        for key, val in self.params.items():\n",
        "            params[key] = val\n",
        "        with open(file_name, 'wb') as f:\n",
        "            pickle.dump(params, f)\n",
        "\n",
        "    def load_params(self, file_name=\"params.pkl\"):\n",
        "        with open(file_name, 'rb') as f:\n",
        "            params = pickle.load(f)\n",
        "        for key, val in params.items():\n",
        "            self.params[key] = val\n",
        "\n",
        "        for i, key in enumerate(['Conv1', 'Affine1', 'Affine2']):\n",
        "            self.layers[key].W = self.params['W' + str(i+1)]\n",
        "            self.layers[key].b = self.params['b' + str(i+1)]"
      ],
      "metadata": {
        "id": "Vpv5iSTflgGm"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# coding: utf-8\n",
        "import sys, os\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from mnist import load_mnist\n",
        "from trainer import Trainer\n",
        "\n",
        "# 데이터 읽기\n",
        "(x_train, t_train), (x_test, t_test) = load_mnist(flatten=False)\n",
        "\n",
        "# 시간이 오래 걸릴 경우 데이터를 줄인다.\n",
        "#x_train, t_train = x_train[:5000], t_train[:5000]\n",
        "#x_test, t_test = x_test[:1000], t_test[:1000]\n",
        "\n",
        "max_epochs = 20\n",
        "\n",
        "network = SimpleConvNet(input_dim=(1,28,28),\n",
        "                        conv_param = {'filter_num': 30, 'filter_size': 5, 'pad': 0, 'stride': 1},\n",
        "                        hidden_size=100, output_size=10, weight_init_std=0.01)\n",
        "\n",
        "trainer = Trainer(network, x_train, t_train, x_test, t_test,\n",
        "                  epochs=max_epochs, mini_batch_size=100,\n",
        "                  optimizer='Adam', optimizer_param={'lr': 0.001},\n",
        "                  evaluate_sample_num_per_epoch=1000)\n",
        "trainer.train()\n",
        "\n",
        "# 매개변수 보존\n",
        "network.save_params(\"params.pkl\")\n",
        "print(\"Saved Network Parameters!\")\n",
        "\n",
        "# 그래프 그리기\n",
        "markers = {'train': 'o', 'test': 's'}\n",
        "x = np.arange(max_epochs)\n",
        "plt.plot(x, trainer.train_acc_list, marker='o', label='train', markevery=2)\n",
        "plt.plot(x, trainer.test_acc_list, marker='s', label='test', markevery=2)\n",
        "plt.xlabel(\"epochs\")\n",
        "plt.ylabel(\"accuracy\")\n",
        "plt.ylim(0, 1.0)\n",
        "plt.legend(loc='lower right')\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "v4ElakbrmO3E",
        "outputId": "5d954436-bca0-4399-e2a5-4b01b7e155e8"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1;30;43m스트리밍 출력 내용이 길어서 마지막 5000줄이 삭제되었습니다.\u001b[0m\n",
            "train loss:0.001574577812952196\n",
            "train loss:0.005072132769071111\n",
            "train loss:0.003931319203366165\n",
            "train loss:0.00030053704114715743\n",
            "train loss:0.04520950126988189\n",
            "train loss:0.09654554799413419\n",
            "train loss:0.003131620545215487\n",
            "train loss:0.00241113985612563\n",
            "train loss:0.0008735860616658293\n",
            "train loss:0.019146470504993775\n",
            "train loss:0.012289558136927257\n",
            "train loss:0.002145707643157784\n",
            "train loss:0.0038727332183360465\n",
            "train loss:0.0036522275235960438\n",
            "train loss:0.004075366614102789\n",
            "train loss:0.028449807600529887\n",
            "train loss:0.009172772336530499\n",
            "train loss:0.014630087505723281\n",
            "train loss:0.010401092269992952\n",
            "train loss:0.0008728146401218182\n",
            "train loss:0.002188233184014461\n",
            "train loss:0.002399457435226728\n",
            "train loss:0.005541330246342707\n",
            "train loss:0.0034914204355686574\n",
            "train loss:0.013590476497155342\n",
            "train loss:0.0005204222964053689\n",
            "train loss:0.003105074800148667\n",
            "train loss:0.003013543201644791\n",
            "train loss:0.004041894060130521\n",
            "train loss:0.0024683521136648918\n",
            "train loss:0.006707405151345621\n",
            "train loss:0.008001700897849469\n",
            "train loss:0.001122031880348745\n",
            "train loss:0.001227731601910115\n",
            "train loss:0.0009730725009245149\n",
            "train loss:0.0035735481379636958\n",
            "train loss:0.00574779384211401\n",
            "train loss:0.0182061053360113\n",
            "train loss:0.0021200933439136857\n",
            "train loss:0.0028589125589032188\n",
            "train loss:0.0070434443544310325\n",
            "train loss:0.007817063433749005\n",
            "train loss:0.004065610803921579\n",
            "train loss:0.0010015461073867146\n",
            "train loss:0.0030370084059584148\n",
            "train loss:0.00530166280131704\n",
            "train loss:0.00561991502381481\n",
            "train loss:0.0071545853714633815\n",
            "train loss:0.008736423712065877\n",
            "train loss:0.004861925533602123\n",
            "train loss:0.0008702469326251272\n",
            "train loss:0.0030051361451076293\n",
            "train loss:0.0006668528139999154\n",
            "train loss:0.024968181736515715\n",
            "train loss:0.010809022818753938\n",
            "train loss:0.0029015036550393437\n",
            "train loss:0.000814355373139217\n",
            "train loss:0.0020487940517976387\n",
            "train loss:0.0029704041381584338\n",
            "train loss:0.003169500792034333\n",
            "train loss:0.0011938503181385762\n",
            "train loss:0.007682497359300056\n",
            "train loss:0.0009625216583188937\n",
            "train loss:0.0021518681785852286\n",
            "train loss:0.007508159229715906\n",
            "train loss:0.0035089944435219025\n",
            "train loss:0.0016518939802440968\n",
            "train loss:0.003119573091284639\n",
            "train loss:0.008378761346712903\n",
            "train loss:0.005729740773824276\n",
            "train loss:0.0024255081169072905\n",
            "train loss:0.006412268799209415\n",
            "train loss:0.0015797232822662836\n",
            "train loss:0.014136397830630809\n",
            "train loss:0.0068327099578746435\n",
            "train loss:0.001041764586753513\n",
            "train loss:0.02666660121249952\n",
            "train loss:0.005717528401214919\n",
            "train loss:0.0013595390278066178\n",
            "train loss:0.0009795380688904754\n",
            "train loss:0.006356164156577234\n",
            "train loss:0.00044658851545863377\n",
            "train loss:0.0026634606581526694\n",
            "train loss:0.0015688004491356293\n",
            "train loss:0.002504409617463366\n",
            "train loss:0.001289966611589602\n",
            "train loss:0.0021067556367369705\n",
            "train loss:0.010841707707981\n",
            "train loss:0.0026367864451382877\n",
            "train loss:0.0019047900814970556\n",
            "train loss:5.0924233012020515e-05\n",
            "train loss:0.0007189772385914179\n",
            "train loss:0.0023122039158004447\n",
            "train loss:0.004364777371119557\n",
            "train loss:0.0014345866899795386\n",
            "train loss:0.0013338513052874928\n",
            "train loss:0.0010865651760661602\n",
            "train loss:0.005641303967923691\n",
            "train loss:0.0022638433324061844\n",
            "train loss:0.003669091292261234\n",
            "train loss:0.01061702323835114\n",
            "train loss:0.004103929769686324\n",
            "train loss:0.003827358269503147\n",
            "train loss:0.0006743624785868233\n",
            "train loss:0.0024048934275174177\n",
            "train loss:0.008408483755550226\n",
            "train loss:0.006393211802109106\n",
            "train loss:0.007157328732788179\n",
            "train loss:0.001994968529066984\n",
            "train loss:0.0074445848529638425\n",
            "train loss:0.0019281880415479497\n",
            "train loss:0.0005975811604225035\n",
            "train loss:0.0006191371608492908\n",
            "train loss:0.0022705917049974413\n",
            "train loss:0.008941490542465087\n",
            "train loss:0.0056913404558686055\n",
            "train loss:0.00029048008276826964\n",
            "train loss:0.009749718290787775\n",
            "train loss:0.001887593379697394\n",
            "train loss:0.0005061700003046538\n",
            "train loss:0.0028422042893576573\n",
            "train loss:0.005553981122016573\n",
            "train loss:0.005058178747485807\n",
            "train loss:0.0003716327921882614\n",
            "train loss:0.0007417763303347459\n",
            "train loss:0.0005403301183580898\n",
            "train loss:0.016387980737615576\n",
            "train loss:0.0005312222220450651\n",
            "train loss:0.02386270978997345\n",
            "train loss:0.0038438127510528343\n",
            "train loss:0.0037645442813392794\n",
            "train loss:0.0016058738651477035\n",
            "train loss:0.001026389505017882\n",
            "train loss:0.002562280950496792\n",
            "train loss:0.003654303623601729\n",
            "train loss:0.025625504456707943\n",
            "train loss:0.0031064014206708375\n",
            "train loss:0.0019364540747046919\n",
            "train loss:0.005450279835668116\n",
            "train loss:0.0029331135475389384\n",
            "train loss:0.00403756281869317\n",
            "train loss:0.014123451588521787\n",
            "train loss:0.0049546371452099626\n",
            "train loss:0.0018572629908647836\n",
            "train loss:0.0016567362505469788\n",
            "train loss:0.0030500201679363407\n",
            "train loss:0.004610288317139169\n",
            "train loss:0.0007914815430844582\n",
            "train loss:0.0048984266709160736\n",
            "train loss:0.00024298480628423834\n",
            "train loss:0.0007304831484559628\n",
            "train loss:0.010422152365120323\n",
            "train loss:0.021760642324913256\n",
            "train loss:0.0871124414585696\n",
            "train loss:0.00016204473307974496\n",
            "train loss:0.005394429198492288\n",
            "train loss:0.0021057302260771606\n",
            "train loss:0.0034688708888139163\n",
            "train loss:0.005233466061508755\n",
            "train loss:0.006342326019352924\n",
            "train loss:0.0008090032291921994\n",
            "train loss:0.05608372099614058\n",
            "train loss:0.0039068137624196485\n",
            "train loss:0.01690206347919366\n",
            "train loss:0.0049450997493338455\n",
            "train loss:0.00620206897860067\n",
            "train loss:0.0032853743515656782\n",
            "train loss:0.01101477370504791\n",
            "train loss:0.004057080511310422\n",
            "train loss:0.003274142354922675\n",
            "train loss:0.0007103778546889961\n",
            "train loss:0.0029222903035271853\n",
            "train loss:0.002168004443546179\n",
            "train loss:0.019386241994514376\n",
            "train loss:0.0012217439433278617\n",
            "train loss:0.0015864046752371058\n",
            "train loss:0.009551687688387408\n",
            "train loss:0.06427620022814123\n",
            "train loss:0.007190714168664657\n",
            "train loss:0.0002671845227547288\n",
            "train loss:0.0008683148326402717\n",
            "train loss:0.0028287636863750304\n",
            "train loss:0.004584493997512002\n",
            "train loss:0.0011011775432417676\n",
            "train loss:0.0010639774881492114\n",
            "train loss:0.002463742037008586\n",
            "train loss:0.0010128330624126131\n",
            "train loss:0.008242051867893372\n",
            "train loss:0.00215529224485886\n",
            "train loss:0.005634339105755004\n",
            "=== epoch:13, train acc:0.995, test acc:0.984 ===\n",
            "train loss:0.0015580735253729438\n",
            "train loss:0.0029584492149163115\n",
            "train loss:0.002482020970345661\n",
            "train loss:0.009936437255897207\n",
            "train loss:0.0028673043937758485\n",
            "train loss:0.0002067509429510968\n",
            "train loss:0.004331171116555885\n",
            "train loss:0.0036702098881778956\n",
            "train loss:0.0011546759990626633\n",
            "train loss:0.002119928251234855\n",
            "train loss:0.06587681331236833\n",
            "train loss:0.018136136874583864\n",
            "train loss:0.0008523974305974356\n",
            "train loss:0.00127419066442392\n",
            "train loss:0.0016903354546918953\n",
            "train loss:0.0022498445375504074\n",
            "train loss:0.0008219357083607362\n",
            "train loss:0.0009106384352856121\n",
            "train loss:0.0042285624381470455\n",
            "train loss:0.0032348226213607906\n",
            "train loss:0.018430412844946547\n",
            "train loss:0.013643497975526055\n",
            "train loss:0.0004128470333856904\n",
            "train loss:0.002718410551815938\n",
            "train loss:0.0017310111861403375\n",
            "train loss:0.001132281842786376\n",
            "train loss:0.004525283109765323\n",
            "train loss:0.0006443090701224898\n",
            "train loss:0.004274821926295704\n",
            "train loss:0.008676401736421441\n",
            "train loss:0.000629915692991748\n",
            "train loss:0.0031564627914774357\n",
            "train loss:0.00590992687874463\n",
            "train loss:0.0082486100699807\n",
            "train loss:0.003430505074919878\n",
            "train loss:0.00283467944684857\n",
            "train loss:0.0025538946891058645\n",
            "train loss:0.014487581177113391\n",
            "train loss:0.0017787777062922994\n",
            "train loss:0.005958408112497491\n",
            "train loss:0.00281290156401222\n",
            "train loss:0.005286589763266009\n",
            "train loss:0.003561512803464449\n",
            "train loss:0.0007460898429600269\n",
            "train loss:0.02339206614322623\n",
            "train loss:0.00019145294725400792\n",
            "train loss:0.00845674616207033\n",
            "train loss:0.002530674501761839\n",
            "train loss:0.003721320931563309\n",
            "train loss:0.0014242562738604957\n",
            "train loss:0.008299641313493636\n",
            "train loss:0.0006852893214088039\n",
            "train loss:0.0008478449158941993\n",
            "train loss:0.0013015608275485752\n",
            "train loss:0.01983167115103975\n",
            "train loss:0.005118027275138567\n",
            "train loss:0.00925557563770043\n",
            "train loss:0.003917714739163278\n",
            "train loss:0.002327507054443269\n",
            "train loss:0.005609539398145481\n",
            "train loss:0.00555073607816059\n",
            "train loss:0.002626477785460068\n",
            "train loss:0.010096789810614\n",
            "train loss:0.006305401683255854\n",
            "train loss:0.0022224640046318166\n",
            "train loss:0.0006534143061606601\n",
            "train loss:0.002468272086692307\n",
            "train loss:0.0029201007993427714\n",
            "train loss:0.003313111999220971\n",
            "train loss:0.0012332097368273696\n",
            "train loss:0.0034087764225052865\n",
            "train loss:0.003860962095334771\n",
            "train loss:0.0024227704065362403\n",
            "train loss:0.0069840285684275505\n",
            "train loss:0.005884926119909517\n",
            "train loss:0.0032174916972427547\n",
            "train loss:0.0012351072692333876\n",
            "train loss:0.0020283894053526\n",
            "train loss:0.002906059081695401\n",
            "train loss:0.012593857416100768\n",
            "train loss:0.0030035732727768205\n",
            "train loss:0.0010904089069599316\n",
            "train loss:0.008402297854373828\n",
            "train loss:0.0029159106749859862\n",
            "train loss:0.001528287379070562\n",
            "train loss:0.00589017950589326\n",
            "train loss:0.0016090042776297788\n",
            "train loss:0.00030394390562482615\n",
            "train loss:0.0009512205922997835\n",
            "train loss:0.042678219929998894\n",
            "train loss:0.021412088523921416\n",
            "train loss:0.0062210768323587694\n",
            "train loss:0.0022064665660531117\n",
            "train loss:0.0006718988842752231\n",
            "train loss:0.005553416062445047\n",
            "train loss:0.005844779693945048\n",
            "train loss:0.00388056370783514\n",
            "train loss:0.0003984505425646641\n",
            "train loss:0.004256987096634097\n",
            "train loss:0.004546794012693503\n",
            "train loss:0.0025340831802371806\n",
            "train loss:0.004683232152915724\n",
            "train loss:0.0029396707301811925\n",
            "train loss:0.01156560934506264\n",
            "train loss:0.019329252757177246\n",
            "train loss:0.0009598616877768285\n",
            "train loss:0.0006204291583786913\n",
            "train loss:0.002254507867186846\n",
            "train loss:0.0015587350030942058\n",
            "train loss:0.0014194665831207792\n",
            "train loss:0.0025324984900802687\n",
            "train loss:0.0024704902072572397\n",
            "train loss:0.0022417927887792197\n",
            "train loss:0.02440478092592774\n",
            "train loss:0.011332407354819884\n",
            "train loss:0.0023436659051703545\n",
            "train loss:0.002651394743730444\n",
            "train loss:0.0019146373510733744\n",
            "train loss:0.0011352334053311977\n",
            "train loss:0.003505348350843272\n",
            "train loss:0.00046490552766778235\n",
            "train loss:0.003786949224930715\n",
            "train loss:0.014987667311259955\n",
            "train loss:0.003463091323146918\n",
            "train loss:0.001259431279274113\n",
            "train loss:0.00268861639439308\n",
            "train loss:0.0034199799167334947\n",
            "train loss:0.0040137014186238966\n",
            "train loss:0.003647149117998361\n",
            "train loss:0.0007993635449780985\n",
            "train loss:0.0011977731140703207\n",
            "train loss:0.000731832445546239\n",
            "train loss:0.0010958296225331463\n",
            "train loss:0.0032061640022012002\n",
            "train loss:0.0038298150125987782\n",
            "train loss:0.0016251742785785895\n",
            "train loss:0.004605400407564072\n",
            "train loss:0.001029869618650728\n",
            "train loss:0.0014584912690079484\n",
            "train loss:0.003164284205338759\n",
            "train loss:0.005207658569113715\n",
            "train loss:0.004229124271406356\n",
            "train loss:0.0022694468803769983\n",
            "train loss:0.0019541825652201915\n",
            "train loss:0.009150938313452216\n",
            "train loss:0.007871242173314858\n",
            "train loss:0.00511299755164605\n",
            "train loss:0.006473019901767669\n",
            "train loss:0.0025496326487955\n",
            "train loss:0.004686736671793836\n",
            "train loss:0.019502178930331803\n",
            "train loss:0.0009568992283164996\n",
            "train loss:0.0010676146774443306\n",
            "train loss:0.0021787684123626646\n",
            "train loss:0.0018068247195651194\n",
            "train loss:0.021490360500154987\n",
            "train loss:0.00244230201440661\n",
            "train loss:0.0020436967329350284\n",
            "train loss:0.0005193079256593202\n",
            "train loss:0.0008465789207784706\n",
            "train loss:0.0033121980512183817\n",
            "train loss:0.0009079202417691514\n",
            "train loss:0.0031401228855059897\n",
            "train loss:0.00420463038592782\n",
            "train loss:0.005212246452782632\n",
            "train loss:0.001716698517476377\n",
            "train loss:0.0034791012523848817\n",
            "train loss:0.0039993109139304815\n",
            "train loss:0.001275357470171933\n",
            "train loss:0.0020157094323456155\n",
            "train loss:0.002195680687058437\n",
            "train loss:0.036396671169344245\n",
            "train loss:0.002515661739736547\n",
            "train loss:0.0008334565841121011\n",
            "train loss:0.02053882486112814\n",
            "train loss:0.005451732076913749\n",
            "train loss:0.0022648977201232463\n",
            "train loss:0.002068905232080179\n",
            "train loss:0.027899162132943675\n",
            "train loss:0.0005564083651718218\n",
            "train loss:0.0027553793647866563\n",
            "train loss:0.002358949945700096\n",
            "train loss:0.0027898798072798782\n",
            "train loss:0.006023434347176474\n",
            "train loss:0.016126938030578127\n",
            "train loss:0.0020171072503408455\n",
            "train loss:0.0055306195861135835\n",
            "train loss:0.0026689904334702397\n",
            "train loss:0.0119434420332469\n",
            "train loss:0.0006550100144423612\n",
            "train loss:0.0021663446183450667\n",
            "train loss:0.004363409945936781\n",
            "train loss:0.003753205915920434\n",
            "train loss:0.0023495795799038386\n",
            "train loss:0.0027563262354898593\n",
            "train loss:0.0025720615927173674\n",
            "train loss:0.0008189996077282455\n",
            "train loss:0.011187920256372885\n",
            "train loss:0.0015411129392475023\n",
            "train loss:0.0023914871473029588\n",
            "train loss:0.012875805257063149\n",
            "train loss:0.005040073264261598\n",
            "train loss:0.0003639759904246483\n",
            "train loss:0.0044361915869643\n",
            "train loss:0.0060152151690704625\n",
            "train loss:0.00377101198560572\n",
            "train loss:0.005396022744746542\n",
            "train loss:0.0025584698830708377\n",
            "train loss:0.0037239254722039082\n",
            "train loss:0.0029553113644480767\n",
            "train loss:0.001815480691717492\n",
            "train loss:0.0031832152761806947\n",
            "train loss:0.0013761139394164019\n",
            "train loss:0.02086588967572812\n",
            "train loss:0.0009855716507931925\n",
            "train loss:0.006566384402349277\n",
            "train loss:0.0017085235922886836\n",
            "train loss:0.0005427536540611277\n",
            "train loss:0.0012462406276196206\n",
            "train loss:0.002103447309354364\n",
            "train loss:0.009771045350124468\n",
            "train loss:0.0024847849793588925\n",
            "train loss:0.00983928949115012\n",
            "train loss:0.0022832943343389324\n",
            "train loss:0.004574492514147078\n",
            "train loss:0.03383895859002532\n",
            "train loss:0.0008210929418580315\n",
            "train loss:0.004135726334460545\n",
            "train loss:0.0001743349200461948\n",
            "train loss:0.00032453850416919644\n",
            "train loss:0.0005257137154987481\n",
            "train loss:0.006435519844564688\n",
            "train loss:0.0006223613651609489\n",
            "train loss:0.0007645817357448905\n",
            "train loss:0.0017796002110779731\n",
            "train loss:0.0032738145863619285\n",
            "train loss:0.00037728326582483373\n",
            "train loss:0.0038263643456565933\n",
            "train loss:0.004795824414525002\n",
            "train loss:0.003276879822999823\n",
            "train loss:0.0007464416426006407\n",
            "train loss:0.00040810771852956696\n",
            "train loss:0.0023952617626392496\n",
            "train loss:0.0022571891091334685\n",
            "train loss:0.001840492347985705\n",
            "train loss:0.0016075125451070372\n",
            "train loss:0.005224206839300988\n",
            "train loss:0.0006347018971789693\n",
            "train loss:0.0018645785787738988\n",
            "train loss:0.002754024547403537\n",
            "train loss:0.003601982403128577\n",
            "train loss:0.0016079643838805132\n",
            "train loss:0.002040489934074996\n",
            "train loss:0.03186288101586452\n",
            "train loss:0.00047617921113888244\n",
            "train loss:0.004708449261024579\n",
            "train loss:0.0028297631904042417\n",
            "train loss:0.015083055153563958\n",
            "train loss:0.0022386186754286483\n",
            "train loss:0.0037799070914228306\n",
            "train loss:0.00045119796774241645\n",
            "train loss:0.0010342882059698572\n",
            "train loss:0.002106157482670443\n",
            "train loss:0.0010676692778650721\n",
            "train loss:0.005864722760579616\n",
            "train loss:0.0032872984308440416\n",
            "train loss:0.0027552693213262584\n",
            "train loss:0.00025380056187873044\n",
            "train loss:0.00736463673283448\n",
            "train loss:0.00041736073413174175\n",
            "train loss:0.0022760563753746244\n",
            "train loss:0.004927682601978383\n",
            "train loss:0.006256172913130096\n",
            "train loss:0.0021828206045072314\n",
            "train loss:0.006145682938264984\n",
            "train loss:0.06398266866581657\n",
            "train loss:0.0003051583228097886\n",
            "train loss:0.0006025223834361506\n",
            "train loss:0.0015771465998228895\n",
            "train loss:0.004589321689063368\n",
            "train loss:0.002188736103106691\n",
            "train loss:0.0003525920028447609\n",
            "train loss:0.002923222019097778\n",
            "train loss:0.006223988204406645\n",
            "train loss:0.0032554469636389555\n",
            "train loss:0.003189106076424574\n",
            "train loss:0.007417878606541577\n",
            "train loss:0.0016854976383562498\n",
            "train loss:0.004667684702689752\n",
            "train loss:0.004615636558552762\n",
            "train loss:0.0006904976482866523\n",
            "train loss:0.001237396970563389\n",
            "train loss:0.06291000266600179\n",
            "train loss:0.0012572714537650198\n",
            "train loss:0.005648122449394174\n",
            "train loss:0.0010790411295612896\n",
            "train loss:0.0027098016118474867\n",
            "train loss:0.026730851047022844\n",
            "train loss:0.004089726690908324\n",
            "train loss:0.001121708680832897\n",
            "train loss:0.005359406064792195\n",
            "train loss:0.0012735218000264987\n",
            "train loss:0.0018483895954759196\n",
            "train loss:0.006086046464470826\n",
            "train loss:0.001288407361075776\n",
            "train loss:0.012350084028838295\n",
            "train loss:0.006586558462846871\n",
            "train loss:0.004877203798250508\n",
            "train loss:0.002443682290999634\n",
            "train loss:0.007857404115579053\n",
            "train loss:0.003442579223821998\n",
            "train loss:0.004149722769499965\n",
            "train loss:0.0032701798643080893\n",
            "train loss:0.0065716193871396674\n",
            "train loss:0.004013963633815965\n",
            "train loss:0.00530478325740478\n",
            "train loss:0.0017514298238407245\n",
            "train loss:0.006933919324621934\n",
            "train loss:0.004312919602918627\n",
            "train loss:0.0032735253069232583\n",
            "train loss:0.0017142576896246005\n",
            "train loss:0.007968499718428649\n",
            "train loss:0.0002859077401369611\n",
            "train loss:0.0002463450527758558\n",
            "train loss:0.0003914636915718491\n",
            "train loss:0.0014011936951295802\n",
            "train loss:0.003722907751691887\n",
            "train loss:0.000249171835186855\n",
            "train loss:0.0024296125406090044\n",
            "train loss:0.0030675962812766883\n",
            "train loss:0.0017202988486547202\n",
            "train loss:0.0034952968274146413\n",
            "train loss:0.0007845093939084303\n",
            "train loss:0.004725641914396492\n",
            "train loss:0.0006223691969579556\n",
            "train loss:0.00072982366401058\n",
            "train loss:0.0005532320838344746\n",
            "train loss:0.001521983959287907\n",
            "train loss:0.006411882892771083\n",
            "train loss:0.0009548186172440071\n",
            "train loss:0.0014474189892157805\n",
            "train loss:0.0028776769331137564\n",
            "train loss:0.0008472248363918315\n",
            "train loss:0.006853957484910337\n",
            "train loss:0.0008199594457524525\n",
            "train loss:0.007662073243905337\n",
            "train loss:0.003150319602292241\n",
            "train loss:0.001415660052647132\n",
            "train loss:0.002729932212789765\n",
            "train loss:0.0008720875392823008\n",
            "train loss:0.02765935747616405\n",
            "train loss:0.002951475053275644\n",
            "train loss:0.002053765714427146\n",
            "train loss:0.000562421587759179\n",
            "train loss:0.0003021352854435499\n",
            "train loss:0.003921512502014425\n",
            "train loss:0.004225656372791993\n",
            "train loss:0.04346399668154501\n",
            "train loss:0.0016845919656566254\n",
            "train loss:0.0009086266243295291\n",
            "train loss:0.0009521196720368997\n",
            "train loss:0.0006890281846494737\n",
            "train loss:0.0015699760698363128\n",
            "train loss:0.02380319636344845\n",
            "train loss:0.0004788444750819935\n",
            "train loss:0.0058477646215070005\n",
            "train loss:0.02423673736970373\n",
            "train loss:0.0009575463656806634\n",
            "train loss:0.002757684342435434\n",
            "train loss:0.002174329208106531\n",
            "train loss:0.003616536734834354\n",
            "train loss:0.001864215284012582\n",
            "train loss:0.0050942414036434895\n",
            "train loss:0.0013310144002155314\n",
            "train loss:0.01959665916114396\n",
            "train loss:0.002532277086494093\n",
            "train loss:0.0014799044583107583\n",
            "train loss:0.004195843993576053\n",
            "train loss:0.0007541721344876718\n",
            "train loss:0.0006614849786478346\n",
            "train loss:0.0019458251687986471\n",
            "train loss:0.0008652922972333\n",
            "train loss:0.012068460769047005\n",
            "train loss:0.0065658814748849395\n",
            "train loss:0.006387817205449269\n",
            "train loss:0.002589858562577874\n",
            "train loss:0.00588605107688029\n",
            "train loss:0.005362343566022689\n",
            "train loss:0.0051344368427969685\n",
            "train loss:0.002044331036515358\n",
            "train loss:0.004523368525767492\n",
            "train loss:0.004890278932717246\n",
            "train loss:0.0015446744913711053\n",
            "train loss:0.006414240288991987\n",
            "train loss:0.0008901718670275106\n",
            "train loss:0.000483551276429398\n",
            "train loss:0.004107908272127098\n",
            "train loss:0.0008968547213268857\n",
            "train loss:9.921658780312207e-05\n",
            "train loss:0.014979904121830303\n",
            "train loss:0.003872460705521359\n",
            "train loss:0.00546735175996311\n",
            "train loss:0.0007174155483989616\n",
            "train loss:0.004879181044130137\n",
            "train loss:0.00015539081291840323\n",
            "train loss:0.00015118887386912993\n",
            "train loss:0.011863318383945977\n",
            "train loss:0.000671782082856502\n",
            "train loss:0.003204895794692697\n",
            "train loss:0.05951049938058878\n",
            "train loss:0.0034460054144027836\n",
            "train loss:0.005553968146315107\n",
            "train loss:0.0026689598902962224\n",
            "train loss:0.0036254766776340147\n",
            "train loss:0.004189694089774933\n",
            "train loss:0.0009033064470895168\n",
            "train loss:0.0007025555244081\n",
            "train loss:0.0038992635090830787\n",
            "train loss:0.0027279130283423265\n",
            "train loss:0.004766196132796018\n",
            "train loss:0.00380728831574443\n",
            "train loss:0.00425237099011175\n",
            "train loss:7.416968504900598e-05\n",
            "train loss:0.0007184505335406237\n",
            "train loss:0.0030101081167173754\n",
            "train loss:0.0005203171589646696\n",
            "train loss:0.002940857295069847\n",
            "train loss:0.0001155963703874151\n",
            "train loss:0.0007665159429004119\n",
            "train loss:0.001081619539392884\n",
            "train loss:0.012017311465272811\n",
            "train loss:0.004333541487419344\n",
            "train loss:0.0004211790291893478\n",
            "train loss:6.623131844350477e-05\n",
            "train loss:0.0019323046041453732\n",
            "train loss:0.0005636586452541314\n",
            "train loss:0.002373072111139746\n",
            "train loss:0.005793445597563369\n",
            "train loss:0.00035079640995361696\n",
            "train loss:0.010609606520036072\n",
            "train loss:0.006335690697948423\n",
            "train loss:0.00013598023677932028\n",
            "train loss:0.0011607025711942518\n",
            "train loss:4.666055865416513e-05\n",
            "train loss:0.004817805107210289\n",
            "train loss:0.0015414544398789376\n",
            "train loss:0.0006027570601524481\n",
            "train loss:0.0006264219746418656\n",
            "train loss:0.00059355984836231\n",
            "train loss:0.0071412067060888105\n",
            "train loss:0.002234282667616745\n",
            "train loss:0.0034685048993325605\n",
            "train loss:0.015175051282436913\n",
            "train loss:0.0028274406197132938\n",
            "train loss:0.006615536859751634\n",
            "train loss:0.0009478712315655061\n",
            "train loss:0.016092306122313296\n",
            "train loss:0.004224346859134705\n",
            "train loss:0.0016882582274914007\n",
            "train loss:0.004633902445476085\n",
            "train loss:0.001905638816861964\n",
            "train loss:0.00911452447188778\n",
            "train loss:0.000619341500482493\n",
            "train loss:0.002598572845725481\n",
            "train loss:0.00362713334765212\n",
            "train loss:0.00015484364362479674\n",
            "train loss:0.002269796312807898\n",
            "train loss:0.0008456589080744076\n",
            "train loss:0.0015829111495629573\n",
            "train loss:0.0016765368166366452\n",
            "train loss:0.0016532489203084214\n",
            "train loss:0.00022319743563874446\n",
            "train loss:0.0021481349068271614\n",
            "train loss:0.0002749068271016794\n",
            "train loss:0.0026212441582886647\n",
            "train loss:0.0029272327534812874\n",
            "train loss:0.004127404042061751\n",
            "train loss:0.001975686798656105\n",
            "train loss:0.0020148970337330006\n",
            "train loss:0.00047820729326008253\n",
            "train loss:0.000371160036203826\n",
            "train loss:0.0010380317426211816\n",
            "train loss:0.0012707103594176011\n",
            "train loss:0.009824558375251154\n",
            "train loss:0.004088067031551109\n",
            "train loss:0.0019386563397101974\n",
            "train loss:0.0016523190134930734\n",
            "train loss:0.0034782977030268143\n",
            "train loss:0.004366094500804311\n",
            "train loss:0.00039845332230214613\n",
            "train loss:0.0023499425983193874\n",
            "train loss:0.0017172406150485486\n",
            "train loss:0.008415115363667902\n",
            "train loss:0.004308887454909328\n",
            "train loss:0.0002535555589618602\n",
            "train loss:0.0019023435171667375\n",
            "train loss:0.0025706464909568687\n",
            "train loss:0.0004243658245380375\n",
            "train loss:0.0014038259278611564\n",
            "train loss:0.002002016238807966\n",
            "train loss:0.007191775348284857\n",
            "train loss:0.00013677949250192985\n",
            "train loss:0.001380534626674208\n",
            "train loss:0.009696982097837547\n",
            "train loss:0.006587099714251013\n",
            "train loss:0.0008529791156079955\n",
            "train loss:0.0016399983179851663\n",
            "train loss:0.001589798030877752\n",
            "train loss:0.002033375157271349\n",
            "train loss:0.01571917217884558\n",
            "train loss:0.011758994162472038\n",
            "train loss:0.002786166058676946\n",
            "train loss:0.0016395203812651124\n",
            "train loss:0.001920321231210424\n",
            "train loss:0.0017156010317200899\n",
            "train loss:0.0031383362417808426\n",
            "train loss:0.0020171295465438587\n",
            "train loss:0.00029644815757453756\n",
            "train loss:0.0033771899270765102\n",
            "train loss:0.0010393064619277513\n",
            "train loss:0.00035802285890953605\n",
            "train loss:0.007317184641500002\n",
            "train loss:0.000951328821817232\n",
            "train loss:0.0003412739408742923\n",
            "train loss:0.002995389830688333\n",
            "train loss:0.001814810155874058\n",
            "train loss:0.002723433499969783\n",
            "train loss:0.002930631788975731\n",
            "train loss:0.0023314712321747975\n",
            "train loss:0.001475334539919308\n",
            "train loss:0.005549282488613075\n",
            "train loss:0.004210380681756329\n",
            "train loss:0.004013354547948994\n",
            "train loss:0.0005585779212024355\n",
            "train loss:0.003038208850900738\n",
            "train loss:0.0008367554516349298\n",
            "train loss:0.00630399260598356\n",
            "train loss:0.0015169966222177004\n",
            "train loss:0.00014656689196125032\n",
            "train loss:0.002401294005166113\n",
            "train loss:0.0038900254234490207\n",
            "train loss:0.0019220771837326523\n",
            "train loss:0.0007723000431300611\n",
            "train loss:0.013252359547596002\n",
            "train loss:0.0004159808754130254\n",
            "train loss:0.00633548114588263\n",
            "train loss:0.004639788000377718\n",
            "train loss:0.018648501360312555\n",
            "train loss:0.0005971873589997378\n",
            "train loss:0.002373113327414783\n",
            "train loss:0.0005291421951299713\n",
            "train loss:0.0077178162226187496\n",
            "train loss:0.006547104212919989\n",
            "train loss:0.01001668661365255\n",
            "train loss:0.004901282747432531\n",
            "train loss:0.0016623156506917463\n",
            "train loss:0.000579862626860477\n",
            "train loss:0.001577273072542901\n",
            "train loss:0.0002165810937653255\n",
            "train loss:0.0014964886410620125\n",
            "train loss:0.00680129697388177\n",
            "train loss:0.010895402966500527\n",
            "train loss:0.0001831947015277651\n",
            "train loss:0.004767590474521954\n",
            "train loss:0.00318254128403645\n",
            "train loss:0.008666902746453452\n",
            "train loss:0.011793465735754027\n",
            "train loss:0.0014862957777276517\n",
            "train loss:0.005120035695509634\n",
            "train loss:0.001461284971635566\n",
            "train loss:0.00015758936501057743\n",
            "train loss:0.0020118809398871717\n",
            "train loss:0.0032053219158166995\n",
            "train loss:0.005406177712579496\n",
            "train loss:0.0047342610507795655\n",
            "train loss:0.00514629909016713\n",
            "train loss:0.003746785431687343\n",
            "train loss:0.008387557369528518\n",
            "train loss:0.007049335986182038\n",
            "train loss:0.0013768128236520166\n",
            "train loss:0.0018763805401334898\n",
            "train loss:0.00042063989506030376\n",
            "train loss:0.00016248973455885087\n",
            "train loss:0.0007116179341535436\n",
            "train loss:0.002127568789822841\n",
            "train loss:0.0006366644236818059\n",
            "train loss:0.0019936637784298706\n",
            "train loss:0.00340735318971175\n",
            "train loss:0.0004435140631161521\n",
            "train loss:0.0054148220530931355\n",
            "train loss:0.0036251171839963483\n",
            "train loss:0.005117114100326621\n",
            "train loss:0.0010274064056733053\n",
            "train loss:0.002381058729109841\n",
            "train loss:0.0002450698025464091\n",
            "train loss:0.0016914553992417394\n",
            "train loss:0.01947101302460998\n",
            "train loss:0.003921377857324234\n",
            "train loss:0.0035696468525771925\n",
            "train loss:0.036334638640016785\n",
            "=== epoch:14, train acc:0.999, test acc:0.988 ===\n",
            "train loss:0.0018821646096366872\n",
            "train loss:0.0012027988640262593\n",
            "train loss:0.005971383683425802\n",
            "train loss:0.0018976985090892165\n",
            "train loss:0.0038744506509981683\n",
            "train loss:0.002870864753646694\n",
            "train loss:0.0005137569994062734\n",
            "train loss:0.003131971218968696\n",
            "train loss:0.007313362316441731\n",
            "train loss:0.00018368131942587246\n",
            "train loss:0.001174166254686441\n",
            "train loss:0.0017201532177690895\n",
            "train loss:0.0011296765987116267\n",
            "train loss:0.000330853099213052\n",
            "train loss:0.00587619769210402\n",
            "train loss:0.0029871827094421427\n",
            "train loss:0.003127253443851774\n",
            "train loss:0.0022787905513722935\n",
            "train loss:0.0038278341263804894\n",
            "train loss:0.0016379581388000675\n",
            "train loss:0.005244815171969135\n",
            "train loss:0.001971622696428732\n",
            "train loss:0.000631403584881375\n",
            "train loss:0.00031686509919457437\n",
            "train loss:0.00041591825068090386\n",
            "train loss:0.003651475189189416\n",
            "train loss:0.002613342564093174\n",
            "train loss:0.0011624954921582347\n",
            "train loss:0.0006736359474192535\n",
            "train loss:0.005832351257028482\n",
            "train loss:0.003936550428144092\n",
            "train loss:0.0037768682299263267\n",
            "train loss:0.00347556702987614\n",
            "train loss:0.024945164342376328\n",
            "train loss:0.004787798567079123\n",
            "train loss:0.0009917041263220806\n",
            "train loss:0.0016346669406828978\n",
            "train loss:0.0005715356297854508\n",
            "train loss:0.0032459405250661617\n",
            "train loss:0.016148401781988\n",
            "train loss:0.005518102319847048\n",
            "train loss:0.0004168634123154642\n",
            "train loss:0.0007945031805944793\n",
            "train loss:0.017695516284960924\n",
            "train loss:0.0007764675769246651\n",
            "train loss:0.0046320000322315775\n",
            "train loss:0.0009533005135270424\n",
            "train loss:0.0033361236401598404\n",
            "train loss:0.0026970638402515003\n",
            "train loss:0.0015017553158071352\n",
            "train loss:0.007584389585773581\n",
            "train loss:0.000734948484935064\n",
            "train loss:0.0050557801462088004\n",
            "train loss:0.023652240287018792\n",
            "train loss:0.007537807272562527\n",
            "train loss:0.0035308194615215494\n",
            "train loss:0.011195304409958562\n",
            "train loss:0.001105432365481742\n",
            "train loss:0.0038900332087391863\n",
            "train loss:0.005371540378654738\n",
            "train loss:0.0004815533387879634\n",
            "train loss:0.0007321388764010245\n",
            "train loss:0.0019220175634342878\n",
            "train loss:0.004320320281344002\n",
            "train loss:0.013486553570376892\n",
            "train loss:0.0017624634489955423\n",
            "train loss:0.0006984545211767716\n",
            "train loss:0.005462208115395379\n",
            "train loss:0.00972991147585282\n",
            "train loss:0.0008934212362984886\n",
            "train loss:0.015492618536271721\n",
            "train loss:0.0004178145884176675\n",
            "train loss:0.008685593749225498\n",
            "train loss:0.005638188973086366\n",
            "train loss:0.00402171015785095\n",
            "train loss:0.008822173604056214\n",
            "train loss:0.004446167579838707\n",
            "train loss:0.0025941040454557918\n",
            "train loss:0.0018956102446992017\n",
            "train loss:0.0015302556108947013\n",
            "train loss:0.015012170184420732\n",
            "train loss:0.037148276161308454\n",
            "train loss:0.0033231755721053234\n",
            "train loss:0.0011856450651850342\n",
            "train loss:0.0038376637329188847\n",
            "train loss:0.0038269995286117357\n",
            "train loss:0.02408932385289393\n",
            "train loss:0.0006032559976206471\n",
            "train loss:0.0069770992788676985\n",
            "train loss:0.0011977104145425934\n",
            "train loss:0.005490923950911788\n",
            "train loss:0.027557520041256032\n",
            "train loss:0.0017831689986040535\n",
            "train loss:0.004165572357802922\n",
            "train loss:0.00037070660375255137\n",
            "train loss:0.00045072652989110027\n",
            "train loss:0.0005235632845149337\n",
            "train loss:0.020421920841140927\n",
            "train loss:0.0021585703599485886\n",
            "train loss:0.0026633354207779634\n",
            "train loss:0.005542167603625751\n",
            "train loss:0.0025736334259239225\n",
            "train loss:0.005795492914028233\n",
            "train loss:0.0052054556580192046\n",
            "train loss:0.001020281833497159\n",
            "train loss:0.005347888261760134\n",
            "train loss:0.00023015594350398055\n",
            "train loss:0.0036613221171249233\n",
            "train loss:0.0054172844754662676\n",
            "train loss:0.0022033312088417913\n",
            "train loss:0.006354943831476775\n",
            "train loss:0.0016733793801755681\n",
            "train loss:0.0009462848042627358\n",
            "train loss:0.006110356819406232\n",
            "train loss:0.0012997635483048971\n",
            "train loss:0.001077409916485617\n",
            "train loss:0.0043763983188253915\n",
            "train loss:0.018294440438894964\n",
            "train loss:0.0005235615635013014\n",
            "train loss:0.013083851267089441\n",
            "train loss:0.006180048427493054\n",
            "train loss:0.0033561752682035144\n",
            "train loss:0.016999991416865264\n",
            "train loss:0.0007584924789208166\n",
            "train loss:0.0027721432000339823\n",
            "train loss:0.0013709046177657626\n",
            "train loss:0.010225637541317967\n",
            "train loss:0.0023457789574137695\n",
            "train loss:0.001391250819989243\n",
            "train loss:0.003928162415325292\n",
            "train loss:0.032542988237519624\n",
            "train loss:0.01771377421954485\n",
            "train loss:0.00805690820235085\n",
            "train loss:0.010901085276687239\n",
            "train loss:0.0007019355689493917\n",
            "train loss:0.0028561963559957525\n",
            "train loss:0.003498502076321669\n",
            "train loss:0.009934412701970359\n",
            "train loss:0.002541533574504705\n",
            "train loss:0.0011575327366547179\n",
            "train loss:0.004401140181397159\n",
            "train loss:0.005757897811123596\n",
            "train loss:0.006462321002068187\n",
            "train loss:0.0345961571233014\n",
            "train loss:0.005870553914467759\n",
            "train loss:0.003443208203346761\n",
            "train loss:0.0062263780971047525\n",
            "train loss:0.019375865073142266\n",
            "train loss:0.0021727216008757917\n",
            "train loss:0.0006468652820683186\n",
            "train loss:0.012502137193760232\n",
            "train loss:0.011397886664583759\n",
            "train loss:0.0004075427536152353\n",
            "train loss:0.0045701850593298015\n",
            "train loss:0.007568700548333942\n",
            "train loss:0.003976709332541984\n",
            "train loss:0.005158775725144902\n",
            "train loss:0.0071729042848949496\n",
            "train loss:0.010815613891978224\n",
            "train loss:0.0020748451271066486\n",
            "train loss:0.0005468903333071554\n",
            "train loss:0.005001729090949821\n",
            "train loss:0.005213655979284907\n",
            "train loss:0.022675955019499722\n",
            "train loss:0.020484534508034054\n",
            "train loss:0.009730193010431078\n",
            "train loss:0.0029789462116473707\n",
            "train loss:0.0008619338300508201\n",
            "train loss:0.0015933380569730705\n",
            "train loss:0.0015181161090758275\n",
            "train loss:0.003228754850436291\n",
            "train loss:0.0043563349204430515\n",
            "train loss:0.008286499923043138\n",
            "train loss:0.0008625364452279136\n",
            "train loss:0.0006770165677054273\n",
            "train loss:0.015307679273216279\n",
            "train loss:0.0021631205086715326\n",
            "train loss:0.007796537503458456\n",
            "train loss:0.0030643561851353457\n",
            "train loss:0.0011945166812038378\n",
            "train loss:0.0008458453488910453\n",
            "train loss:0.0029234852790477905\n",
            "train loss:0.0028035802397990014\n",
            "train loss:0.008461944687603676\n",
            "train loss:0.018869769770998676\n",
            "train loss:0.0015414080224538567\n",
            "train loss:0.0012974829895783632\n",
            "train loss:0.005149471820879997\n",
            "train loss:0.007848914127512598\n",
            "train loss:0.002303893975622285\n",
            "train loss:0.006090102834395419\n",
            "train loss:0.004818570428357445\n",
            "train loss:0.0034507302411048916\n",
            "train loss:0.005451641541723447\n",
            "train loss:0.012888646174643081\n",
            "train loss:0.0023676356518844894\n",
            "train loss:0.0028786297287492573\n",
            "train loss:0.0008588996268304803\n",
            "train loss:0.0012265825413336597\n",
            "train loss:0.003591586489515767\n",
            "train loss:0.0038132304649876074\n",
            "train loss:0.0015757580089821972\n",
            "train loss:0.00017501467534209757\n",
            "train loss:0.004314221586431425\n",
            "train loss:0.001580383440820459\n",
            "train loss:0.000974065476044057\n",
            "train loss:0.0015388198123089042\n",
            "train loss:0.0038185965571271435\n",
            "train loss:0.035331425768900686\n",
            "train loss:0.0017433791406286205\n",
            "train loss:0.0019419066019380291\n",
            "train loss:0.00037397468586321427\n",
            "train loss:0.0006970072225017269\n",
            "train loss:0.0027936279135286146\n",
            "train loss:0.0005158673630856957\n",
            "train loss:0.007889479354154405\n",
            "train loss:0.0017023514744633566\n",
            "train loss:0.002897543471698877\n",
            "train loss:0.004048464654043415\n",
            "train loss:0.002006244175568716\n",
            "train loss:0.0019208534293860963\n",
            "train loss:0.00029274716195381006\n",
            "train loss:0.017346536533407755\n",
            "train loss:0.0007477634657044567\n",
            "train loss:0.003783990370094743\n",
            "train loss:0.0006211083858900849\n",
            "train loss:0.0005400167240106411\n",
            "train loss:0.007818847513428933\n",
            "train loss:0.001294285635743149\n",
            "train loss:0.0015219454778888346\n",
            "train loss:0.0006548745948827375\n",
            "train loss:0.006216813476743295\n",
            "train loss:0.0004813748027188442\n",
            "train loss:0.008870034408505013\n",
            "train loss:0.004167440265844548\n",
            "train loss:0.02181517363600979\n",
            "train loss:0.0011349250799769423\n",
            "train loss:0.002529436829064928\n",
            "train loss:0.001051519033761915\n",
            "train loss:0.0006131398823196775\n",
            "train loss:0.0038025904338026472\n",
            "train loss:0.002081417424105813\n",
            "train loss:0.00044022288691991033\n",
            "train loss:0.005799587821093799\n",
            "train loss:0.0008997541958225698\n",
            "train loss:0.005162169890178378\n",
            "train loss:0.001056813814089087\n",
            "train loss:0.0023050324515834827\n",
            "train loss:0.0027916116346870146\n",
            "train loss:0.0004037114236435939\n",
            "train loss:0.02525503187685022\n",
            "train loss:0.00031165028039818743\n",
            "train loss:0.006690386445806262\n",
            "train loss:0.0004241849063939483\n",
            "train loss:0.003060898843744135\n",
            "train loss:0.00662670545684406\n",
            "train loss:0.0007417760197025877\n",
            "train loss:0.0041863704031943625\n",
            "train loss:0.014966299328140168\n",
            "train loss:0.0006178863204248668\n",
            "train loss:0.0016371408915101901\n",
            "train loss:0.0011125919627299962\n",
            "train loss:0.006153770227595998\n",
            "train loss:0.005621476852202128\n",
            "train loss:0.001373171201554612\n",
            "train loss:0.0065060507146816856\n",
            "train loss:0.0037730837951093247\n",
            "train loss:0.006986639570670394\n",
            "train loss:0.0008232340513005647\n",
            "train loss:0.002424923291425039\n",
            "train loss:0.0020241867515773007\n",
            "train loss:0.00011990811839508301\n",
            "train loss:0.002723036061399854\n",
            "train loss:0.005275414609374295\n",
            "train loss:0.0005862227004874328\n",
            "train loss:0.0009675592876798352\n",
            "train loss:0.00029551367688105524\n",
            "train loss:0.0013686847274839073\n",
            "train loss:0.0005141927030114395\n",
            "train loss:0.0024369974186131964\n",
            "train loss:0.005613409219406822\n",
            "train loss:0.0025674686592225666\n",
            "train loss:0.0016218541958958543\n",
            "train loss:0.011359762580756988\n",
            "train loss:0.009489181333864432\n",
            "train loss:0.003169231816963499\n",
            "train loss:0.0019558460030024384\n",
            "train loss:0.007279169756458439\n",
            "train loss:0.0001290445812075538\n",
            "train loss:0.004965764462858506\n",
            "train loss:0.006849122919821854\n",
            "train loss:0.004313361318091312\n",
            "train loss:0.009746598654560377\n",
            "train loss:0.0038055220280866584\n",
            "train loss:0.0037289604642527334\n",
            "train loss:0.0037534779161725097\n",
            "train loss:0.002489811745121181\n",
            "train loss:0.0006665748159555682\n",
            "train loss:0.000945579933656289\n",
            "train loss:0.0004297989248893883\n",
            "train loss:0.005637392391225573\n",
            "train loss:0.004241756845107502\n",
            "train loss:0.0013306574888610743\n",
            "train loss:0.0014647470323826016\n",
            "train loss:0.0025960726377069255\n",
            "train loss:0.00815983904622663\n",
            "train loss:0.014027860418928707\n",
            "train loss:0.0013039969660832234\n",
            "train loss:0.005524876829297709\n",
            "train loss:0.0022235031896130427\n",
            "train loss:0.0068270984557226196\n",
            "train loss:0.001288811628946438\n",
            "train loss:7.925487508829519e-05\n",
            "train loss:0.011835016578659254\n",
            "train loss:0.053136012937028056\n",
            "train loss:0.00579553911691262\n",
            "train loss:0.031910301690728164\n",
            "train loss:0.0025089587323665168\n",
            "train loss:0.015420095513844007\n",
            "train loss:0.00526793957253343\n",
            "train loss:0.003402025323353202\n",
            "train loss:0.005419230033004863\n",
            "train loss:0.007239821829303632\n",
            "train loss:0.0007839945963372234\n",
            "train loss:0.0026863622852290846\n",
            "train loss:0.009643522784056358\n",
            "train loss:0.03508231619645536\n",
            "train loss:0.018178004021707102\n",
            "train loss:0.001882524511959643\n",
            "train loss:0.007335074331022989\n",
            "train loss:0.004645095495884355\n",
            "train loss:0.021978853895109705\n",
            "train loss:0.001932468707697628\n",
            "train loss:0.01160651158825072\n",
            "train loss:0.0014626880886568444\n",
            "train loss:0.0009790073255502604\n",
            "train loss:0.015481660182380888\n",
            "train loss:0.0013233249850374904\n",
            "train loss:0.003028447242933353\n",
            "train loss:0.005956334650011508\n",
            "train loss:0.005563394320083729\n",
            "train loss:0.00044110201625492814\n",
            "train loss:0.004298047677228125\n",
            "train loss:0.0021019667769545405\n",
            "train loss:0.004555152046602868\n",
            "train loss:0.0015381694344302871\n",
            "train loss:0.0016784037603204116\n",
            "train loss:0.01012258724314755\n",
            "train loss:0.022741988103060364\n",
            "train loss:0.0045682906227854075\n",
            "train loss:0.00016914663711724713\n",
            "train loss:0.0005254141028514624\n",
            "train loss:0.000547282235116997\n",
            "train loss:0.000715023551966286\n",
            "train loss:0.019027711164805793\n",
            "train loss:0.0034273924882644944\n",
            "train loss:0.003651351548199903\n",
            "train loss:0.00392279337288788\n",
            "train loss:0.002553599831380516\n",
            "train loss:0.003942211730950612\n",
            "train loss:0.004745330764060727\n",
            "train loss:0.002975146952484251\n",
            "train loss:0.0020626894412061596\n",
            "train loss:0.0035794027207899986\n",
            "train loss:0.004862286217683481\n",
            "train loss:0.011296588799461186\n",
            "train loss:0.0034524624506153626\n",
            "train loss:0.0006683205377051333\n",
            "train loss:0.014041520668163483\n",
            "train loss:0.00218939871441455\n",
            "train loss:0.0038369697298016032\n",
            "train loss:0.003644429300760749\n",
            "train loss:0.0038166363257729906\n",
            "train loss:0.001972515992794272\n",
            "train loss:0.0002121537285567243\n",
            "train loss:0.0027245282841133036\n",
            "train loss:0.0015867307390649368\n",
            "train loss:0.0011797992271353236\n",
            "train loss:0.0007785668957182285\n",
            "train loss:0.0050429331299320515\n",
            "train loss:0.003890930108299616\n",
            "train loss:0.005405687905447968\n",
            "train loss:0.005167089411071577\n",
            "train loss:0.005899214298138313\n",
            "train loss:0.00025671503076204864\n",
            "train loss:0.029357792248521437\n",
            "train loss:0.008325083464921182\n",
            "train loss:0.009723294848465038\n",
            "train loss:0.0012208474513406384\n",
            "train loss:0.005027060412463103\n",
            "train loss:0.00974324906090712\n",
            "train loss:0.0008685111565867795\n",
            "train loss:0.0016150956989952486\n",
            "train loss:0.004896832806408173\n",
            "train loss:0.002193698298386495\n",
            "train loss:0.0006967647286840506\n",
            "train loss:0.0034311852142072767\n",
            "train loss:0.007264215163808621\n",
            "train loss:0.0028631808944512348\n",
            "train loss:0.005305638134955833\n",
            "train loss:0.0033209363464170357\n",
            "train loss:0.0019051845286650205\n",
            "train loss:0.010377532068166011\n",
            "train loss:0.002282383229718799\n",
            "train loss:0.0002942630170152795\n",
            "train loss:0.010965387081780315\n",
            "train loss:0.0027353748713365546\n",
            "train loss:0.0004078435802196207\n",
            "train loss:0.002389149020862515\n",
            "train loss:0.005565095214667918\n",
            "train loss:0.0037788788972299807\n",
            "train loss:0.001119019669353821\n",
            "train loss:0.01650445818051419\n",
            "train loss:0.010758572585857667\n",
            "train loss:0.004787551341740531\n",
            "train loss:0.017221435615961674\n",
            "train loss:0.006330728354242521\n",
            "train loss:0.007342554672601352\n",
            "train loss:0.002417398165609129\n",
            "train loss:0.00020536928241862828\n",
            "train loss:0.0028944450923386305\n",
            "train loss:0.0010306581789609511\n",
            "train loss:0.0034996528768084525\n",
            "train loss:0.0021880758101588856\n",
            "train loss:0.0006227615602813494\n",
            "train loss:0.0006636789135435614\n",
            "train loss:0.00390871837551251\n",
            "train loss:0.0013520358562054955\n",
            "train loss:0.012057281286428185\n",
            "train loss:0.0024256513277715512\n",
            "train loss:0.00012051979088344247\n",
            "train loss:0.0010965170293656358\n",
            "train loss:0.0018422369021251\n",
            "train loss:0.0004330853670551778\n",
            "train loss:0.0017068643716096525\n",
            "train loss:0.004117317079488955\n",
            "train loss:0.0014452102885681472\n",
            "train loss:0.006093211778998253\n",
            "train loss:0.002434212657030891\n",
            "train loss:0.0008305512308144583\n",
            "train loss:0.00447553075469719\n",
            "train loss:0.006862813898093663\n",
            "train loss:0.0008118894225087136\n",
            "train loss:0.00023654593544007483\n",
            "train loss:0.013444491513601728\n",
            "train loss:0.01613871945045449\n",
            "train loss:0.002637035747706181\n",
            "train loss:0.0016021877698552233\n",
            "train loss:0.008815260299552482\n",
            "train loss:0.0007382975979908546\n",
            "train loss:0.005769600051527281\n",
            "train loss:0.004153213927566642\n",
            "train loss:0.0068711206565441\n",
            "train loss:0.0015116037276559522\n",
            "train loss:0.007163961513843957\n",
            "train loss:0.0012792573455661183\n",
            "train loss:0.0030110746321160165\n",
            "train loss:0.0023360934898246123\n",
            "train loss:0.003516229055235643\n",
            "train loss:0.001056011246805785\n",
            "train loss:0.00016787770123051545\n",
            "train loss:0.0011595064640171206\n",
            "train loss:0.00012278410982208184\n",
            "train loss:0.00034783317455171827\n",
            "train loss:0.0037896692308818304\n",
            "train loss:0.00470281085556169\n",
            "train loss:0.0011062878777788995\n",
            "train loss:0.0012989100271215187\n",
            "train loss:0.01252334880301962\n",
            "train loss:0.0010222948123989534\n",
            "train loss:0.0010585117705913675\n",
            "train loss:0.001986622376166091\n",
            "train loss:0.003996576949319665\n",
            "train loss:0.0005979147421948064\n",
            "train loss:0.0015529524005600311\n",
            "train loss:0.006110114314498894\n",
            "train loss:0.0017852096613998167\n",
            "train loss:0.00418337505025036\n",
            "train loss:0.000958061343396887\n",
            "train loss:0.00019359761090739555\n",
            "train loss:0.0007754916961023406\n",
            "train loss:0.0006606833751133584\n",
            "train loss:0.0009712064045659476\n",
            "train loss:0.004267685991680971\n",
            "train loss:0.004907872694591629\n",
            "train loss:0.004736152857920115\n",
            "train loss:0.0025456476086825764\n",
            "train loss:0.004564403096435022\n",
            "train loss:0.0038267866223122855\n",
            "train loss:0.00039620972552477786\n",
            "train loss:0.0027231075871110284\n",
            "train loss:0.000920155468433236\n",
            "train loss:0.0023715757040394126\n",
            "train loss:0.00585155366988827\n",
            "train loss:0.013030841039321344\n",
            "train loss:8.004876817305353e-05\n",
            "train loss:0.0012252864326576006\n",
            "train loss:0.0007336288967823415\n",
            "train loss:0.0032041665599774204\n",
            "train loss:0.0008478895256110648\n",
            "train loss:0.0015204703114201652\n",
            "train loss:0.0037072470859245434\n",
            "train loss:0.0009501328653846562\n",
            "train loss:0.001781217962110848\n",
            "train loss:0.000892873155423938\n",
            "train loss:0.002100699973711099\n",
            "train loss:0.0012572884010609744\n",
            "train loss:0.0028746196624802327\n",
            "train loss:0.0022355473692582057\n",
            "train loss:0.0005275468862460902\n",
            "train loss:0.003875957312957568\n",
            "train loss:0.019367160547261775\n",
            "train loss:0.0010701007765600316\n",
            "train loss:0.0003556079505652392\n",
            "train loss:0.0007064107969887352\n",
            "train loss:0.00585035938511739\n",
            "train loss:0.0025489484333513367\n",
            "train loss:0.0008116328527115363\n",
            "train loss:0.003331883794364269\n",
            "train loss:0.00043662521746594353\n",
            "train loss:0.0030568990299586728\n",
            "train loss:6.339347756553659e-05\n",
            "train loss:0.0002966912077314143\n",
            "train loss:0.0009404128130483711\n",
            "train loss:0.01094302389944357\n",
            "train loss:0.011696622282271998\n",
            "train loss:0.0019708323599653584\n",
            "train loss:0.0008812398905321658\n",
            "train loss:0.003329927863194004\n",
            "train loss:0.0033666681108645365\n",
            "train loss:0.00230528271496197\n",
            "train loss:0.003073052891125335\n",
            "train loss:0.0032128635406590213\n",
            "train loss:0.0007901252180632197\n",
            "train loss:0.0006590307275692251\n",
            "train loss:0.0010835207864896227\n",
            "train loss:0.001539839299867193\n",
            "train loss:0.002564241000487603\n",
            "train loss:0.0015440628252979185\n",
            "train loss:0.0018050497072571092\n",
            "train loss:0.007058877378705133\n",
            "train loss:0.00045019984143403885\n",
            "train loss:0.004082520632507928\n",
            "train loss:0.0026742190007477723\n",
            "train loss:0.000502152220812869\n",
            "train loss:0.0001404360565150215\n",
            "train loss:0.00036125739857656485\n",
            "train loss:0.00630081965986744\n",
            "train loss:0.0016841681233433195\n",
            "train loss:0.0018728090797704396\n",
            "train loss:0.004308189607563307\n",
            "train loss:0.004490081677202342\n",
            "train loss:0.002318046770661257\n",
            "train loss:0.0009069885475135376\n",
            "train loss:0.005458390754850579\n",
            "train loss:0.012889237204231891\n",
            "train loss:0.009074660365872136\n",
            "train loss:0.0006660884731160109\n",
            "train loss:0.0040228759306894695\n",
            "train loss:0.009748013432333676\n",
            "train loss:0.0005777933921930448\n",
            "train loss:0.0007995566007463267\n",
            "train loss:0.0010502394431353636\n",
            "train loss:0.0054772585454346\n",
            "train loss:0.007059483317373937\n",
            "train loss:0.0011263014958956643\n",
            "train loss:0.0004965097858754426\n",
            "train loss:0.0033341764053137785\n",
            "train loss:0.00013981383934716615\n",
            "train loss:0.0021113126060712667\n",
            "train loss:0.002204245536964263\n",
            "train loss:0.0006760788363243378\n",
            "train loss:0.00022682329579514076\n",
            "train loss:0.00014804693959742473\n",
            "train loss:0.00047727735526647446\n",
            "train loss:0.0004911290655049673\n",
            "train loss:0.0031497011425702815\n",
            "train loss:0.002149965854561167\n",
            "train loss:0.0010164445796692003\n",
            "train loss:0.0013555390005200601\n",
            "train loss:0.0012233963442648274\n",
            "train loss:0.00018351258441769904\n",
            "train loss:0.005116340590835166\n",
            "train loss:0.030096509992273188\n",
            "train loss:0.00259844397862028\n",
            "train loss:0.002448753165215213\n",
            "train loss:0.00359180390713971\n",
            "train loss:0.0011838381608807865\n",
            "train loss:0.0006752435875796671\n",
            "train loss:0.0008615275016716326\n",
            "train loss:0.0004004977084490914\n",
            "train loss:0.00511865404465273\n",
            "train loss:0.0009496415661325612\n",
            "train loss:0.0010225433954876724\n",
            "train loss:0.0008411996726239072\n",
            "train loss:0.004030266282463277\n",
            "train loss:0.000842735243085969\n",
            "train loss:0.015605643649496695\n",
            "train loss:0.00025800437384677254\n",
            "train loss:0.0005069470347975822\n",
            "=== epoch:15, train acc:0.997, test acc:0.989 ===\n",
            "train loss:0.0009769395657372728\n",
            "train loss:0.0021641424679012993\n",
            "train loss:0.0012143030250810624\n",
            "train loss:0.004723356423970315\n",
            "train loss:0.0010575168443077614\n",
            "train loss:0.002340826149065034\n",
            "train loss:0.01222227461673155\n",
            "train loss:0.0255447712104112\n",
            "train loss:0.000723013672507564\n",
            "train loss:0.0013886734906979454\n",
            "train loss:0.001869246044513056\n",
            "train loss:0.0024928295462853522\n",
            "train loss:0.0036126024888095552\n",
            "train loss:0.0035708293922530198\n",
            "train loss:0.01984346469955954\n",
            "train loss:0.005936552386519225\n",
            "train loss:0.0034282036937467804\n",
            "train loss:0.002700181165947553\n",
            "train loss:0.00258255337902151\n",
            "train loss:0.0020504057974122993\n",
            "train loss:0.010004504964887495\n",
            "train loss:0.01324981295704674\n",
            "train loss:0.0031246584740961206\n",
            "train loss:0.0047396183044862\n",
            "train loss:0.0038365720258353997\n",
            "train loss:0.005326923353485589\n",
            "train loss:0.007338911373415767\n",
            "train loss:0.0003158976179677423\n",
            "train loss:0.005432504409480651\n",
            "train loss:0.002180028263110026\n",
            "train loss:0.0035546329979913825\n",
            "train loss:0.002475820097559449\n",
            "train loss:0.0004100452464897285\n",
            "train loss:0.0015017424432786867\n",
            "train loss:0.0013388115578550342\n",
            "train loss:0.001988212518423731\n",
            "train loss:0.0004202156586983418\n",
            "train loss:0.003419207622389535\n",
            "train loss:0.0005475790781624915\n",
            "train loss:0.006491125771098106\n",
            "train loss:0.002893665571607335\n",
            "train loss:0.0033647931665696292\n",
            "train loss:0.001797600283748321\n",
            "train loss:0.00044949487604673785\n",
            "train loss:0.0009504418849200938\n",
            "train loss:0.0014619662087712317\n",
            "train loss:0.00022536467704105972\n",
            "train loss:0.003354862656728452\n",
            "train loss:0.0015904767198775508\n",
            "train loss:0.008633173463742604\n",
            "train loss:0.003789947030719093\n",
            "train loss:0.0011021623219778923\n",
            "train loss:0.0012983271601359322\n",
            "train loss:0.00537472193921796\n",
            "train loss:0.0009120996215156511\n",
            "train loss:0.001842501417446644\n",
            "train loss:0.0005007736223325538\n",
            "train loss:0.0008069805638627098\n",
            "train loss:0.002055152968809798\n",
            "train loss:0.00044393299649348605\n",
            "train loss:0.0008948135602863546\n",
            "train loss:0.0008932529855611092\n",
            "train loss:0.0002267416786071332\n",
            "train loss:0.0013966334827776774\n",
            "train loss:0.0020852941861914066\n",
            "train loss:0.002590197394765223\n",
            "train loss:0.0008444179475437121\n",
            "train loss:0.003938073447140171\n",
            "train loss:0.001510087293066703\n",
            "train loss:0.004681827822354524\n",
            "train loss:0.0022790049892690155\n",
            "train loss:0.0009648638214561798\n",
            "train loss:0.0027576223210734247\n",
            "train loss:0.002249950536915932\n",
            "train loss:0.07006175742148396\n",
            "train loss:0.0005278479994616736\n",
            "train loss:0.0004948357569912123\n",
            "train loss:0.0012782180873591288\n",
            "train loss:0.00021231233750414294\n",
            "train loss:0.0008606802185945577\n",
            "train loss:0.00351200312832123\n",
            "train loss:0.005168337315543413\n",
            "train loss:0.001523166208393789\n",
            "train loss:0.0008081003254260269\n",
            "train loss:0.0028797979162638577\n",
            "train loss:0.0013662172734512218\n",
            "train loss:0.0004274425592265117\n",
            "train loss:0.01108860606711889\n",
            "train loss:0.000601497292793049\n",
            "train loss:0.00018978926279963274\n",
            "train loss:0.00038688531791234977\n",
            "train loss:0.0010979282753820034\n",
            "train loss:0.00031472482374853374\n",
            "train loss:0.002275653213912159\n",
            "train loss:0.00031852220145596585\n",
            "train loss:0.002868295676387256\n",
            "train loss:0.001049143181457177\n",
            "train loss:0.0018543208259389518\n",
            "train loss:0.00010892003379109378\n",
            "train loss:0.006220387258294082\n",
            "train loss:0.002110591480312142\n",
            "train loss:0.008292322687000064\n",
            "train loss:0.0015308569191208935\n",
            "train loss:0.002062472118790322\n",
            "train loss:0.001285212539662969\n",
            "train loss:0.002859294098490176\n",
            "train loss:0.00011865921989799863\n",
            "train loss:0.001386195806323102\n",
            "train loss:0.0007308143006943909\n",
            "train loss:0.00014052384192697056\n",
            "train loss:0.005256053880112816\n",
            "train loss:0.0021452530974452764\n",
            "train loss:0.0026917269090343194\n",
            "train loss:0.00024001719748792594\n",
            "train loss:0.0023836190686036774\n",
            "train loss:0.00039591676162407615\n",
            "train loss:0.00027277234847092357\n",
            "train loss:0.0007874031686401924\n",
            "train loss:0.002306344228211056\n",
            "train loss:0.015011891591527713\n",
            "train loss:0.0005012898549780697\n",
            "train loss:0.0021814613454785763\n",
            "train loss:0.0003811858206882533\n",
            "train loss:0.0010177803892327088\n",
            "train loss:0.003556424573377596\n",
            "train loss:0.0021599635627636503\n",
            "train loss:0.0010640833127278843\n",
            "train loss:0.002371019973768532\n",
            "train loss:0.004170460691269518\n",
            "train loss:0.0017689153800900296\n",
            "train loss:0.0004700574451269666\n",
            "train loss:0.004588849217700666\n",
            "train loss:0.0011564397783275556\n",
            "train loss:0.015496730274212073\n",
            "train loss:0.0016815908702798002\n",
            "train loss:0.0009005274792575035\n",
            "train loss:0.0021343185680703456\n",
            "train loss:0.01502574745575281\n",
            "train loss:0.002937528901396801\n",
            "train loss:0.004581472121644833\n",
            "train loss:0.0005484410050169487\n",
            "train loss:0.0002343438599968551\n",
            "train loss:0.0009467205217622798\n",
            "train loss:0.006769038114167909\n",
            "train loss:0.0008255430808094523\n",
            "train loss:0.008742547719627323\n",
            "train loss:0.000892468454206678\n",
            "train loss:0.0007536465204009883\n",
            "train loss:0.0070058700763986894\n",
            "train loss:0.012161822280055802\n",
            "train loss:0.00232187936236595\n",
            "train loss:0.002770443006393837\n",
            "train loss:0.0014963173005946215\n",
            "train loss:0.0009628889834527279\n",
            "train loss:0.001925678375486129\n",
            "train loss:0.005207714546024656\n",
            "train loss:0.00037592798086840366\n",
            "train loss:0.001097461858035023\n",
            "train loss:0.00040188392111093195\n",
            "train loss:0.0022352494931084915\n",
            "train loss:0.011608889510880816\n",
            "train loss:0.0008771712805230121\n",
            "train loss:0.0019189295865275468\n",
            "train loss:0.0022326837872341985\n",
            "train loss:0.000506938954776938\n",
            "train loss:0.019201856835438572\n",
            "train loss:0.0032560481181833113\n",
            "train loss:0.005878589358620136\n",
            "train loss:0.03746985527897494\n",
            "train loss:0.0021812079210674095\n",
            "train loss:0.0011191665871349646\n",
            "train loss:0.00044673655953370625\n",
            "train loss:0.007104924308356284\n",
            "train loss:0.001734529518828076\n",
            "train loss:0.0017651559340392548\n",
            "train loss:0.0003751912437962317\n",
            "train loss:0.0012848239394793164\n",
            "train loss:0.0020617342831743364\n",
            "train loss:0.0030138571212854313\n",
            "train loss:0.001169310209292217\n",
            "train loss:0.0004413237877127038\n",
            "train loss:0.00510724540407001\n",
            "train loss:0.004759730757923949\n",
            "train loss:0.0009752095425492987\n",
            "train loss:0.0025011099735663777\n",
            "train loss:0.0006880658990945867\n",
            "train loss:0.0024631971803538337\n",
            "train loss:0.0022970208284882956\n",
            "train loss:0.005823648767976554\n",
            "train loss:0.0013324054058059382\n",
            "train loss:0.008639372313702843\n",
            "train loss:0.017940245432771017\n",
            "train loss:0.0009596316810191286\n",
            "train loss:0.0006625678657510207\n",
            "train loss:0.003268097619872094\n",
            "train loss:0.01315066045794196\n",
            "train loss:0.0030128449465117225\n",
            "train loss:0.007001764782280238\n",
            "train loss:0.0008978928529123879\n",
            "train loss:0.0031087836475696086\n",
            "train loss:0.00024031443712022802\n",
            "train loss:0.0007150195190511296\n",
            "train loss:0.009353616839355487\n",
            "train loss:0.00612917223609632\n",
            "train loss:0.014012575514168091\n",
            "train loss:0.006121304509886539\n",
            "train loss:0.00037987888050344554\n",
            "train loss:0.000268940515535438\n",
            "train loss:0.001870109297898506\n",
            "train loss:0.001681871751074676\n",
            "train loss:0.001169560871067367\n",
            "train loss:0.000636246543962701\n",
            "train loss:0.0051966732559315535\n",
            "train loss:0.0007376766679166391\n",
            "train loss:0.008934403857627624\n",
            "train loss:0.00115681076784056\n",
            "train loss:0.00025366755415528433\n",
            "train loss:0.003879297623466986\n",
            "train loss:0.010350508445525984\n",
            "train loss:0.006072602780668009\n",
            "train loss:0.004462232913881881\n",
            "train loss:0.003920347072009061\n",
            "train loss:0.003590904617818865\n",
            "train loss:0.02016910601336307\n",
            "train loss:0.007783802965016976\n",
            "train loss:0.0020245035270235498\n",
            "train loss:0.009333157804160525\n",
            "train loss:0.00033327427616111816\n",
            "train loss:5.692952603958471e-05\n",
            "train loss:0.00022302632083638218\n",
            "train loss:0.0015421895778190446\n",
            "train loss:0.0004998120867798286\n",
            "train loss:0.004096662267318096\n",
            "train loss:0.003152742413253522\n",
            "train loss:0.005672296072900984\n",
            "train loss:0.005060125717104087\n",
            "train loss:0.007868546554965678\n",
            "train loss:0.004123605603587001\n",
            "train loss:0.009836256877434505\n",
            "train loss:0.002174233978867655\n",
            "train loss:0.0001471459735263564\n",
            "train loss:0.0131062346720821\n",
            "train loss:0.005333138095107045\n",
            "train loss:0.0006347115398821665\n",
            "train loss:0.013344047928219133\n",
            "train loss:0.002224664506713284\n",
            "train loss:0.001026014553549526\n",
            "train loss:0.0017335443420916886\n",
            "train loss:0.0026552481737173008\n",
            "train loss:0.0014202639408043137\n",
            "train loss:0.0010980614202515853\n",
            "train loss:0.0017546263206405857\n",
            "train loss:0.004190779686831759\n",
            "train loss:0.0006590137509614627\n",
            "train loss:0.0046703512095711336\n",
            "train loss:0.0015022511001423228\n",
            "train loss:0.010547377851227318\n",
            "train loss:0.000545744502101048\n",
            "train loss:0.005730728709858151\n",
            "train loss:0.002165888666957389\n",
            "train loss:0.00842433134934553\n",
            "train loss:0.0011927638268775806\n",
            "train loss:0.014089785302517784\n",
            "train loss:0.00048318661428405193\n",
            "train loss:0.0003620428690401349\n",
            "train loss:0.006473721235708085\n",
            "train loss:0.0006330554141933595\n",
            "train loss:0.00019628859702638016\n",
            "train loss:0.0003757370609755423\n",
            "train loss:0.0008103551253866667\n",
            "train loss:0.0036782105109283104\n",
            "train loss:0.001984565375382463\n",
            "train loss:0.005673818311525863\n",
            "train loss:0.0012368969203525395\n",
            "train loss:0.006475025608648273\n",
            "train loss:0.036119417768866666\n",
            "train loss:0.00076660468819577\n",
            "train loss:0.004493623740904956\n",
            "train loss:0.00020526202649758904\n",
            "train loss:0.004539081894928518\n",
            "train loss:0.0035155281118565505\n",
            "train loss:0.0008630210792158066\n",
            "train loss:0.0006016462456795564\n",
            "train loss:0.0006019288206347195\n",
            "train loss:0.0008561870008892303\n",
            "train loss:0.001041780336657625\n",
            "train loss:0.007710468501464206\n",
            "train loss:0.0023273674514743582\n",
            "train loss:0.0007035515638141296\n",
            "train loss:0.006191680765285123\n",
            "train loss:0.0007125038779487535\n",
            "train loss:0.0006698323445138327\n",
            "train loss:2.2414919510575607e-05\n",
            "train loss:0.0007997860658682902\n",
            "train loss:0.0004390511911447747\n",
            "train loss:0.0003319714571627769\n",
            "train loss:0.0024836599298809817\n",
            "train loss:0.03698079592642\n",
            "train loss:0.0002487309706115552\n",
            "train loss:0.0006729812372420063\n",
            "train loss:0.0055003194657512015\n",
            "train loss:0.0025260267913955695\n",
            "train loss:0.0029457089205365433\n",
            "train loss:0.004385319440299841\n",
            "train loss:0.013078507245113122\n",
            "train loss:0.0010509755790126293\n",
            "train loss:0.008142384512978797\n",
            "train loss:0.0006346327171691758\n",
            "train loss:0.004234761949223986\n",
            "train loss:0.0021481666768841286\n",
            "train loss:0.03251128272666559\n",
            "train loss:0.0031395954955669146\n",
            "train loss:0.00036420387723204916\n",
            "train loss:0.0026075478903383686\n",
            "train loss:0.00043292395557147865\n",
            "train loss:0.00027452070543293656\n",
            "train loss:0.0009124485196231678\n",
            "train loss:0.0004094260887555229\n",
            "train loss:0.015446716093005367\n",
            "train loss:0.007442431283389831\n",
            "train loss:0.0032328374202339537\n",
            "train loss:0.0013489226288247774\n",
            "train loss:0.0018944176685824574\n",
            "train loss:0.0018435088820312128\n",
            "train loss:0.0003160443190277962\n",
            "train loss:0.0005338654671448277\n",
            "train loss:0.0019978345561140532\n",
            "train loss:0.0031336646434902745\n",
            "train loss:0.0014063739320405591\n",
            "train loss:0.0007092202740375897\n",
            "train loss:0.0004006356424308464\n",
            "train loss:0.0021669435448084372\n",
            "train loss:0.0025033185834807525\n",
            "train loss:0.00031577044355491415\n",
            "train loss:0.0014469693949518685\n",
            "train loss:0.0016629624279251545\n",
            "train loss:0.001431547631748276\n",
            "train loss:0.003054496528947744\n",
            "train loss:0.00026601467677697725\n",
            "train loss:0.0005513025822201418\n",
            "train loss:0.001998475048867437\n",
            "train loss:0.0008512958679985988\n",
            "train loss:0.0030386563485338735\n",
            "train loss:0.0041601625876049675\n",
            "train loss:0.005807026571478112\n",
            "train loss:0.0006024977049958216\n",
            "train loss:0.0004450794270528991\n",
            "train loss:0.004187958334782344\n",
            "train loss:0.0015743637389026113\n",
            "train loss:0.0007716201826876533\n",
            "train loss:0.000249331325908953\n",
            "train loss:0.0009191878443338855\n",
            "train loss:0.0010094656628415262\n",
            "train loss:0.00040572429919102286\n",
            "train loss:0.0009448305152098427\n",
            "train loss:0.003979257251717384\n",
            "train loss:0.002335131938352546\n",
            "train loss:0.0007279396623249185\n",
            "train loss:0.00022822652242000002\n",
            "train loss:0.004662642009155491\n",
            "train loss:0.00012819118861324866\n",
            "train loss:0.0018557445981920223\n",
            "train loss:0.006555477073363902\n",
            "train loss:0.003139148411574207\n",
            "train loss:0.005498435733419496\n",
            "train loss:0.0021441957396480314\n",
            "train loss:0.0006539112213075437\n",
            "train loss:0.0014205545416689377\n",
            "train loss:0.0008535946451295719\n",
            "train loss:4.129280946631764e-05\n",
            "train loss:0.004360379024036185\n",
            "train loss:0.0003665420418201245\n",
            "train loss:0.0003377442684948766\n",
            "train loss:0.001749505385269645\n",
            "train loss:0.0020525912306084758\n",
            "train loss:0.005275718573749886\n",
            "train loss:0.005825596657200362\n",
            "train loss:0.004955741829666425\n",
            "train loss:0.0012335481302942083\n",
            "train loss:0.0021639558894276105\n",
            "train loss:0.008140742570328884\n",
            "train loss:0.00028771815215662255\n",
            "train loss:0.0006690692907969304\n",
            "train loss:0.004758937908161121\n",
            "train loss:0.011886820998482726\n",
            "train loss:0.001978554981475009\n",
            "train loss:0.009492008551092888\n",
            "train loss:0.012765373661085067\n",
            "train loss:0.0018760012967673153\n",
            "train loss:0.005416564439042737\n",
            "train loss:0.002096580123956437\n",
            "train loss:0.006201088571849187\n",
            "train loss:0.0033844735644114415\n",
            "train loss:0.00040492030514616633\n",
            "train loss:0.00035050485766287666\n",
            "train loss:0.0021522116583067063\n",
            "train loss:0.005011095472227004\n",
            "train loss:0.0020488188829843688\n",
            "train loss:0.00032825891810594283\n",
            "train loss:0.006434275738762137\n",
            "train loss:0.0001717588197294151\n",
            "train loss:0.002580466712524201\n",
            "train loss:0.0008355595316168237\n",
            "train loss:0.0029615487966577207\n",
            "train loss:0.0060866535985652755\n",
            "train loss:0.002621042268739441\n",
            "train loss:0.006136060172932291\n",
            "train loss:0.0004577492514893015\n",
            "train loss:0.014485829913197584\n",
            "train loss:0.0043165344251268065\n",
            "train loss:0.0027532099932446935\n",
            "train loss:0.0010070308594832562\n",
            "train loss:0.0013889286042581487\n",
            "train loss:0.00018891678961941623\n",
            "train loss:0.0001637876481614689\n",
            "train loss:0.010625381628579636\n",
            "train loss:0.0012293229186188372\n",
            "train loss:0.005142210676390916\n",
            "train loss:0.0013354622395358431\n",
            "train loss:0.0010106856984759146\n",
            "train loss:0.0002499662389618701\n",
            "train loss:0.0025756734366328914\n",
            "train loss:0.0006862661898782574\n",
            "train loss:0.0009538266116873821\n",
            "train loss:0.00036081588381582473\n",
            "train loss:0.0018975810508928964\n",
            "train loss:0.0009157846875920975\n",
            "train loss:0.0006103692856330445\n",
            "train loss:0.0013309361724108564\n",
            "train loss:0.0003736216917449016\n",
            "train loss:0.0008418340215348755\n",
            "train loss:0.0036627758993955178\n",
            "train loss:0.0006799016830735111\n",
            "train loss:0.002839610131602148\n",
            "train loss:0.0020504541076398737\n",
            "train loss:0.0004025289381043342\n",
            "train loss:0.0003265547958113148\n",
            "train loss:0.0016476005664461419\n",
            "train loss:0.007480324973889818\n",
            "train loss:0.0009379080086826926\n",
            "train loss:0.0006845449463850577\n",
            "train loss:0.0019965849187569985\n",
            "train loss:0.00017374936784532213\n",
            "train loss:0.004811414338148703\n",
            "train loss:0.0002567301098199085\n",
            "train loss:0.0006109225891579106\n",
            "train loss:0.0010717399973681379\n",
            "train loss:0.0008924621246166407\n",
            "train loss:0.0002789680177227412\n",
            "train loss:0.0006149088222168924\n",
            "train loss:0.0005699123938963618\n",
            "train loss:0.0005317872929121296\n",
            "train loss:0.014980968804337422\n",
            "train loss:0.002461422763926719\n",
            "train loss:0.0025037925239483464\n",
            "train loss:0.001275644998888438\n",
            "train loss:0.002645759180349632\n",
            "train loss:0.08125474509926855\n",
            "train loss:0.005235051737933832\n",
            "train loss:0.008010056689859116\n",
            "train loss:0.010569251404541123\n",
            "train loss:0.0023679916248892726\n",
            "train loss:0.007629354839379519\n",
            "train loss:0.002957851525642596\n",
            "train loss:0.0006125926365057793\n",
            "train loss:5.0550872023694236e-05\n",
            "train loss:0.003946734426384433\n",
            "train loss:0.0021268041964913635\n",
            "train loss:0.001117514646476996\n",
            "train loss:0.0014904405410700319\n",
            "train loss:0.0003753267739432552\n",
            "train loss:0.001292266882003158\n",
            "train loss:8.873572954382261e-05\n",
            "train loss:0.0023477510668878203\n",
            "train loss:0.008679659157078119\n",
            "train loss:0.0002634775824593475\n",
            "train loss:0.0004096159476421805\n",
            "train loss:0.0032897459352798724\n",
            "train loss:0.0011026303797670978\n",
            "train loss:0.004658274906860362\n",
            "train loss:0.0009749895482204199\n",
            "train loss:0.0015404028371793074\n",
            "train loss:0.0031155960437051567\n",
            "train loss:0.0049481887261643345\n",
            "train loss:0.00044434738214220687\n",
            "train loss:0.007569650348239916\n",
            "train loss:0.0004841862016045448\n",
            "train loss:0.0009210860616104956\n",
            "train loss:0.004698040612668029\n",
            "train loss:0.031055033219972102\n",
            "train loss:0.0014631802431469547\n",
            "train loss:7.723860692963054e-05\n",
            "train loss:0.0017769274870795043\n",
            "train loss:0.011476278161850269\n",
            "train loss:0.0023925730727926645\n",
            "train loss:0.00022010696640419134\n",
            "train loss:0.012530001209996906\n",
            "train loss:0.007871072555151952\n",
            "train loss:0.0009169533127867908\n",
            "train loss:0.0016027367755574776\n",
            "train loss:0.0002043851431181808\n",
            "train loss:0.0009361233224945389\n",
            "train loss:0.0001773747317101395\n",
            "train loss:0.0033244993113696296\n",
            "train loss:0.0005190098426336332\n",
            "train loss:8.864782587379407e-05\n",
            "train loss:0.001843667240371824\n",
            "train loss:0.0015616190636354261\n",
            "train loss:0.0023294837183332633\n",
            "train loss:0.0059575708758501655\n",
            "train loss:8.699586377138939e-05\n",
            "train loss:0.0009107671163828436\n",
            "train loss:0.0010457119808280472\n",
            "train loss:0.0031119851492842493\n",
            "train loss:7.141487820261061e-05\n",
            "train loss:0.005347353302768654\n",
            "train loss:0.00261046797614807\n",
            "train loss:0.00031126918081092583\n",
            "train loss:0.0006120676570009695\n",
            "train loss:0.006865358704968939\n",
            "train loss:0.005888512338523931\n",
            "train loss:0.002614416018782819\n",
            "train loss:0.000790981946272613\n",
            "train loss:0.0030869155937472033\n",
            "train loss:0.0007324549126667893\n",
            "train loss:0.004378647792940418\n",
            "train loss:0.007580865318588787\n",
            "train loss:0.0009686058760642156\n",
            "train loss:0.0015163192847639617\n",
            "train loss:0.00045515366329900516\n",
            "train loss:0.0021856525927217894\n",
            "train loss:0.0001352361610538873\n",
            "train loss:0.0007531519476942594\n",
            "train loss:0.010464197750228485\n",
            "train loss:0.0013339638338077378\n",
            "train loss:0.017444964402747714\n",
            "train loss:0.011385990960416982\n",
            "train loss:0.000127481618505893\n",
            "train loss:0.0012635025082816573\n",
            "train loss:9.055862059963064e-05\n",
            "train loss:0.0006417157238168563\n",
            "train loss:0.0014775162104413639\n",
            "train loss:0.0003376724167412888\n",
            "train loss:0.0012764784038367591\n",
            "train loss:0.0022552949943384203\n",
            "train loss:0.0016685825149636746\n",
            "train loss:0.00030605353378861057\n",
            "train loss:0.0070578271138721105\n",
            "train loss:0.00015913270829038712\n",
            "train loss:0.0007669701094457338\n",
            "train loss:0.001560732536470697\n",
            "train loss:0.0029802842030734987\n",
            "train loss:0.003131846966871073\n",
            "train loss:0.00019893592425874725\n",
            "train loss:0.0002924716570712895\n",
            "train loss:0.01165852550227011\n",
            "train loss:0.0021409713983204515\n",
            "train loss:0.0005398640645681889\n",
            "train loss:0.0020235531262124016\n",
            "train loss:0.00027269344723269723\n",
            "train loss:0.0008242279774249259\n",
            "train loss:0.0001391381412209176\n",
            "train loss:0.0032589478310575204\n",
            "train loss:0.005648303886698831\n",
            "train loss:0.00019656437586034365\n",
            "train loss:0.0005992362319206025\n",
            "train loss:0.0001766975971305472\n",
            "train loss:0.00010436257057009101\n",
            "train loss:0.0008462873047186429\n",
            "train loss:0.00854819548759185\n",
            "train loss:0.0012621210491389119\n",
            "train loss:0.00170681177374831\n",
            "train loss:0.0008234506141443023\n",
            "train loss:0.0006175793764434664\n",
            "train loss:0.0021860964405505962\n",
            "train loss:0.0022696487323536994\n",
            "train loss:0.0020009520491026467\n",
            "train loss:0.001109180154773592\n",
            "train loss:0.0004431304935407094\n",
            "train loss:0.0011137559884521354\n",
            "train loss:0.002117296347061832\n",
            "train loss:0.00040750360773687567\n",
            "train loss:0.0012201559808872233\n",
            "train loss:0.006505812441200654\n",
            "train loss:0.002467136419822223\n",
            "train loss:9.185043465782148e-05\n",
            "train loss:0.0022611621694149965\n",
            "train loss:0.00074215071187276\n",
            "train loss:0.001855646443698768\n",
            "train loss:0.004831088724441721\n",
            "train loss:0.0011552481204587385\n",
            "train loss:0.008962999932218574\n",
            "train loss:6.850635164244078e-05\n",
            "train loss:0.0005346027959891072\n",
            "train loss:0.0016960553999521327\n",
            "train loss:0.0027514034010629324\n",
            "train loss:0.0007913105811619579\n",
            "train loss:0.0032437881647981626\n",
            "train loss:0.003675654504798699\n",
            "train loss:0.0028683206997538553\n",
            "=== epoch:16, train acc:0.993, test acc:0.986 ===\n",
            "train loss:0.0022692692851941135\n",
            "train loss:0.0028009568283512472\n",
            "train loss:0.0011859215156608297\n",
            "train loss:0.0004351814551584585\n",
            "train loss:0.001902363157769489\n",
            "train loss:0.002627470843170659\n",
            "train loss:0.0005720884493070392\n",
            "train loss:0.007960478531917637\n",
            "train loss:0.0001677894685183334\n",
            "train loss:0.002673489233075211\n",
            "train loss:0.0027721620086444704\n",
            "train loss:0.00030073889864472574\n",
            "train loss:0.0032068401637406763\n",
            "train loss:0.0009125020136662144\n",
            "train loss:0.004637649465835392\n",
            "train loss:0.002664361302397092\n",
            "train loss:0.00010688072821709195\n",
            "train loss:0.0026169769461498564\n",
            "train loss:0.00016878791480410204\n",
            "train loss:0.0015588388608675757\n",
            "train loss:0.00018604463178369365\n",
            "train loss:0.005519341798068794\n",
            "train loss:0.016225573111953714\n",
            "train loss:8.602909453225305e-05\n",
            "train loss:0.003677000805975828\n",
            "train loss:0.003263573275718879\n",
            "train loss:0.0012152996499341035\n",
            "train loss:0.0024033153034772776\n",
            "train loss:0.0004482563310638069\n",
            "train loss:0.00028504919143900717\n",
            "train loss:0.0007178536024736314\n",
            "train loss:0.0009957304133545255\n",
            "train loss:0.0004994507489397378\n",
            "train loss:0.0008155372699927767\n",
            "train loss:0.0010567425524866147\n",
            "train loss:0.003412411315897818\n",
            "train loss:0.0008768509531364327\n",
            "train loss:0.00043730903593341324\n",
            "train loss:0.00011229757494737518\n",
            "train loss:0.0004697650816053725\n",
            "train loss:0.00042053360112352526\n",
            "train loss:0.0031533965579859276\n",
            "train loss:4.6094050401866386e-05\n",
            "train loss:0.0025430414488537014\n",
            "train loss:0.01828729273028763\n",
            "train loss:0.002817554480912308\n",
            "train loss:0.002313541245026865\n",
            "train loss:0.001912187013828887\n",
            "train loss:0.00025148883031955255\n",
            "train loss:0.0008042803479345871\n",
            "train loss:0.0005686936638118669\n",
            "train loss:0.0005189741778163718\n",
            "train loss:0.0005477501555556184\n",
            "train loss:0.00043640455238047176\n",
            "train loss:0.009426156861145969\n",
            "train loss:0.00179223260395238\n",
            "train loss:0.002714941594307281\n",
            "train loss:0.00423892537490794\n",
            "train loss:0.002812965057238485\n",
            "train loss:0.0021326520411917033\n",
            "train loss:0.000410094205430495\n",
            "train loss:0.0037257273421014312\n",
            "train loss:0.0006188286304074915\n",
            "train loss:0.00038831455262428483\n",
            "train loss:0.0002768009981046176\n",
            "train loss:0.0003333550056567902\n",
            "train loss:0.0026769158663954586\n",
            "train loss:0.002166453652782705\n",
            "train loss:0.001106010565916386\n",
            "train loss:0.0019279569667796826\n",
            "train loss:0.0009962892271848777\n",
            "train loss:0.000926256450028813\n",
            "train loss:0.00027728342310200804\n",
            "train loss:0.0013762932136046804\n",
            "train loss:0.00048011735235733104\n",
            "train loss:0.0006989428371791776\n",
            "train loss:0.03783530593217116\n",
            "train loss:0.006573401642721295\n",
            "train loss:0.0004684498246271921\n",
            "train loss:0.0020780387043859492\n",
            "train loss:6.210071616943057e-05\n",
            "train loss:0.004146520105413127\n",
            "train loss:0.0009576285890857013\n",
            "train loss:0.0016806028542264643\n",
            "train loss:0.0015261919880089356\n",
            "train loss:0.009292905580709948\n",
            "train loss:0.004289671398973342\n",
            "train loss:0.0034672423590299275\n",
            "train loss:0.009447350300423546\n",
            "train loss:0.008304771618763496\n",
            "train loss:0.001050707385911294\n",
            "train loss:0.0002273069364830427\n",
            "train loss:0.000342318829163598\n",
            "train loss:0.003745354346127825\n",
            "train loss:0.0023050898754847695\n",
            "train loss:0.0015560309558409688\n",
            "train loss:0.004296948132389395\n",
            "train loss:0.00038369987762862384\n",
            "train loss:0.0025882817032141163\n",
            "train loss:0.000360952708766286\n",
            "train loss:0.0001447548889463515\n",
            "train loss:0.0018129731446247836\n",
            "train loss:4.162108807511595e-05\n",
            "train loss:0.0022898673967382743\n",
            "train loss:0.00012570905544305864\n",
            "train loss:0.00023156427242396595\n",
            "train loss:0.0002239255826457013\n",
            "train loss:0.00014347043815001327\n",
            "train loss:0.00015469964467085303\n",
            "train loss:0.0028696610730418737\n",
            "train loss:0.002843615971978738\n",
            "train loss:0.0023174631999413097\n",
            "train loss:0.0008225569798148394\n",
            "train loss:0.0020756479654828407\n",
            "train loss:0.007315679914207318\n",
            "train loss:8.112811647959303e-05\n",
            "train loss:0.0033429617877697992\n",
            "train loss:0.0042991454872383745\n",
            "train loss:0.0015001210386318114\n",
            "train loss:1.2588858130153666e-05\n",
            "train loss:0.0020848636952668226\n",
            "train loss:0.0011083714570850052\n",
            "train loss:0.0035600415189929323\n",
            "train loss:0.0012502305789456342\n",
            "train loss:0.005363668159539936\n",
            "train loss:0.005232285185623974\n",
            "train loss:0.006733202440203271\n",
            "train loss:0.0005871270196339566\n",
            "train loss:0.0010703626827651366\n",
            "train loss:0.00026312476627906206\n",
            "train loss:0.0007341317762510774\n",
            "train loss:0.0005880350150735651\n",
            "train loss:0.0001940239192698848\n",
            "train loss:0.002377821875244783\n",
            "train loss:0.0004072732354675826\n",
            "train loss:0.005891872504315248\n",
            "train loss:0.001236517896524363\n",
            "train loss:0.0005832194709588655\n",
            "train loss:0.004071362547758082\n",
            "train loss:0.0002514258706782427\n",
            "train loss:0.002007284679219\n",
            "train loss:0.0010396641448907839\n",
            "train loss:0.00019398608056205653\n",
            "train loss:0.0008564643136583519\n",
            "train loss:0.0038565432088092984\n",
            "train loss:0.0010019055982639872\n",
            "train loss:0.001584514020712786\n",
            "train loss:0.0004700843204304873\n",
            "train loss:0.0008168973722118064\n",
            "train loss:0.0033059020242823638\n",
            "train loss:0.00011969772526552841\n",
            "train loss:0.002892405847824288\n",
            "train loss:0.003356007598784381\n",
            "train loss:0.0006262683927411295\n",
            "train loss:0.0010579148486189844\n",
            "train loss:0.0029367295113882793\n",
            "train loss:0.0007320709239413374\n",
            "train loss:0.0005182173496415002\n",
            "train loss:0.0008522869871352337\n",
            "train loss:0.0012375649773308855\n",
            "train loss:0.0012987647668500182\n",
            "train loss:0.0018083477505471028\n",
            "train loss:7.353344338294325e-05\n",
            "train loss:0.0013631337907737166\n",
            "train loss:0.004291202867674363\n",
            "train loss:0.002399862795703114\n",
            "train loss:0.0005104240555655371\n",
            "train loss:0.002395916774682637\n",
            "train loss:0.007420204430555362\n",
            "train loss:0.004159461704933592\n",
            "train loss:0.001420060138005883\n",
            "train loss:0.003530736863622379\n",
            "train loss:0.0018642777714786988\n",
            "train loss:0.0030227543052777496\n",
            "train loss:0.0038153142785699395\n",
            "train loss:0.0062511018048588465\n",
            "train loss:0.0017844937355788717\n",
            "train loss:0.00024237356357990916\n",
            "train loss:0.00030994943715598883\n",
            "train loss:0.00031979100994029044\n",
            "train loss:0.0008836592602279458\n",
            "train loss:0.00020650024811120376\n",
            "train loss:0.0016518707046286123\n",
            "train loss:0.001103345281680933\n",
            "train loss:0.004950102824104984\n",
            "train loss:0.0008494839972683819\n",
            "train loss:0.0042526645832016395\n",
            "train loss:0.0016325582071735649\n",
            "train loss:0.0014980834477708487\n",
            "train loss:0.0006454399282766878\n",
            "train loss:0.0003900704284542359\n",
            "train loss:0.0019940575012950294\n",
            "train loss:0.0010549596105051816\n",
            "train loss:0.0005578346115156487\n",
            "train loss:0.0023186536108670847\n",
            "train loss:0.0007451694848547566\n",
            "train loss:0.0017096311242060219\n",
            "train loss:0.001732486923659822\n",
            "train loss:0.0005316688339563136\n",
            "train loss:0.010426667879656524\n",
            "train loss:0.002886354997010089\n",
            "train loss:0.0012223121626566131\n",
            "train loss:0.0030747351888625483\n",
            "train loss:0.0012290478539894463\n",
            "train loss:0.001982917815076468\n",
            "train loss:0.0011467879335493153\n",
            "train loss:0.0104483061120843\n",
            "train loss:0.0014335980055730372\n",
            "train loss:0.00023528465843179424\n",
            "train loss:0.0010217194411617733\n",
            "train loss:0.00047052816339961166\n",
            "train loss:0.0002615561504594649\n",
            "train loss:0.02386754356823866\n",
            "train loss:0.0002858864094288143\n",
            "train loss:0.0016529720516958693\n",
            "train loss:0.003507644272813304\n",
            "train loss:0.0011052212126705999\n",
            "train loss:0.010599005152504256\n",
            "train loss:0.0009384257000999857\n",
            "train loss:0.0011400668131423114\n",
            "train loss:0.001999871803011191\n",
            "train loss:0.0013248385034644707\n",
            "train loss:0.0018267664579530642\n",
            "train loss:0.0005649712570883838\n",
            "train loss:0.0004314352813672475\n",
            "train loss:0.0016524655613020377\n",
            "train loss:0.000709964551669089\n",
            "train loss:0.0007379764682600745\n",
            "train loss:0.0009152834535489494\n",
            "train loss:0.007046431538408413\n",
            "train loss:0.00018998332864444734\n",
            "train loss:0.00013536404078588095\n",
            "train loss:0.0011481173813823742\n",
            "train loss:0.00024497616715065616\n",
            "train loss:0.0005182662449979512\n",
            "train loss:0.00179706774589957\n",
            "train loss:0.0007451984627292302\n",
            "train loss:0.00956970718768272\n",
            "train loss:0.0006772385386415772\n",
            "train loss:0.00010275891259396015\n",
            "train loss:0.007397955485585862\n",
            "train loss:1.4050234856081108e-05\n",
            "train loss:0.00040161372150152274\n",
            "train loss:0.00032234104927690403\n",
            "train loss:0.0011107024720290817\n",
            "train loss:0.0011723239483351282\n",
            "train loss:0.0020129726661109407\n",
            "train loss:0.00046648047950249426\n",
            "train loss:0.0006905766561064623\n",
            "train loss:0.00192541114304956\n",
            "train loss:0.004925098499585044\n",
            "train loss:0.0010494748834507814\n",
            "train loss:0.0055129814913937114\n",
            "train loss:0.0005689868110358998\n",
            "train loss:0.004889065274162046\n",
            "train loss:0.000806762077237884\n",
            "train loss:0.001887137961488156\n",
            "train loss:0.001400209870301848\n",
            "train loss:0.00012665553312821338\n",
            "train loss:0.0001368761068060029\n",
            "train loss:0.0006621328881599174\n",
            "train loss:0.004216636206192426\n",
            "train loss:0.007264380980179496\n",
            "train loss:0.00037070471276213926\n",
            "train loss:0.0007576095850556788\n",
            "train loss:0.004934839702918774\n",
            "train loss:0.0010908109925670175\n",
            "train loss:0.0033132753095771743\n",
            "train loss:0.0039550968376809055\n",
            "train loss:0.0011000729436708655\n",
            "train loss:0.0003741518146412859\n",
            "train loss:0.0006773313068321094\n",
            "train loss:0.002094980013318272\n",
            "train loss:0.005961825712983644\n",
            "train loss:0.00014354432501368958\n",
            "train loss:0.0011485726198673934\n",
            "train loss:0.0001789885038661287\n",
            "train loss:0.0006875071993325459\n",
            "train loss:0.0008304977219758602\n",
            "train loss:0.000381258199350826\n",
            "train loss:0.0008984747342952682\n",
            "train loss:0.0004886643714768217\n",
            "train loss:0.0012550120219597222\n",
            "train loss:0.002673499095599278\n",
            "train loss:0.009755916050300299\n",
            "train loss:0.002945418477142024\n",
            "train loss:0.0018690480424665445\n",
            "train loss:0.00047659840500532675\n",
            "train loss:0.0008190208719236842\n",
            "train loss:0.0002075795211392086\n",
            "train loss:0.0007122913672356104\n",
            "train loss:0.0004772067439519104\n",
            "train loss:0.003304068244723426\n",
            "train loss:0.0026915375445796274\n",
            "train loss:0.0020857852944499104\n",
            "train loss:0.0007830963069418704\n",
            "train loss:0.003041630181510439\n",
            "train loss:0.0009430700043108504\n",
            "train loss:0.00046873213419780806\n",
            "train loss:0.00035172207565674644\n",
            "train loss:0.003019965222936907\n",
            "train loss:0.0005361556416398138\n",
            "train loss:0.0022037952356948444\n",
            "train loss:0.0001995751705474873\n",
            "train loss:0.007961844190847943\n",
            "train loss:0.00011255944730477158\n",
            "train loss:0.0011652947505697482\n",
            "train loss:0.0005634422081285788\n",
            "train loss:9.609127562607121e-05\n",
            "train loss:0.00028849872787539916\n",
            "train loss:0.00010956219822288986\n",
            "train loss:0.0005282719330874984\n",
            "train loss:0.0021132031997492455\n",
            "train loss:0.008288237731206061\n",
            "train loss:0.0029440622269828838\n",
            "train loss:0.011074034186020421\n",
            "train loss:0.0013225496194781705\n",
            "train loss:0.0002944647623427752\n",
            "train loss:0.002415069007167821\n",
            "train loss:0.0008941160603915786\n",
            "train loss:0.0016873915841488596\n",
            "train loss:0.007822355032647256\n",
            "train loss:0.0013166252213115414\n",
            "train loss:0.0006460296222302512\n",
            "train loss:0.0010613725292589816\n",
            "train loss:0.005395602549739395\n",
            "train loss:0.000619627835235388\n",
            "train loss:0.001322419476092864\n",
            "train loss:0.002196737501268452\n",
            "train loss:0.0003658643349989214\n",
            "train loss:0.000713794774083927\n",
            "train loss:0.0010233133154704851\n",
            "train loss:6.8034268248574e-05\n",
            "train loss:0.006052413254724283\n",
            "train loss:0.00031610034525863655\n",
            "train loss:0.0010310302105529237\n",
            "train loss:0.001753216516466758\n",
            "train loss:0.0021339658591466837\n",
            "train loss:0.000955755888838815\n",
            "train loss:0.0003643075470609712\n",
            "train loss:0.00011051109736711726\n",
            "train loss:0.00011742595823117596\n",
            "train loss:0.0010503076076180146\n",
            "train loss:0.00023240112745974168\n",
            "train loss:0.0008498851479271842\n",
            "train loss:0.00039475198628012\n",
            "train loss:0.0008475467972708135\n",
            "train loss:0.00012208360821877053\n",
            "train loss:0.002864379458358036\n",
            "train loss:0.00019462520048435983\n",
            "train loss:0.0009397529261237121\n",
            "train loss:0.0007835156235988589\n",
            "train loss:0.00026745103132107305\n",
            "train loss:0.0022459197070825564\n",
            "train loss:0.00028985648004605213\n",
            "train loss:0.0008182600981865301\n",
            "train loss:0.00018662487798797354\n",
            "train loss:0.00048461249191796853\n",
            "train loss:0.0011734458142945766\n",
            "train loss:0.0002020373878627925\n",
            "train loss:0.005116238006957796\n",
            "train loss:0.0042550071468744535\n",
            "train loss:0.0012924371222252445\n",
            "train loss:0.0017670923726793618\n",
            "train loss:0.0013979040043668137\n",
            "train loss:0.00021918645453154783\n",
            "train loss:0.002736662632338984\n",
            "train loss:0.0018376240334245092\n",
            "train loss:0.0013860208501511985\n",
            "train loss:0.004870275973848122\n",
            "train loss:0.03744347826441442\n",
            "train loss:0.000556585508880565\n",
            "train loss:0.0002961259230195529\n",
            "train loss:0.0021340157093495253\n",
            "train loss:0.010471681008296381\n",
            "train loss:0.0002282760445514373\n",
            "train loss:0.0018641896623056364\n",
            "train loss:0.0014358451608142918\n",
            "train loss:0.0023404501571100862\n",
            "train loss:0.001773591314160726\n",
            "train loss:0.0009584762756611409\n",
            "train loss:0.0003837532874393033\n",
            "train loss:0.0015199264091039387\n",
            "train loss:0.00018101928762675765\n",
            "train loss:7.068990554002221e-05\n",
            "train loss:0.001973018054601528\n",
            "train loss:0.0016049973321988257\n",
            "train loss:0.0006978024056848407\n",
            "train loss:0.0009117854608694981\n",
            "train loss:0.0004773847967177648\n",
            "train loss:0.0021277332962816105\n",
            "train loss:4.942030310415246e-05\n",
            "train loss:0.002442867077973443\n",
            "train loss:0.0015330281327234864\n",
            "train loss:0.002344552697843341\n",
            "train loss:0.0029158187426703867\n",
            "train loss:0.0007748445475329553\n",
            "train loss:0.001887030784043009\n",
            "train loss:0.013583822742152345\n",
            "train loss:0.005042646815118345\n",
            "train loss:0.0038085070472949132\n",
            "train loss:0.00476705918704424\n",
            "train loss:0.0006156399869763399\n",
            "train loss:0.001351682081804874\n",
            "train loss:0.003798405406999026\n",
            "train loss:0.0010990127901160907\n",
            "train loss:0.004308871137100409\n",
            "train loss:0.004740746623869618\n",
            "train loss:0.0009740101857282113\n",
            "train loss:0.002759412586478019\n",
            "train loss:0.004605915382803535\n",
            "train loss:0.0006425970919263567\n",
            "train loss:0.000404128637363879\n",
            "train loss:0.00022558803059160763\n",
            "train loss:0.00019899403062260097\n",
            "train loss:0.00040728746217419615\n",
            "train loss:0.004334241698240092\n",
            "train loss:0.004732606636802343\n",
            "train loss:0.0001161569302393535\n",
            "train loss:0.00012524366691437546\n",
            "train loss:0.0015640469577936627\n",
            "train loss:0.00030967704258439804\n",
            "train loss:0.00022061929923711262\n",
            "train loss:0.0010533835622358676\n",
            "train loss:0.00026511672648400737\n",
            "train loss:0.0001866526712052532\n",
            "train loss:0.0011891907900661762\n",
            "train loss:0.00032918229608220624\n",
            "train loss:0.0005117952673157649\n",
            "train loss:0.0011867250032786538\n",
            "train loss:0.0008655929290489441\n",
            "train loss:0.0005119366804551375\n",
            "train loss:0.00024653070745154164\n",
            "train loss:0.0004780566634920963\n",
            "train loss:0.0007589259286422758\n",
            "train loss:0.0007363009532502172\n",
            "train loss:0.041825539823007636\n",
            "train loss:0.002509656033581048\n",
            "train loss:0.00019041083211599928\n",
            "train loss:0.00032234255243372456\n",
            "train loss:0.0002481689966118134\n",
            "train loss:0.0019515238553305997\n",
            "train loss:0.0008520874790672183\n",
            "train loss:0.001199299295501234\n",
            "train loss:0.004393434612595085\n",
            "train loss:0.0008045990791000738\n",
            "train loss:0.000288229123012584\n",
            "train loss:0.00246552874042009\n",
            "train loss:0.0004659432823831607\n",
            "train loss:0.0007167447048632169\n",
            "train loss:0.0018082163738656223\n",
            "train loss:0.0003663137852885245\n",
            "train loss:0.0003286828237698979\n",
            "train loss:0.002653498066772052\n",
            "train loss:0.0026299463097046354\n",
            "train loss:0.0009020984761484853\n",
            "train loss:0.0015325669364190317\n",
            "train loss:0.0004657394969513412\n",
            "train loss:0.00032033860074733315\n",
            "train loss:0.0002820391746464571\n",
            "train loss:0.0016534945220693636\n",
            "train loss:0.0008501285632456792\n",
            "train loss:0.0006606236015708749\n",
            "train loss:0.00017999745718480582\n",
            "train loss:0.00114792018071211\n",
            "train loss:0.0003097605869811662\n",
            "train loss:0.0015466580403779464\n",
            "train loss:3.9626500422955945e-05\n",
            "train loss:0.0013533770412509178\n",
            "train loss:0.0004046710194659213\n",
            "train loss:0.0010116890795536244\n",
            "train loss:0.002578596430301369\n",
            "train loss:0.0005683132383609631\n",
            "train loss:0.0002744719785354037\n",
            "train loss:0.0007992651904299352\n",
            "train loss:0.0002867844350404493\n",
            "train loss:0.0047657875797320216\n",
            "train loss:0.00017858857413116484\n",
            "train loss:0.00012774271126919393\n",
            "train loss:0.0035573950321627994\n",
            "train loss:0.0003785807251206534\n",
            "train loss:0.0008802626573373979\n",
            "train loss:0.005458406009833168\n",
            "train loss:0.00047245099269977033\n",
            "train loss:0.0009105208308464501\n",
            "train loss:0.0002580752716999323\n",
            "train loss:0.0009356661474872066\n",
            "train loss:0.0001500250091028543\n",
            "train loss:0.0005445981989367897\n",
            "train loss:6.768451530496828e-05\n",
            "train loss:0.001493190533094334\n",
            "train loss:0.000476031439292062\n",
            "train loss:0.001063810746815444\n",
            "train loss:0.0006355893059091466\n",
            "train loss:0.0011238621092782052\n",
            "train loss:0.001289984730167276\n",
            "train loss:0.0009210299175130525\n",
            "train loss:0.002078528223919395\n",
            "train loss:0.0003831725859093861\n",
            "train loss:0.0031022817863808807\n",
            "train loss:0.0003531148056997825\n",
            "train loss:8.679405584756679e-05\n",
            "train loss:0.00037479023427476355\n",
            "train loss:9.757191707026396e-05\n",
            "train loss:0.00038652201225935026\n",
            "train loss:0.0021346805793059013\n",
            "train loss:0.0002820739050316939\n",
            "train loss:0.002733809516654806\n",
            "train loss:0.0011034496508093995\n",
            "train loss:0.001525114057588593\n",
            "train loss:0.0023755566371438594\n",
            "train loss:0.0005013430291470283\n",
            "train loss:0.0018195731818917208\n",
            "train loss:0.00015560175671225173\n",
            "train loss:0.00017443787084664336\n",
            "train loss:0.0005016277285099175\n",
            "train loss:0.001729290055840087\n",
            "train loss:0.00039077270548569325\n",
            "train loss:0.0007780440215263888\n",
            "train loss:0.0017783772468246506\n",
            "train loss:0.0022289127225786033\n",
            "train loss:0.00011996659943804754\n",
            "train loss:8.132960073892117e-05\n",
            "train loss:0.00048225230973492436\n",
            "train loss:0.000309454914005902\n",
            "train loss:0.0020260618013919042\n",
            "train loss:0.00034351691679392656\n",
            "train loss:0.0027685719495405487\n",
            "train loss:0.0013895945681999982\n",
            "train loss:0.004190708942403037\n",
            "train loss:0.00010100821102431057\n",
            "train loss:0.0026972860164758716\n",
            "train loss:0.0001119693985501959\n",
            "train loss:0.00012620892896205872\n",
            "train loss:0.0002560750008880458\n",
            "train loss:0.0017730891481224473\n",
            "train loss:0.0036517680507280443\n",
            "train loss:0.0013752357670198387\n",
            "train loss:0.003133762046902427\n",
            "train loss:0.0008185783938490784\n",
            "train loss:0.0038198059134598294\n",
            "train loss:0.0012978785947752496\n",
            "train loss:0.0008088704516112917\n",
            "train loss:0.0003616791428689679\n",
            "train loss:0.0005663864714056057\n",
            "train loss:0.0004464811808959133\n",
            "train loss:0.0002971891687112125\n",
            "train loss:0.015685699763391615\n",
            "train loss:0.0012950476655203727\n",
            "train loss:0.0004742534275956411\n",
            "train loss:0.00017164859636408997\n",
            "train loss:0.0037665042720460827\n",
            "train loss:0.002736224218660228\n",
            "train loss:0.000727020943431419\n",
            "train loss:0.0003831627429994275\n",
            "train loss:0.00014018000852266432\n",
            "train loss:0.0010211986318857528\n",
            "train loss:0.0003500591216477455\n",
            "train loss:0.008613552896725753\n",
            "train loss:0.0002500507817685222\n",
            "train loss:0.002392242445204107\n",
            "train loss:0.001632632015289146\n",
            "train loss:0.0022116514173421893\n",
            "train loss:0.00043223793309668816\n",
            "train loss:0.001349713423279754\n",
            "train loss:0.00848735040991615\n",
            "train loss:0.0014846505845483888\n",
            "train loss:0.004225219448451478\n",
            "train loss:0.0008391712178603017\n",
            "train loss:0.001652636585217558\n",
            "train loss:0.0026988671359957374\n",
            "train loss:0.0001060508393708563\n",
            "train loss:0.002277840359891395\n",
            "train loss:0.000954769418391467\n",
            "train loss:0.0020957508957190844\n",
            "train loss:0.0003435201454893759\n",
            "train loss:0.0014723670533829838\n",
            "train loss:0.0001107293929277503\n",
            "train loss:0.000574463101749014\n",
            "train loss:0.0003002679292037491\n",
            "train loss:4.7871673697181364e-05\n",
            "train loss:7.449805334615784e-05\n",
            "train loss:0.0012202402652641488\n",
            "train loss:0.00038720739740425196\n",
            "train loss:0.0005331235936937151\n",
            "train loss:0.0006021238402552779\n",
            "train loss:0.0010062963547560878\n",
            "train loss:0.0019217726961579943\n",
            "train loss:0.003114885777585388\n",
            "train loss:0.0011408361705353462\n",
            "train loss:2.6340519578607053e-05\n",
            "train loss:0.0006806949814609925\n",
            "train loss:0.0004212263501971988\n",
            "train loss:0.0005836257440941902\n",
            "train loss:0.009839259263296532\n",
            "train loss:0.0006623846317726729\n",
            "train loss:7.71976564927803e-05\n",
            "train loss:0.00043642617193598304\n",
            "train loss:0.0015912700710400019\n",
            "train loss:0.0017036388392529366\n",
            "=== epoch:17, train acc:0.999, test acc:0.988 ===\n",
            "train loss:0.00010024058843172949\n",
            "train loss:0.0035773781871106004\n",
            "train loss:0.0002224514360713546\n",
            "train loss:0.0006509040060906007\n",
            "train loss:0.00015416428314435297\n",
            "train loss:0.0007870034102290783\n",
            "train loss:0.001198539768351152\n",
            "train loss:0.00024783330787691253\n",
            "train loss:0.0019754068628750746\n",
            "train loss:0.0005633216539971431\n",
            "train loss:0.00050051989559317\n",
            "train loss:0.0015020856527920142\n",
            "train loss:0.004320718016105858\n",
            "train loss:0.0023315377441405372\n",
            "train loss:0.0007039615787676191\n",
            "train loss:0.0019626020653767196\n",
            "train loss:0.0003885536132075101\n",
            "train loss:0.0003926250569742112\n",
            "train loss:0.0005748266123447426\n",
            "train loss:0.0005653340261772046\n",
            "train loss:0.001223421427991696\n",
            "train loss:0.002377049788966372\n",
            "train loss:0.006343963380654505\n",
            "train loss:6.735836972435832e-05\n",
            "train loss:0.0014122055346516285\n",
            "train loss:0.001250054003890206\n",
            "train loss:0.0015641105820760979\n",
            "train loss:0.0016658384065010388\n",
            "train loss:0.0003744557629014652\n",
            "train loss:0.0008868383783702983\n",
            "train loss:0.0010072157509164285\n",
            "train loss:0.0003693495044852668\n",
            "train loss:0.0008452972076547314\n",
            "train loss:0.00035567965498233255\n",
            "train loss:0.0003353611672481026\n",
            "train loss:0.001972232699487128\n",
            "train loss:0.001382994113393456\n",
            "train loss:0.0015484686023952325\n",
            "train loss:0.0025292094802771953\n",
            "train loss:0.00038907223744541556\n",
            "train loss:0.0009135558152194196\n",
            "train loss:0.00014666082948913302\n",
            "train loss:0.00036587058761657\n",
            "train loss:0.0005941442323956588\n",
            "train loss:0.0002756260356168537\n",
            "train loss:0.0010986110067408143\n",
            "train loss:0.0004298920572468094\n",
            "train loss:0.0014518194804477979\n",
            "train loss:0.0009923719434738164\n",
            "train loss:0.0002522256233736037\n",
            "train loss:0.000631692198818314\n",
            "train loss:0.0004325016523767068\n",
            "train loss:0.0009699923470932984\n",
            "train loss:0.00048499370293535594\n",
            "train loss:0.0008564351012573677\n",
            "train loss:0.001871362823737502\n",
            "train loss:0.0001970789543336111\n",
            "train loss:0.0001274284120129225\n",
            "train loss:0.0002386813209192542\n",
            "train loss:0.00014394584556397017\n",
            "train loss:0.0003551004031446336\n",
            "train loss:0.00019144595365840777\n",
            "train loss:0.0006051856127856303\n",
            "train loss:0.0006134468130823594\n",
            "train loss:0.0001303398186188995\n",
            "train loss:0.0007036864611223334\n",
            "train loss:0.0008646430356962726\n",
            "train loss:0.0006551696236099063\n",
            "train loss:0.0008512406763083646\n",
            "train loss:0.0005562931638541106\n",
            "train loss:0.006475915177872283\n",
            "train loss:0.0006235288054586644\n",
            "train loss:0.0009885151822562582\n",
            "train loss:0.00043110396209822835\n",
            "train loss:0.000667559558304997\n",
            "train loss:0.0013046706021998481\n",
            "train loss:0.0007671098553359428\n",
            "train loss:6.214780447265471e-05\n",
            "train loss:0.0024106988795772923\n",
            "train loss:0.00033749969374525666\n",
            "train loss:0.00075557593097937\n",
            "train loss:0.004433669968575248\n",
            "train loss:0.0016731630308742492\n",
            "train loss:0.0002444053357991767\n",
            "train loss:0.00038851865491007373\n",
            "train loss:0.0019570969236378543\n",
            "train loss:0.0014824554344660358\n",
            "train loss:0.0023762913853808357\n",
            "train loss:0.00013214907605082408\n",
            "train loss:0.0014728538749142544\n",
            "train loss:0.0016947423239279122\n",
            "train loss:0.000909315375304177\n",
            "train loss:0.0053485791831733446\n",
            "train loss:5.12403063799871e-05\n",
            "train loss:0.003611753340462777\n",
            "train loss:0.0016156964485332045\n",
            "train loss:0.0013050829212420086\n",
            "train loss:0.0002759106075832302\n",
            "train loss:0.0006642219601946382\n",
            "train loss:0.00042854189681580765\n",
            "train loss:0.0004690818191420637\n",
            "train loss:0.0011263914376477685\n",
            "train loss:0.0011458943415737263\n",
            "train loss:0.0020158372834043524\n",
            "train loss:0.0010206401204777978\n",
            "train loss:0.00015116986286058056\n",
            "train loss:0.0015015723200426634\n",
            "train loss:0.00023480840725962178\n",
            "train loss:0.0012668801752166816\n",
            "train loss:0.0003286499816459746\n",
            "train loss:0.00015925165785768555\n",
            "train loss:0.0009470428280282603\n",
            "train loss:0.0006518062106539984\n",
            "train loss:0.0004818580491424499\n",
            "train loss:0.0017446471310036185\n",
            "train loss:0.004381957731278924\n",
            "train loss:0.0009750402285890911\n",
            "train loss:0.0008403245143780192\n",
            "train loss:0.0009673418206982069\n",
            "train loss:0.0002841172907686436\n",
            "train loss:0.0016723990112098438\n",
            "train loss:0.00013900535102342893\n",
            "train loss:3.383016968767846e-05\n",
            "train loss:0.0019870897320068102\n",
            "train loss:0.00026619210083664145\n",
            "train loss:0.00016778734964276575\n",
            "train loss:0.00034347069008578145\n",
            "train loss:0.0016105807838431114\n",
            "train loss:0.004286826485309289\n",
            "train loss:0.0006372200169179936\n",
            "train loss:0.00036938020855586257\n",
            "train loss:0.0002691120105150675\n",
            "train loss:0.0002970312835415321\n",
            "train loss:0.006050863038424865\n",
            "train loss:0.00014662491524451972\n",
            "train loss:0.0008031843917307716\n",
            "train loss:0.00042238344997360316\n",
            "train loss:0.00018519854893129077\n",
            "train loss:0.0017892815509317074\n",
            "train loss:0.0003889841691864984\n",
            "train loss:0.000197569011146528\n",
            "train loss:0.000684177003695534\n",
            "train loss:0.0010057505068539188\n",
            "train loss:8.257414340098004e-05\n",
            "train loss:0.0005891704527721923\n",
            "train loss:1.3753213986773053e-05\n",
            "train loss:0.0014392052227687852\n",
            "train loss:0.002649802988384117\n",
            "train loss:0.0009237380721476968\n",
            "train loss:0.0005918277089148659\n",
            "train loss:6.943793503141906e-05\n",
            "train loss:0.0021624050391451837\n",
            "train loss:0.0007603977033188678\n",
            "train loss:0.0015011149091482443\n",
            "train loss:0.0012668777571215573\n",
            "train loss:0.0008713903509107736\n",
            "train loss:0.002061158292946292\n",
            "train loss:0.0005137175463532565\n",
            "train loss:0.0011799531610591236\n",
            "train loss:0.0001182298700588218\n",
            "train loss:0.0003663994589241227\n",
            "train loss:0.0005575223439122798\n",
            "train loss:0.0026054581481412147\n",
            "train loss:0.0012948716227182645\n",
            "train loss:0.002029400678453704\n",
            "train loss:0.0015042607321657715\n",
            "train loss:0.003532122150809877\n",
            "train loss:0.0015430485094648965\n",
            "train loss:0.001635278268057812\n",
            "train loss:0.0018024949757936388\n",
            "train loss:0.0012761140353687286\n",
            "train loss:0.0007025990151703314\n",
            "train loss:0.000160749768075953\n",
            "train loss:0.00039823727646126134\n",
            "train loss:0.0004220643478367904\n",
            "train loss:0.0012988062289280094\n",
            "train loss:3.458535390197538e-05\n",
            "train loss:0.0013583553249720585\n",
            "train loss:0.0019289306228776515\n",
            "train loss:0.00024671081109242676\n",
            "train loss:0.0003635648343592308\n",
            "train loss:0.00018187709090461513\n",
            "train loss:0.0015197047148224213\n",
            "train loss:0.002320568009926522\n",
            "train loss:0.001633188170023962\n",
            "train loss:0.00372473701286469\n",
            "train loss:0.00018657025203614445\n",
            "train loss:0.0011604900600922388\n",
            "train loss:0.00043748586681548944\n",
            "train loss:0.0009356464385100249\n",
            "train loss:0.0010149691993975036\n",
            "train loss:0.0007378038974653707\n",
            "train loss:6.524636815393718e-05\n",
            "train loss:0.003032575423828871\n",
            "train loss:0.0007253795089260274\n",
            "train loss:0.0005020619437536535\n",
            "train loss:0.0015968217850240518\n",
            "train loss:0.0001374632579124411\n",
            "train loss:0.0006014766871483315\n",
            "train loss:0.0025523896563248317\n",
            "train loss:0.0020603221244882143\n",
            "train loss:9.323625858080533e-05\n",
            "train loss:0.0004680219957989629\n",
            "train loss:0.0004339358227060447\n",
            "train loss:0.00015947727816365942\n",
            "train loss:0.001547096999998996\n",
            "train loss:0.0019689245309529\n",
            "train loss:0.0028498216170371516\n",
            "train loss:0.001597220443191134\n",
            "train loss:0.002890489310766266\n",
            "train loss:0.0001469676673694954\n",
            "train loss:0.0056546829734630275\n",
            "train loss:0.0007262807708237009\n",
            "train loss:0.0003154025542970035\n",
            "train loss:0.0028125103881562036\n",
            "train loss:0.0002810590017913343\n",
            "train loss:0.0003170053479977467\n",
            "train loss:0.0003958647819538777\n",
            "train loss:0.02625989709595487\n",
            "train loss:0.00021647273329394542\n",
            "train loss:0.003839030484335803\n",
            "train loss:0.00041325437680778186\n",
            "train loss:0.0003909940033292186\n",
            "train loss:0.00032030889746835706\n",
            "train loss:0.00021140769375612944\n",
            "train loss:0.010763574872965842\n",
            "train loss:0.001153353674752378\n",
            "train loss:0.0018957412944038924\n",
            "train loss:0.002041179159011367\n",
            "train loss:0.001560898357765686\n",
            "train loss:0.0005025999405074168\n",
            "train loss:0.0005682863077825367\n",
            "train loss:5.222590934069484e-05\n",
            "train loss:0.0029218891750906724\n",
            "train loss:0.00010350877081733033\n",
            "train loss:0.0022707651026707683\n",
            "train loss:0.0014687060221226106\n",
            "train loss:0.003359611107106833\n",
            "train loss:0.0009801701569637188\n",
            "train loss:0.0024883499605353686\n",
            "train loss:0.00012115542928396946\n",
            "train loss:0.0015503868272240553\n",
            "train loss:0.0020844918753645083\n",
            "train loss:0.00018807511574755944\n",
            "train loss:0.0006732866616059989\n",
            "train loss:0.004008417669253798\n",
            "train loss:0.010360206657831836\n",
            "train loss:0.020837026645976912\n",
            "train loss:0.007645845637374515\n",
            "train loss:0.025927307635265344\n",
            "train loss:0.005514120354573707\n",
            "train loss:0.0014219556540929449\n",
            "train loss:0.0016944918587614949\n",
            "train loss:0.001773692185349206\n",
            "train loss:0.0005835015151848658\n",
            "train loss:0.0018009869792342567\n",
            "train loss:0.002839311267059748\n",
            "train loss:0.0017619673569017544\n",
            "train loss:0.00013631466159097993\n",
            "train loss:0.0009764575765370799\n",
            "train loss:0.0012391888428179834\n",
            "train loss:0.0004970602513890771\n",
            "train loss:0.001014345179582902\n",
            "train loss:0.0039802493640733886\n",
            "train loss:0.0016012063191335575\n",
            "train loss:0.0011153588701976675\n",
            "train loss:0.00015120945251766862\n",
            "train loss:0.0011939631210981514\n",
            "train loss:0.0007225617054523518\n",
            "train loss:2.6120344432400926e-05\n",
            "train loss:0.0008201999404200077\n",
            "train loss:0.006493662110484166\n",
            "train loss:0.0005246055372663556\n",
            "train loss:0.02020464199434194\n",
            "train loss:0.00019289907928273151\n",
            "train loss:0.00035852708062320937\n",
            "train loss:0.0016178305079332233\n",
            "train loss:0.0002969855698481077\n",
            "train loss:0.006975551207922528\n",
            "train loss:0.000725981799812608\n",
            "train loss:0.034383920762850136\n",
            "train loss:0.0002481947011084963\n",
            "train loss:0.001391608651374643\n",
            "train loss:0.002883550915857984\n",
            "train loss:0.0006358335434187409\n",
            "train loss:0.0037835466749489744\n",
            "train loss:0.0021431749142405195\n",
            "train loss:0.01291327552436618\n",
            "train loss:3.5711217978196566e-05\n",
            "train loss:0.0016993584106037363\n",
            "train loss:0.0002691825720091169\n",
            "train loss:0.0006815987272180174\n",
            "train loss:0.008707382914272914\n",
            "train loss:0.0014988061738217093\n",
            "train loss:0.003376252376086704\n",
            "train loss:0.0005268332599447855\n",
            "train loss:0.009133512650483892\n",
            "train loss:0.0001783600703106002\n",
            "train loss:0.00021776980214215224\n",
            "train loss:0.0010503174386228263\n",
            "train loss:0.0012921826497944177\n",
            "train loss:0.001816728703828144\n",
            "train loss:0.0019997592315539993\n",
            "train loss:0.001665677953004405\n",
            "train loss:0.00021161633531611046\n",
            "train loss:0.0013443213252618705\n",
            "train loss:0.0003085404742935204\n",
            "train loss:0.0003508878461089443\n",
            "train loss:0.00018765909956906332\n",
            "train loss:0.0025714505669952713\n",
            "train loss:0.003525487123190923\n",
            "train loss:0.016458204911374917\n",
            "train loss:0.0032874958594471624\n",
            "train loss:0.0015438279001322242\n",
            "train loss:0.00123875430326305\n",
            "train loss:0.00190816897057137\n",
            "train loss:0.004889961914306227\n",
            "train loss:0.0023537157834281246\n",
            "train loss:0.005387360310464193\n",
            "train loss:0.0005096903166089324\n",
            "train loss:0.004108420885700827\n",
            "train loss:0.0006988719176450526\n",
            "train loss:0.0002485159799116819\n",
            "train loss:0.000748674620668286\n",
            "train loss:0.00014957248617737063\n",
            "train loss:0.0012014596640556644\n",
            "train loss:0.0016252594078188071\n",
            "train loss:0.004156424374094526\n",
            "train loss:0.004335084501929326\n",
            "train loss:0.001460776764987257\n",
            "train loss:0.0011893410496416677\n",
            "train loss:0.0001242268017334575\n",
            "train loss:0.001494181729265873\n",
            "train loss:0.01962495343391474\n",
            "train loss:0.001084353347911449\n",
            "train loss:0.00021956387303869723\n",
            "train loss:0.003293616141953508\n",
            "train loss:0.0009520127996843915\n",
            "train loss:0.003214793935847271\n",
            "train loss:0.004283529258969822\n",
            "train loss:0.005227135829289279\n",
            "train loss:0.0006462959179851854\n",
            "train loss:0.00014342526691208854\n",
            "train loss:0.0008117266201477518\n",
            "train loss:0.003323454895724011\n",
            "train loss:0.0012364361231196755\n",
            "train loss:0.0008463747083091255\n",
            "train loss:0.00015241877909956937\n",
            "train loss:0.0016128007685180486\n",
            "train loss:0.0014168842235556482\n",
            "train loss:0.0019530324225472627\n",
            "train loss:0.0018349570258327217\n",
            "train loss:0.003656998940214978\n",
            "train loss:0.0006324120794994484\n",
            "train loss:0.003910344784220166\n",
            "train loss:0.00035103973567876344\n",
            "train loss:0.000583976316665323\n",
            "train loss:0.012416624457500658\n",
            "train loss:0.002099033280146915\n",
            "train loss:0.00587323800318976\n",
            "train loss:0.002715289287245711\n",
            "train loss:0.0015137904677710295\n",
            "train loss:0.0009214732557934481\n",
            "train loss:0.00013987027332117697\n",
            "train loss:0.0010678523994495286\n",
            "train loss:0.0002700480390305609\n",
            "train loss:0.003478243839988565\n",
            "train loss:0.0014380183940975436\n",
            "train loss:0.0015881123007673353\n",
            "train loss:0.001928663172681653\n",
            "train loss:0.00042558667842147957\n",
            "train loss:0.0007649178008686629\n",
            "train loss:0.0007366194966921693\n",
            "train loss:0.0004486492405242029\n",
            "train loss:0.0003218500300154375\n",
            "train loss:0.0025572224792365207\n",
            "train loss:0.00040978415720493066\n",
            "train loss:0.004208512941450171\n",
            "train loss:0.003984014148069115\n",
            "train loss:0.0007541240201543752\n",
            "train loss:0.0014301811159555273\n",
            "train loss:0.0012048337550655346\n",
            "train loss:6.375615531149152e-05\n",
            "train loss:0.00010965796028814646\n",
            "train loss:0.0009509546698248978\n",
            "train loss:0.002647613970402826\n",
            "train loss:0.0011862252300839226\n",
            "train loss:7.581912508837104e-05\n",
            "train loss:0.002002889884264237\n",
            "train loss:0.0008207053461218916\n",
            "train loss:0.030802934601213815\n",
            "train loss:0.004983009079997102\n",
            "train loss:0.004233418676729835\n",
            "train loss:0.0013349811558423044\n",
            "train loss:6.849059799530623e-05\n",
            "train loss:0.0011577089841126903\n",
            "train loss:0.0019365577478765795\n",
            "train loss:0.0043077110024007\n",
            "train loss:0.0007593130331832247\n",
            "train loss:0.00018226918607720615\n",
            "train loss:0.002089272100041114\n",
            "train loss:6.256978622542639e-05\n",
            "train loss:0.0009311643286857137\n",
            "train loss:0.0013564588912162577\n",
            "train loss:0.0008777454718969963\n",
            "train loss:0.000632705655274269\n",
            "train loss:0.0019860101943894475\n",
            "train loss:0.009004951211105458\n",
            "train loss:0.004417257872149165\n",
            "train loss:0.006963162646412835\n",
            "train loss:0.011362982509979924\n",
            "train loss:0.0015343798173589008\n",
            "train loss:0.005237772834367046\n",
            "train loss:6.787078705420473e-05\n",
            "train loss:0.0024229378599856725\n",
            "train loss:0.0005549870570487923\n",
            "train loss:0.002297339106235671\n",
            "train loss:0.0031349227066197094\n",
            "train loss:3.6921861446393325e-05\n",
            "train loss:0.0003744636495873484\n",
            "train loss:0.0015530755781606087\n",
            "train loss:0.0035251881567431543\n",
            "train loss:0.00013624693576474133\n",
            "train loss:0.0007615489118874031\n",
            "train loss:0.0007408290228253299\n",
            "train loss:0.00026801525005347356\n",
            "train loss:0.00044841356493432934\n",
            "train loss:0.0003485535604926411\n",
            "train loss:0.002584256976325339\n",
            "train loss:0.0002379364056600488\n",
            "train loss:0.00010515974402999658\n",
            "train loss:0.0021793816491344287\n",
            "train loss:0.0021727075691779845\n",
            "train loss:0.003141513227587128\n",
            "train loss:0.0032595882577628062\n",
            "train loss:0.000351763249016503\n",
            "train loss:0.0008543671076326283\n",
            "train loss:6.552932011122308e-05\n",
            "train loss:0.00019864972361388806\n",
            "train loss:2.8883749346136626e-05\n",
            "train loss:0.0006765066574944825\n",
            "train loss:0.0013129532466610394\n",
            "train loss:0.0002402488618728734\n",
            "train loss:0.00040928547577423176\n",
            "train loss:0.004462153378387983\n",
            "train loss:9.563623135313494e-05\n",
            "train loss:0.0005391064089486488\n",
            "train loss:0.009221098396608882\n",
            "train loss:0.0012019538366244652\n",
            "train loss:0.00022508421296720067\n",
            "train loss:0.003457992506870318\n",
            "train loss:0.001619505142965565\n",
            "train loss:0.00023378332604029987\n",
            "train loss:0.0007398796106671505\n",
            "train loss:0.0016471073331794607\n",
            "train loss:0.007867377369801688\n",
            "train loss:0.00042855685128512016\n",
            "train loss:0.00745042909296125\n",
            "train loss:0.0008497240583531186\n",
            "train loss:0.003658157002553956\n",
            "train loss:0.0033180288402616166\n",
            "train loss:0.00031996778566684\n",
            "train loss:0.0006862812562771429\n",
            "train loss:0.004688517197129938\n",
            "train loss:0.0014552811583864512\n",
            "train loss:0.0011104465862130195\n",
            "train loss:0.0015674404444275527\n",
            "train loss:0.0003918216779083101\n",
            "train loss:0.0007345155891796584\n",
            "train loss:0.0012888295479574529\n",
            "train loss:0.0004017723432429082\n",
            "train loss:0.001520271327046275\n",
            "train loss:0.0014167983673233702\n",
            "train loss:0.012341531216562585\n",
            "train loss:0.0013527077234420867\n",
            "train loss:0.00031645008919290075\n",
            "train loss:0.012051360369792987\n",
            "train loss:0.000637330619152067\n",
            "train loss:0.0030517903340030123\n",
            "train loss:0.0009134643289562646\n",
            "train loss:0.0036308591064982405\n",
            "train loss:0.0031254489535816003\n",
            "train loss:0.0005504707634037915\n",
            "train loss:0.010989059077944629\n",
            "train loss:0.0011499005542948662\n",
            "train loss:0.0011009780552237816\n",
            "train loss:9.574317877211416e-05\n",
            "train loss:0.00018556187653451537\n",
            "train loss:0.0019727834220016824\n",
            "train loss:0.0007413192480404784\n",
            "train loss:0.0009502837914505153\n",
            "train loss:8.174721173741231e-05\n",
            "train loss:0.009745306126793047\n",
            "train loss:0.003988396671066259\n",
            "train loss:0.004051703108112508\n",
            "train loss:0.0028108384244801177\n",
            "train loss:0.0008993995911260977\n",
            "train loss:0.00042739910493646416\n",
            "train loss:0.0007344258621403505\n",
            "train loss:0.004268947865646099\n",
            "train loss:0.0006557500618097574\n",
            "train loss:0.00030667615803433364\n",
            "train loss:0.002726551831434606\n",
            "train loss:0.0003508229666180997\n",
            "train loss:0.003551784421567175\n",
            "train loss:0.0025619404042706347\n",
            "train loss:0.0005074294291273434\n",
            "train loss:0.0009480550727459674\n",
            "train loss:0.0063618142231159905\n",
            "train loss:0.0355219587716926\n",
            "train loss:0.0029972656242759628\n",
            "train loss:0.001897192984034012\n",
            "train loss:0.001634999282534453\n",
            "train loss:0.00039214139820887835\n",
            "train loss:0.00014057156341507323\n",
            "train loss:0.00017078178926643888\n",
            "train loss:0.002460685190639596\n",
            "train loss:0.0013765350669369964\n",
            "train loss:0.0007033651578609075\n",
            "train loss:0.00292826420122355\n",
            "train loss:0.01368279433633192\n",
            "train loss:0.00026372076649386675\n",
            "train loss:0.06681124521109118\n",
            "train loss:0.0025124077264113203\n",
            "train loss:0.0004577632974158297\n",
            "train loss:0.0004855234298430222\n",
            "train loss:0.0010775842094973536\n",
            "train loss:0.0005149385697188321\n",
            "train loss:0.00010961106916087319\n",
            "train loss:0.00016743996361085156\n",
            "train loss:0.0011104988957149195\n",
            "train loss:0.0009071165732506156\n",
            "train loss:0.00015881935106542287\n",
            "train loss:0.005754059585684256\n",
            "train loss:0.01263593265676103\n",
            "train loss:0.00013564143604888383\n",
            "train loss:0.018032656293782135\n",
            "train loss:0.007298455395638186\n",
            "train loss:0.002577359230324391\n",
            "train loss:0.0006982865653933892\n",
            "train loss:0.005281194753136249\n",
            "train loss:0.0004746882068338768\n",
            "train loss:0.00025557267756296366\n",
            "train loss:0.0004511180925683886\n",
            "train loss:0.0009672021274846649\n",
            "train loss:0.00021924012369502766\n",
            "train loss:0.0002018675761254522\n",
            "train loss:0.0007546028433820736\n",
            "train loss:0.002607845910444913\n",
            "train loss:6.126692934088367e-05\n",
            "train loss:0.001464392821227879\n",
            "train loss:0.00047160066616571815\n",
            "train loss:0.0053299939221523205\n",
            "train loss:0.0011868549444112777\n",
            "train loss:0.0006370791890987409\n",
            "train loss:0.00028246912279272405\n",
            "train loss:0.003122786131174041\n",
            "train loss:0.002352537099328315\n",
            "train loss:0.0029087838775324866\n",
            "train loss:0.0018670536287491287\n",
            "train loss:0.0017194573856501153\n",
            "train loss:0.006641178350807475\n",
            "train loss:0.004566323767116481\n",
            "train loss:0.0036043830312627507\n",
            "train loss:0.00024490772331376743\n",
            "train loss:0.003387563140061506\n",
            "train loss:0.0072412637529385825\n",
            "train loss:0.004911337163501339\n",
            "train loss:0.0002611066026959809\n",
            "train loss:0.000771044196103453\n",
            "train loss:0.0005207338407009759\n",
            "train loss:0.0006404701084208844\n",
            "train loss:0.00019245995510280093\n",
            "train loss:0.005530300364222311\n",
            "train loss:0.0015656655765319437\n",
            "train loss:0.0008944176269686878\n",
            "train loss:4.440185420291059e-05\n",
            "train loss:0.000792842076752156\n",
            "train loss:0.0033081539791427693\n",
            "train loss:0.000506156103420216\n",
            "train loss:0.00037854991704314416\n",
            "train loss:0.001742913541513626\n",
            "train loss:7.144861753340423e-05\n",
            "train loss:0.0010001460141572272\n",
            "train loss:0.0023820986803224507\n",
            "train loss:0.0003020233229050288\n",
            "train loss:0.00039062665738200537\n",
            "train loss:0.0012423966093164094\n",
            "train loss:0.0008063911276491433\n",
            "train loss:0.0029687916886595735\n",
            "train loss:0.0017521628992084837\n",
            "train loss:0.0028332180078581392\n",
            "train loss:5.3605819672876854e-05\n",
            "train loss:0.00017361108627679636\n",
            "train loss:0.0006868817884233488\n",
            "train loss:0.0004931638051622563\n",
            "train loss:0.001124283372316297\n",
            "train loss:0.0008565654969584152\n",
            "train loss:0.0009467739096435371\n",
            "train loss:4.531492716978916e-05\n",
            "=== epoch:18, train acc:0.998, test acc:0.988 ===\n",
            "train loss:0.0073146699781283\n",
            "train loss:0.0008621126705204832\n",
            "train loss:0.004028828945194854\n",
            "train loss:0.0025459380690686802\n",
            "train loss:0.0019689389900212156\n",
            "train loss:0.0004985935004847581\n",
            "train loss:0.00027503782710193675\n",
            "train loss:0.002212717259626867\n",
            "train loss:0.0009157081365378517\n",
            "train loss:0.0021014659844121293\n",
            "train loss:0.0012734187757935094\n",
            "train loss:0.0009711908409193688\n",
            "train loss:0.0019166039118109865\n",
            "train loss:0.0011126913953638886\n",
            "train loss:0.000773975427039877\n",
            "train loss:0.0012646984542105278\n",
            "train loss:0.000406193305631849\n",
            "train loss:0.0009254118984906228\n",
            "train loss:0.000367604954721727\n",
            "train loss:0.0020518530322294285\n",
            "train loss:0.0003271641819950439\n",
            "train loss:0.0009913485340122437\n",
            "train loss:0.0009630046241852902\n",
            "train loss:0.0005937732390104503\n",
            "train loss:0.0017726888337527772\n",
            "train loss:6.664390444688804e-05\n",
            "train loss:5.354808805712439e-05\n",
            "train loss:3.345406455923467e-05\n",
            "train loss:0.0025463288605068273\n",
            "train loss:0.0024147434791463355\n",
            "train loss:0.0007953668565708387\n",
            "train loss:0.00344069513304324\n",
            "train loss:0.007912375057229202\n",
            "train loss:0.0019140704489286414\n",
            "train loss:0.002122687286668912\n",
            "train loss:0.003262950938061464\n",
            "train loss:0.0036202932261574948\n",
            "train loss:0.03998917487726458\n",
            "train loss:0.0014353607902224206\n",
            "train loss:0.00506403637924083\n",
            "train loss:0.002641445099354089\n",
            "train loss:0.0021673638406574046\n",
            "train loss:0.0006210837277717968\n",
            "train loss:0.0014676932475737308\n",
            "train loss:0.0010790721125230318\n",
            "train loss:0.0004987478802999023\n",
            "train loss:0.001784103908603396\n",
            "train loss:0.0048692570684309745\n",
            "train loss:0.0005897672865490864\n",
            "train loss:0.0027815735976597076\n",
            "train loss:0.0003042083095438548\n",
            "train loss:0.0031890830895707615\n",
            "train loss:0.00211540583002992\n",
            "train loss:0.0004255400730177388\n",
            "train loss:0.006599750243160294\n",
            "train loss:0.0027300743978616974\n",
            "train loss:0.0004949252970404645\n",
            "train loss:0.0006846538254006697\n",
            "train loss:0.0029040568174593957\n",
            "train loss:0.0006692026594747314\n",
            "train loss:0.00039820748755394446\n",
            "train loss:0.00021125997625315256\n",
            "train loss:0.00011678127381487275\n",
            "train loss:0.0006832152940954067\n",
            "train loss:0.0005227831801240506\n",
            "train loss:0.021097631248087147\n",
            "train loss:6.0212435300591084e-05\n",
            "train loss:0.00020561520738976612\n",
            "train loss:1.6684410917697694e-05\n",
            "train loss:0.0004498159426098707\n",
            "train loss:0.00032326911027498136\n",
            "train loss:0.0003759780760524748\n",
            "train loss:0.00039373068932282854\n",
            "train loss:0.002890101741427782\n",
            "train loss:0.0010313417960405003\n",
            "train loss:0.00047985871620271416\n",
            "train loss:0.00013911040807686658\n",
            "train loss:0.00048353421468011264\n",
            "train loss:0.0034931799085170657\n",
            "train loss:4.849527387460751e-05\n",
            "train loss:0.002606120176909832\n",
            "train loss:0.038015402117763414\n",
            "train loss:0.00012093473437608028\n",
            "train loss:0.0004302759913045956\n",
            "train loss:0.00350371810142638\n",
            "train loss:0.0017644237381097658\n",
            "train loss:5.171599419717983e-05\n",
            "train loss:0.005543442378465123\n",
            "train loss:0.0010083384195154427\n",
            "train loss:0.00019121078832028615\n",
            "train loss:0.00291062231356801\n",
            "train loss:0.00021983275791070283\n",
            "train loss:0.0008978523028618775\n",
            "train loss:0.000387298567398975\n",
            "train loss:0.0007014790507115867\n",
            "train loss:0.0008697648812084556\n",
            "train loss:5.153979928761536e-05\n",
            "train loss:0.0035236492026832125\n",
            "train loss:0.0013710124933188717\n",
            "train loss:0.012321029352310647\n",
            "train loss:0.002487240024850505\n",
            "train loss:0.004699937613718837\n",
            "train loss:0.005099002478254573\n",
            "train loss:0.0013791068242547647\n",
            "train loss:0.00031130124337382163\n",
            "train loss:0.005803779402400456\n",
            "train loss:0.005101591535862815\n",
            "train loss:0.005225656329116507\n",
            "train loss:0.00020891533262172978\n",
            "train loss:0.000548022421518708\n",
            "train loss:0.00044696511476288795\n",
            "train loss:0.001300974245573662\n",
            "train loss:0.0037856363224482957\n",
            "train loss:0.0018464433463473006\n",
            "train loss:0.0017945905037828008\n",
            "train loss:0.00034243973355728647\n",
            "train loss:0.002112188225121664\n",
            "train loss:0.0008899806547721632\n",
            "train loss:0.0004044391407107229\n",
            "train loss:0.003743291348131825\n",
            "train loss:0.002313093002662266\n",
            "train loss:0.00026868519606625666\n",
            "train loss:0.0008806139013747409\n",
            "train loss:0.00020702943650112166\n",
            "train loss:0.001474315709547516\n",
            "train loss:0.02361529301725209\n",
            "train loss:0.0015775507761467478\n",
            "train loss:0.001175447768878537\n",
            "train loss:9.276241500297371e-05\n",
            "train loss:0.000358682269878229\n",
            "train loss:0.0029100497317608525\n",
            "train loss:0.0010299119147934642\n",
            "train loss:0.0001258353185983306\n",
            "train loss:0.0005623976889991346\n",
            "train loss:0.00025257798395249384\n",
            "train loss:0.0007121439240063271\n",
            "train loss:0.005258231233141797\n",
            "train loss:0.0018544332162181967\n",
            "train loss:0.004121169739403477\n",
            "train loss:0.0008190144610797634\n",
            "train loss:0.0009606683314648584\n",
            "train loss:0.000492471486901606\n",
            "train loss:0.0013655536967028409\n",
            "train loss:0.002691946331516424\n",
            "train loss:0.0016843288886769137\n",
            "train loss:0.0012233202113405828\n",
            "train loss:0.00020383512761995828\n",
            "train loss:0.0005253527221657484\n",
            "train loss:0.0008492940104270589\n",
            "train loss:0.001077531197656408\n",
            "train loss:6.925741616216243e-05\n",
            "train loss:0.0027688197106236173\n",
            "train loss:0.0005069598094240594\n",
            "train loss:0.00041225288799673533\n",
            "train loss:0.00022706707061790225\n",
            "train loss:0.0002311436671689598\n",
            "train loss:0.002255604904189121\n",
            "train loss:0.0006786032168700654\n",
            "train loss:4.702929973235725e-05\n",
            "train loss:0.0006486986799160344\n",
            "train loss:0.005825005379311457\n",
            "train loss:0.00025938943088683197\n",
            "train loss:0.0004448530009836832\n",
            "train loss:0.0001491270115733164\n",
            "train loss:0.0013847770831597087\n",
            "train loss:0.0033502800100041626\n",
            "train loss:0.0015897964853115976\n",
            "train loss:0.00033744962235206645\n",
            "train loss:0.00870485337960759\n",
            "train loss:0.0005010141102624848\n",
            "train loss:0.0008059763075141248\n",
            "train loss:0.0011442406746168053\n",
            "train loss:0.001243104023586182\n",
            "train loss:0.0020079075109103385\n",
            "train loss:0.003634950363005398\n",
            "train loss:0.005123846233838317\n",
            "train loss:0.0060999228243361235\n",
            "train loss:0.0006873741970115932\n",
            "train loss:0.0017239523399253476\n",
            "train loss:0.004606355329185546\n",
            "train loss:0.00197763121856117\n",
            "train loss:0.0005737839611124104\n",
            "train loss:0.005443006612847301\n",
            "train loss:0.0005879142372535801\n",
            "train loss:0.004658977662963325\n",
            "train loss:0.000875944454325769\n",
            "train loss:0.0012644113500171663\n",
            "train loss:0.0023754861180510117\n",
            "train loss:0.008875006163487503\n",
            "train loss:0.0049761079674420705\n",
            "train loss:0.0004900102717618083\n",
            "train loss:0.0025428036791052696\n",
            "train loss:0.0003631730608012954\n",
            "train loss:0.0004116628171221772\n",
            "train loss:0.0021551335882817667\n",
            "train loss:0.000985685005672945\n",
            "train loss:0.0007804757053365554\n",
            "train loss:0.0006830757203972561\n",
            "train loss:0.002886879908362662\n",
            "train loss:0.006274776535674472\n",
            "train loss:0.0003874735771409702\n",
            "train loss:0.000997969737878685\n",
            "train loss:0.0005589360445422544\n",
            "train loss:0.0033744738111269397\n",
            "train loss:0.001263387889546437\n",
            "train loss:0.00013277296387158495\n",
            "train loss:7.277996417566033e-05\n",
            "train loss:0.0004633903416900853\n",
            "train loss:9.927346898967818e-05\n",
            "train loss:0.0025347891145788308\n",
            "train loss:0.0011739655222764737\n",
            "train loss:0.000906167160737735\n",
            "train loss:0.00016760398773494527\n",
            "train loss:0.0014015333122839894\n",
            "train loss:0.0038625303344928773\n",
            "train loss:0.0009372733162613417\n",
            "train loss:0.0011540088251279397\n",
            "train loss:0.0009745434588857634\n",
            "train loss:0.0004506694401406704\n",
            "train loss:0.0009810681919570404\n",
            "train loss:0.0016261537072850504\n",
            "train loss:0.002410980068225654\n",
            "train loss:0.0007203751992290318\n",
            "train loss:0.0031850084803740136\n",
            "train loss:0.00027025932863466464\n",
            "train loss:0.026930960823528305\n",
            "train loss:0.0028215816030451152\n",
            "train loss:0.00034893725190988565\n",
            "train loss:0.0015967336434715184\n",
            "train loss:0.00970205807746746\n",
            "train loss:0.0009431648996990814\n",
            "train loss:0.00015149920919794844\n",
            "train loss:0.004659838429485519\n",
            "train loss:0.00045480011000683925\n",
            "train loss:0.000564345087061115\n",
            "train loss:0.001590438179953081\n",
            "train loss:0.001957081559422289\n",
            "train loss:6.491932323037982e-05\n",
            "train loss:0.00019901332062843388\n",
            "train loss:0.000519209390791366\n",
            "train loss:0.0021408733544803853\n",
            "train loss:0.0028910079411901923\n",
            "train loss:0.0019643279718974884\n",
            "train loss:0.000562532598446822\n",
            "train loss:0.0015758787180679553\n",
            "train loss:0.005723084118997741\n",
            "train loss:0.0022845929437306633\n",
            "train loss:0.0007165896729708974\n",
            "train loss:0.0004162466292202068\n",
            "train loss:0.0012101508670981967\n",
            "train loss:0.0001788903041851965\n",
            "train loss:0.0013492429955496306\n",
            "train loss:0.0003897866723787248\n",
            "train loss:0.0021072747953601208\n",
            "train loss:0.0022371294922115185\n",
            "train loss:0.0003223617232231978\n",
            "train loss:0.00016782268503322349\n",
            "train loss:5.249076976494853e-05\n",
            "train loss:0.0010779094423767246\n",
            "train loss:0.00749760062623085\n",
            "train loss:0.0001637289430636024\n",
            "train loss:0.004133933825288861\n",
            "train loss:6.915650447402054e-05\n",
            "train loss:0.0004599179668211328\n",
            "train loss:0.0005507293772762552\n",
            "train loss:0.0006343618324302855\n",
            "train loss:0.0016157610094787293\n",
            "train loss:0.0009173438675019162\n",
            "train loss:0.0006339870417885234\n",
            "train loss:0.00031884484385629434\n",
            "train loss:0.0008268935428370027\n",
            "train loss:0.0007280968410472605\n",
            "train loss:0.015013840161061108\n",
            "train loss:0.0012549576430828581\n",
            "train loss:0.002869708464219784\n",
            "train loss:0.005647815637721936\n",
            "train loss:0.0015946180335240706\n",
            "train loss:0.002603555474150533\n",
            "train loss:0.0012118479026954757\n",
            "train loss:0.020930701532411354\n",
            "train loss:0.003431775438332241\n",
            "train loss:0.0007559719164133977\n",
            "train loss:0.0002590266537650648\n",
            "train loss:0.00012002919868379173\n",
            "train loss:0.028640388322012465\n",
            "train loss:0.00025120654281387633\n",
            "train loss:0.0024288779471939278\n",
            "train loss:0.0013419412052571697\n",
            "train loss:0.0002787124620719551\n",
            "train loss:0.0009205977525328018\n",
            "train loss:0.0018111786849606846\n",
            "train loss:0.00042105327116890386\n",
            "train loss:0.020206499760397553\n",
            "train loss:0.00042626195431338005\n",
            "train loss:0.0001861591848525518\n",
            "train loss:0.0004547980954628701\n",
            "train loss:0.0010894049538841726\n",
            "train loss:0.0009380355008316542\n",
            "train loss:0.001127042271650737\n",
            "train loss:0.0006616442868417357\n",
            "train loss:5.647119470603499e-05\n",
            "train loss:0.00014803757115980053\n",
            "train loss:0.00347570501780278\n",
            "train loss:0.00010370176485254106\n",
            "train loss:0.0026044704418063925\n",
            "train loss:0.0006072554967022528\n",
            "train loss:0.0005336954269694807\n",
            "train loss:0.0008830062025492836\n",
            "train loss:8.71462969132635e-05\n",
            "train loss:0.0018670630799894586\n",
            "train loss:0.000878675267333908\n",
            "train loss:0.0018500440643459267\n",
            "train loss:7.978437789121988e-05\n",
            "train loss:0.006678510257022173\n",
            "train loss:0.0015064730302325273\n",
            "train loss:0.0026895558064391383\n",
            "train loss:0.00627969989554858\n",
            "train loss:0.0014259052787822274\n",
            "train loss:0.006319221781170026\n",
            "train loss:0.0002113069159920414\n",
            "train loss:0.00038927216373761203\n",
            "train loss:0.020245314702588217\n",
            "train loss:0.002260222106724774\n",
            "train loss:0.0013682564345963585\n",
            "train loss:0.0011088846653389771\n",
            "train loss:0.006222243843931241\n",
            "train loss:7.359733471912429e-05\n",
            "train loss:0.0013282995686381437\n",
            "train loss:0.00014477857277130716\n",
            "train loss:0.005110164279094395\n",
            "train loss:0.0004624210769988664\n",
            "train loss:0.0004673027406634663\n",
            "train loss:0.000877673458167484\n",
            "train loss:0.0010271564135441674\n",
            "train loss:0.00014240051464843025\n",
            "train loss:0.00015860785468467433\n",
            "train loss:0.002022543872517615\n",
            "train loss:0.0007303675426595637\n",
            "train loss:7.180433814473622e-05\n",
            "train loss:0.011338634889522203\n",
            "train loss:0.0026286891169667215\n",
            "train loss:0.00026270641199058615\n",
            "train loss:0.002063807363930983\n",
            "train loss:0.0016413611735971085\n",
            "train loss:0.003158708692641723\n",
            "train loss:0.001655348399774939\n",
            "train loss:0.003945997096053082\n",
            "train loss:0.0012091859227237212\n",
            "train loss:0.0020701077622134584\n",
            "train loss:0.0017505633088949488\n",
            "train loss:0.00489731024047483\n",
            "train loss:0.0014290148066436314\n",
            "train loss:0.0015571461771067322\n",
            "train loss:0.00031642451206228715\n",
            "train loss:0.0010595535740009123\n",
            "train loss:0.0004683099347078791\n",
            "train loss:0.00023507811032747616\n",
            "train loss:0.00020498304644131954\n",
            "train loss:0.0018924957300041892\n",
            "train loss:0.00047318347661877706\n",
            "train loss:0.0013339380646544018\n",
            "train loss:0.000580622348438046\n",
            "train loss:0.03458256355165328\n",
            "train loss:0.009035038212255481\n",
            "train loss:0.0030064093435641704\n",
            "train loss:0.003050347993246702\n",
            "train loss:0.0005118336830566503\n",
            "train loss:0.005065423237386905\n",
            "train loss:0.015080252899448768\n",
            "train loss:0.0004687871059234446\n",
            "train loss:0.0012548910862397774\n",
            "train loss:0.0031251197744336257\n",
            "train loss:0.0007909762475240591\n",
            "train loss:0.0005843111345847119\n",
            "train loss:0.000733161290388723\n",
            "train loss:0.005117609759407581\n",
            "train loss:0.006336818984178975\n",
            "train loss:0.0012012606572271173\n",
            "train loss:0.0346928324367672\n",
            "train loss:0.001960250666852979\n",
            "train loss:0.004295447151194291\n",
            "train loss:0.004990786868757511\n",
            "train loss:0.003736498860493572\n",
            "train loss:0.0017584824210609216\n",
            "train loss:0.004767180569292948\n",
            "train loss:0.009084159709603103\n",
            "train loss:0.006914464779731173\n",
            "train loss:0.0025095203964892593\n",
            "train loss:0.003062613490266492\n",
            "train loss:0.0012579192428500319\n",
            "train loss:0.002674266893472652\n",
            "train loss:0.0014419817213236865\n",
            "train loss:0.002201165882594894\n",
            "train loss:0.0023040831943026713\n",
            "train loss:0.010848246187443386\n",
            "train loss:0.003651054141395341\n",
            "train loss:0.00922122469129188\n",
            "train loss:0.0007918145355595117\n",
            "train loss:0.0010835710314653234\n",
            "train loss:0.0012694911221325892\n",
            "train loss:9.941415024756529e-05\n",
            "train loss:0.0005225466496755597\n",
            "train loss:0.0003135159426921863\n",
            "train loss:0.0017388679033411027\n",
            "train loss:0.0005901316948104908\n",
            "train loss:0.007776353532939497\n",
            "train loss:0.0019358293547330691\n",
            "train loss:0.00038009718393837086\n",
            "train loss:0.00037927889924834374\n",
            "train loss:0.0008835793361923477\n",
            "train loss:0.0023085158377827045\n",
            "train loss:0.0009966761894579109\n",
            "train loss:7.61397395421819e-05\n",
            "train loss:0.0022616970987654225\n",
            "train loss:0.0012288554242004556\n",
            "train loss:0.005986900112965137\n",
            "train loss:0.00425176417355364\n",
            "train loss:0.0005602053194005298\n",
            "train loss:0.0007007299180451723\n",
            "train loss:0.0002642047627318571\n",
            "train loss:0.003116912627228522\n",
            "train loss:0.0003683536100846575\n",
            "train loss:0.002601636580733575\n",
            "train loss:0.0003913754921865656\n",
            "train loss:0.005486910414833638\n",
            "train loss:0.00022988089723143578\n",
            "train loss:0.0014586755541985382\n",
            "train loss:0.0052063987912134594\n",
            "train loss:0.00112959330560035\n",
            "train loss:0.0008091776972823754\n",
            "train loss:0.004153303046552211\n",
            "train loss:0.001862493582065126\n",
            "train loss:0.012636731414520772\n",
            "train loss:0.020070958211660873\n",
            "train loss:0.0010138537173638173\n",
            "train loss:0.0024507032696296487\n",
            "train loss:0.0058838163367059375\n",
            "train loss:0.009268340862378375\n",
            "train loss:0.01339224677271507\n",
            "train loss:0.0003415511991687097\n",
            "train loss:0.002293443179989038\n",
            "train loss:0.006975519090975119\n",
            "train loss:0.0007450791526283849\n",
            "train loss:8.895009320124916e-05\n",
            "train loss:0.002209519656547989\n",
            "train loss:0.00110348766822297\n",
            "train loss:0.0018261742605818463\n",
            "train loss:0.002656154838541455\n",
            "train loss:0.0008247013733915952\n",
            "train loss:0.0035555525963294914\n",
            "train loss:0.00020957162536229274\n",
            "train loss:0.013517772872721497\n",
            "train loss:0.0015161969510529656\n",
            "train loss:0.016153367225424157\n",
            "train loss:0.0020915745910508486\n",
            "train loss:0.00228521328346941\n",
            "train loss:0.003838758264892161\n",
            "train loss:0.0009923134075355041\n",
            "train loss:0.0006254093800882627\n",
            "train loss:0.00013779909544510313\n",
            "train loss:0.0028195004716079376\n",
            "train loss:0.00013521396626842416\n",
            "train loss:0.0007580094564970588\n",
            "train loss:0.027291815929269995\n",
            "train loss:0.0006271004464966949\n",
            "train loss:0.00029665343078111176\n",
            "train loss:0.0020001459946419942\n",
            "train loss:0.002659460418063082\n",
            "train loss:0.0006629778829865302\n",
            "train loss:0.005566406328681751\n",
            "train loss:0.0018373851840119632\n",
            "train loss:0.004143171115261651\n",
            "train loss:0.0020305434586719512\n",
            "train loss:0.0007268777029365586\n",
            "train loss:0.0012759750414094242\n",
            "train loss:0.000738565450705453\n",
            "train loss:0.0011508743140310933\n",
            "train loss:0.0003641520147303371\n",
            "train loss:0.0004350963077652925\n",
            "train loss:0.004570330363237206\n",
            "train loss:0.0002561279971301646\n",
            "train loss:0.009429827308251988\n",
            "train loss:0.002852314572993059\n",
            "train loss:0.010033594350891881\n",
            "train loss:0.002891408639751408\n",
            "train loss:0.002487371383309325\n",
            "train loss:0.004709367548225259\n",
            "train loss:0.0027341494157717\n",
            "train loss:0.0017971999133651897\n",
            "train loss:0.0011633410978841346\n",
            "train loss:0.0003626180239997496\n",
            "train loss:0.0007612692853934712\n",
            "train loss:0.001069912789807491\n",
            "train loss:0.04888259584736076\n",
            "train loss:0.0014445860076595977\n",
            "train loss:0.002917599769348622\n",
            "train loss:0.0015831286173588228\n",
            "train loss:0.0019100955168611434\n",
            "train loss:0.0006159961552750339\n",
            "train loss:0.0005553091994086753\n",
            "train loss:0.0053652800092365005\n",
            "train loss:0.0001539901898754401\n",
            "train loss:0.0036282385531271034\n",
            "train loss:0.0005952169196465082\n",
            "train loss:0.00338560799530745\n",
            "train loss:0.0027799878752761166\n",
            "train loss:0.009588684011681755\n",
            "train loss:0.0071469110454906795\n",
            "train loss:0.0024174107548477057\n",
            "train loss:0.003277310524778798\n",
            "train loss:0.00022669565620998485\n",
            "train loss:0.0007695990812353498\n",
            "train loss:0.0014113570571728995\n",
            "train loss:0.0009176976037818623\n",
            "train loss:0.02490923160887358\n",
            "train loss:0.00514882688722763\n",
            "train loss:0.00015651282602749964\n",
            "train loss:0.000761493917168835\n",
            "train loss:0.0037800354644827\n",
            "train loss:0.00277320840220205\n",
            "train loss:0.0009456672342415337\n",
            "train loss:0.0021683480726494165\n",
            "train loss:0.0021537387669912284\n",
            "train loss:0.005088010454569395\n",
            "train loss:0.00010073631481629951\n",
            "train loss:0.0021928580459433435\n",
            "train loss:0.0018619278355622051\n",
            "train loss:0.00037804981798005276\n",
            "train loss:0.0012241828242616293\n",
            "train loss:0.0009963735354723653\n",
            "train loss:0.00034054089557849186\n",
            "train loss:0.0005041230883930007\n",
            "train loss:0.0011793813390828154\n",
            "train loss:0.00092497790898233\n",
            "train loss:0.006371443178826142\n",
            "train loss:8.078362052769835e-05\n",
            "train loss:0.0015573420831153661\n",
            "train loss:0.0010666575558644085\n",
            "train loss:0.0010203593140010521\n",
            "train loss:0.002091635519097725\n",
            "train loss:0.0005214264540342326\n",
            "train loss:0.0005977565228549992\n",
            "train loss:0.0012980862055756177\n",
            "train loss:0.001444761555879129\n",
            "train loss:0.00020085452822190145\n",
            "train loss:0.005257039925144607\n",
            "train loss:6.805943593722816e-05\n",
            "train loss:0.004818345544805109\n",
            "train loss:0.0007794647629928657\n",
            "train loss:0.0006044152999514882\n",
            "train loss:8.583803430521724e-05\n",
            "train loss:0.008645701129419987\n",
            "train loss:0.003375692525954119\n",
            "train loss:0.0009662141727069476\n",
            "train loss:0.0003783222214290164\n",
            "train loss:0.009186669513410873\n",
            "train loss:0.0024071741456213676\n",
            "train loss:0.00011121270720691954\n",
            "train loss:0.0009814507118211411\n",
            "train loss:0.0030987839013298614\n",
            "train loss:0.004443512770125189\n",
            "train loss:0.0014390779559218427\n",
            "train loss:0.0006643160991462539\n",
            "train loss:0.00799252973112731\n",
            "train loss:0.0009006177192159981\n",
            "train loss:0.0010302557415044133\n",
            "train loss:0.0021567392520165686\n",
            "train loss:0.000427563689783043\n",
            "train loss:0.007152572368444607\n",
            "train loss:0.00014211978451075235\n",
            "train loss:0.0067743794730495\n",
            "train loss:0.0009006182112530873\n",
            "train loss:0.0038324214240777472\n",
            "train loss:0.00046063618063485885\n",
            "train loss:0.002548047152329644\n",
            "train loss:0.0004102160191375176\n",
            "train loss:0.0009034901117840952\n",
            "train loss:0.0005173632229530645\n",
            "train loss:0.0055262216740753324\n",
            "train loss:0.0015412265034672162\n",
            "train loss:0.0004596843864362282\n",
            "train loss:0.009621449071678742\n",
            "train loss:6.7982724610753e-05\n",
            "train loss:0.0008722330257318558\n",
            "train loss:0.0005562710541017542\n",
            "train loss:0.0040374477381328\n",
            "train loss:0.004093880591881376\n",
            "train loss:0.001092062635044974\n",
            "train loss:0.0008774714798183679\n",
            "train loss:0.0022381066042288598\n",
            "train loss:9.010407929897478e-05\n",
            "train loss:0.004198408230347409\n",
            "train loss:0.0002612035099346395\n",
            "train loss:0.004734122856713706\n",
            "train loss:0.009989702812412795\n",
            "train loss:0.0006431251510189645\n",
            "train loss:0.0002480335093768662\n",
            "train loss:0.0026588797188446716\n",
            "train loss:0.0043794879447562\n",
            "train loss:0.0002788119415165507\n",
            "=== epoch:19, train acc:0.999, test acc:0.985 ===\n",
            "train loss:4.949212654010803e-05\n",
            "train loss:0.000812278133183627\n",
            "train loss:0.00034731433420809657\n",
            "train loss:0.0005032236146772351\n",
            "train loss:0.017509244632749475\n",
            "train loss:0.0015056284341968926\n",
            "train loss:0.0008770539678976799\n",
            "train loss:0.0009474047204004013\n",
            "train loss:7.349644887313578e-05\n",
            "train loss:0.00162356940699731\n",
            "train loss:0.0005313812890725174\n",
            "train loss:0.00032999370344457803\n",
            "train loss:0.0002976133649690651\n",
            "train loss:0.0009255240023707509\n",
            "train loss:0.011384714110285975\n",
            "train loss:0.0003023625751161541\n",
            "train loss:0.013969596060899916\n",
            "train loss:0.005627815174823112\n",
            "train loss:0.0007247781582741291\n",
            "train loss:0.00019628355389418665\n",
            "train loss:0.0031081796854476474\n",
            "train loss:0.0005890530489644324\n",
            "train loss:0.0006454225130472359\n",
            "train loss:0.0024014278223343137\n",
            "train loss:0.002465519465616165\n",
            "train loss:0.0032123843520736957\n",
            "train loss:0.000362798800662629\n",
            "train loss:5.333342015459591e-05\n",
            "train loss:0.0026394206967496163\n",
            "train loss:0.00016592525583516705\n",
            "train loss:0.0022169357577060097\n",
            "train loss:0.0014628376709676925\n",
            "train loss:0.00041500212552830144\n",
            "train loss:0.00022364573134568786\n",
            "train loss:0.00011009000358089868\n",
            "train loss:0.001421065788248465\n",
            "train loss:8.862324029185696e-05\n",
            "train loss:0.00037207483820529956\n",
            "train loss:0.0008480691095198853\n",
            "train loss:0.003156058014549393\n",
            "train loss:0.0001494354631505036\n",
            "train loss:0.0006268199729424363\n",
            "train loss:0.001907906132543238\n",
            "train loss:0.0019292384009370895\n",
            "train loss:0.0011672068198723476\n",
            "train loss:0.0016737201361449482\n",
            "train loss:0.0003800500929804035\n",
            "train loss:0.004469592001211774\n",
            "train loss:0.0010639006539752043\n",
            "train loss:0.0003173748187862048\n",
            "train loss:0.0006896332373465521\n",
            "train loss:0.0010264163631026764\n",
            "train loss:0.0001168753875096897\n",
            "train loss:0.0023369829060859047\n",
            "train loss:0.00047914676538978227\n",
            "train loss:0.00012119619691649247\n",
            "train loss:0.0013862590610970794\n",
            "train loss:0.000836897878275946\n",
            "train loss:0.0033989895573313246\n",
            "train loss:0.00031578156671793875\n",
            "train loss:0.002116736065239911\n",
            "train loss:0.004008537507416019\n",
            "train loss:0.0013911885326543887\n",
            "train loss:0.0021883902432691645\n",
            "train loss:0.0006022343019732267\n",
            "train loss:0.000994601842587971\n",
            "train loss:0.00968425010677286\n",
            "train loss:0.0010610089050136836\n",
            "train loss:0.00016348704560016392\n",
            "train loss:0.00046351316469017695\n",
            "train loss:0.0018308651069206857\n",
            "train loss:0.0004432253525546906\n",
            "train loss:0.0012232269043127638\n",
            "train loss:0.0024431914429425827\n",
            "train loss:0.00033118688756770084\n",
            "train loss:0.0010286056467703783\n",
            "train loss:0.007736227480541215\n",
            "train loss:0.0001093710170007207\n",
            "train loss:2.2960095414983796e-05\n",
            "train loss:0.0003753196770568683\n",
            "train loss:0.0019593255114186345\n",
            "train loss:0.0024596622416209113\n",
            "train loss:0.0002828630660386773\n",
            "train loss:0.0010856933373934118\n",
            "train loss:0.0005699057006299292\n",
            "train loss:0.001283144575986015\n",
            "train loss:0.00017301097508435636\n",
            "train loss:0.0012136665127322108\n",
            "train loss:0.00033222612700461193\n",
            "train loss:0.0005785071752324722\n",
            "train loss:0.0013432664943341375\n",
            "train loss:0.00025238982944938445\n",
            "train loss:0.003860658542000243\n",
            "train loss:0.007820708568017172\n",
            "train loss:3.547902295264037e-05\n",
            "train loss:0.000981908000237338\n",
            "train loss:0.0002381169292065219\n",
            "train loss:6.742554736838373e-05\n",
            "train loss:0.0002709351408132927\n",
            "train loss:0.002513665849138902\n",
            "train loss:0.0026357816449783444\n",
            "train loss:0.0006812874090709856\n",
            "train loss:7.552896861546596e-05\n",
            "train loss:0.0015414257412832756\n",
            "train loss:0.0027500408632466227\n",
            "train loss:0.0020340681414043695\n",
            "train loss:0.006498331100149071\n",
            "train loss:0.00112563236264534\n",
            "train loss:0.0007444001709685912\n",
            "train loss:0.0027247602925663038\n",
            "train loss:0.0013231742905136306\n",
            "train loss:0.0005763249352730073\n",
            "train loss:0.00482971905051219\n",
            "train loss:0.00041827359645321986\n",
            "train loss:0.0018851872353286491\n",
            "train loss:0.001840150719937725\n",
            "train loss:0.00034115605979574844\n",
            "train loss:0.0025827437394766277\n",
            "train loss:0.0002563144466584125\n",
            "train loss:0.00016521661853756466\n",
            "train loss:4.314650092360624e-05\n",
            "train loss:0.0004452706149767019\n",
            "train loss:0.004738875254243281\n",
            "train loss:0.00031522262965503567\n",
            "train loss:0.0003678793288879657\n",
            "train loss:0.0006655005670834172\n",
            "train loss:0.00025904302586659807\n",
            "train loss:0.0009871834567091108\n",
            "train loss:0.00020901228327207969\n",
            "train loss:0.006682647493677655\n",
            "train loss:0.0003146890036749065\n",
            "train loss:0.00028901376072940554\n",
            "train loss:0.0020669247881373418\n",
            "train loss:0.0005422889841127488\n",
            "train loss:0.0015236064409930076\n",
            "train loss:0.0003815580433274321\n",
            "train loss:3.698728166239874e-05\n",
            "train loss:0.002012232147535569\n",
            "train loss:0.0018282712573966303\n",
            "train loss:0.0028632805433133402\n",
            "train loss:8.956619797409578e-05\n",
            "train loss:0.0014651922835806682\n",
            "train loss:4.4006310965297774e-05\n",
            "train loss:0.0016408751656136699\n",
            "train loss:0.0005478555591039975\n",
            "train loss:0.0024082770963340613\n",
            "train loss:0.000548812577466595\n",
            "train loss:0.0007382500660483522\n",
            "train loss:0.0003876111899762934\n",
            "train loss:0.0032556190812643836\n",
            "train loss:0.002237010335400053\n",
            "train loss:0.000995661971497354\n",
            "train loss:0.0001719337332502297\n",
            "train loss:0.0017171253967021376\n",
            "train loss:0.0003182308021630803\n",
            "train loss:0.0005068613830575286\n",
            "train loss:5.39526676064719e-05\n",
            "train loss:0.0011931817201452425\n",
            "train loss:0.0017936159382711942\n",
            "train loss:0.0006298117058087391\n",
            "train loss:0.000395051847781886\n",
            "train loss:0.000681359065979221\n",
            "train loss:9.018815174912117e-05\n",
            "train loss:3.766546824663753e-05\n",
            "train loss:0.0004015166938879921\n",
            "train loss:3.593568125287498e-05\n",
            "train loss:6.743356190105673e-05\n",
            "train loss:0.003747149024571907\n",
            "train loss:0.0010073234693900688\n",
            "train loss:0.0023203682763950993\n",
            "train loss:0.0006240776346277565\n",
            "train loss:0.0027644083970577394\n",
            "train loss:0.0011060534763371668\n",
            "train loss:0.000720298260480685\n",
            "train loss:0.001451563717361238\n",
            "train loss:0.00011551341758132201\n",
            "train loss:0.00021571621045375747\n",
            "train loss:0.00011094046883448086\n",
            "train loss:0.00020266587820142728\n",
            "train loss:0.0013907852902634704\n",
            "train loss:0.00011388716587369007\n",
            "train loss:0.00018981180138992387\n",
            "train loss:0.0007801707300208771\n",
            "train loss:1.9783500363953518e-05\n",
            "train loss:0.0014078090073453592\n",
            "train loss:0.0033014136294800623\n",
            "train loss:0.0005596223965612306\n",
            "train loss:0.000573849074942362\n",
            "train loss:0.0048572398503743865\n",
            "train loss:0.0003321656654896365\n",
            "train loss:0.0010154535765742132\n",
            "train loss:0.0014385612587355168\n",
            "train loss:0.0006448377379415096\n",
            "train loss:0.0017919281238409238\n",
            "train loss:0.0032582029388158653\n",
            "train loss:0.0006011906485275286\n",
            "train loss:0.0016812481787866118\n",
            "train loss:9.265122836389719e-05\n",
            "train loss:0.0004892469883696306\n",
            "train loss:0.0033833342899530923\n",
            "train loss:0.0020804959630493914\n",
            "train loss:0.00043602403102463856\n",
            "train loss:0.0007729301707264911\n",
            "train loss:0.0006260787768320691\n",
            "train loss:0.0003920649568317938\n",
            "train loss:0.001849640265354742\n",
            "train loss:0.0007928732046929225\n",
            "train loss:0.0003186830521535953\n",
            "train loss:0.0008174458892199131\n",
            "train loss:0.0008397138174941493\n",
            "train loss:0.0018633555716556407\n",
            "train loss:0.0008749547073167709\n",
            "train loss:0.0024685634024805016\n",
            "train loss:5.8413573742671894e-05\n",
            "train loss:0.00045952596625022096\n",
            "train loss:0.0020687543320960958\n",
            "train loss:0.004283296166490718\n",
            "train loss:0.001520069604091971\n",
            "train loss:0.0015937752770655358\n",
            "train loss:0.0037629343293139354\n",
            "train loss:0.00017179200105655934\n",
            "train loss:0.00013927214077892909\n",
            "train loss:0.0033151218643726107\n",
            "train loss:5.688158052385162e-05\n",
            "train loss:0.0011329157055273446\n",
            "train loss:0.0002075052778422468\n",
            "train loss:0.00020376799731384066\n",
            "train loss:0.0004096769674159318\n",
            "train loss:0.0020006559075523627\n",
            "train loss:0.0029070723730886612\n",
            "train loss:0.001510965121034498\n",
            "train loss:0.00013281369328498307\n",
            "train loss:0.0007177349856167619\n",
            "train loss:0.0018244896890489792\n",
            "train loss:3.1476922959007976e-05\n",
            "train loss:0.00166766766610287\n",
            "train loss:0.00016356702987217902\n",
            "train loss:0.0007601990223068293\n",
            "train loss:0.004345958465016436\n",
            "train loss:0.0077297550300277915\n",
            "train loss:0.002641934793475556\n",
            "train loss:0.0025502723533980753\n",
            "train loss:0.0009951982479945393\n",
            "train loss:0.0012278092891953717\n",
            "train loss:0.0007303555534256662\n",
            "train loss:0.005741819311387259\n",
            "train loss:0.004700082397760807\n",
            "train loss:0.0006965953872185245\n",
            "train loss:0.0008318418555192702\n",
            "train loss:0.024461374140253643\n",
            "train loss:0.0017250980646335862\n",
            "train loss:0.00026077920653815566\n",
            "train loss:0.000614172394160607\n",
            "train loss:0.00042389930402141665\n",
            "train loss:0.0006255357914362525\n",
            "train loss:0.00017469520348096593\n",
            "train loss:0.0004881381799992319\n",
            "train loss:0.0005093379973615106\n",
            "train loss:5.1569308281346286e-05\n",
            "train loss:0.0005349714964273388\n",
            "train loss:0.00337655414665155\n",
            "train loss:0.0006771371270379032\n",
            "train loss:0.0041926028202288\n",
            "train loss:0.002801943369051008\n",
            "train loss:0.004286035422276461\n",
            "train loss:0.0008073798460099475\n",
            "train loss:0.004488425708949171\n",
            "train loss:0.0395613701680858\n",
            "train loss:0.0002390985982769882\n",
            "train loss:0.0001620999033394651\n",
            "train loss:0.001634351776571724\n",
            "train loss:0.002774262622269452\n",
            "train loss:0.00018729894586130838\n",
            "train loss:0.0011049831973511453\n",
            "train loss:0.00043490025701185256\n",
            "train loss:0.03544158422736877\n",
            "train loss:0.009878274218235967\n",
            "train loss:7.530504101148132e-05\n",
            "train loss:0.002882206459388256\n",
            "train loss:0.0009789984105480958\n",
            "train loss:0.0004445521815027918\n",
            "train loss:0.00028489583342924013\n",
            "train loss:0.0025461567368180267\n",
            "train loss:0.0010657778701373263\n",
            "train loss:0.0001269465003610853\n",
            "train loss:0.00015572056264831223\n",
            "train loss:0.00012958339094467234\n",
            "train loss:0.0005013570779457008\n",
            "train loss:0.00011487502884655476\n",
            "train loss:0.002888926779621705\n",
            "train loss:0.011796284894584943\n",
            "train loss:0.00012006546752215689\n",
            "train loss:0.0002763969102928759\n",
            "train loss:0.0006254543216815186\n",
            "train loss:0.0012159810490623873\n",
            "train loss:0.0011409957221344678\n",
            "train loss:0.000204340010390952\n",
            "train loss:0.00027671314997534176\n",
            "train loss:0.00046629731984406937\n",
            "train loss:0.004173394372101907\n",
            "train loss:0.014276880121477492\n",
            "train loss:0.003235895871846398\n",
            "train loss:0.00014638798052064644\n",
            "train loss:0.00046414953121073877\n",
            "train loss:0.0009664428829522867\n",
            "train loss:0.0006301534180103693\n",
            "train loss:0.005802216518432872\n",
            "train loss:0.003844660517738425\n",
            "train loss:0.0025156384149769033\n",
            "train loss:0.002350367821775125\n",
            "train loss:0.0020226349027434843\n",
            "train loss:0.0009484907150294025\n",
            "train loss:0.0012773002556344362\n",
            "train loss:0.0014731774229312163\n",
            "train loss:0.0013229331787375839\n",
            "train loss:0.0014982584135591632\n",
            "train loss:0.0011939288517274294\n",
            "train loss:0.0008199575578536133\n",
            "train loss:0.0003525095777657362\n",
            "train loss:0.0011418054769523874\n",
            "train loss:3.177706331496658e-05\n",
            "train loss:0.00017769292555235152\n",
            "train loss:0.0012369520555072417\n",
            "train loss:0.0002290638091216372\n",
            "train loss:0.00011368307912895818\n",
            "train loss:0.0015761731322989192\n",
            "train loss:0.00012730941957313536\n",
            "train loss:0.00020400529176241498\n",
            "train loss:0.001325842376351119\n",
            "train loss:0.0008356938334219127\n",
            "train loss:0.00027088859220799954\n",
            "train loss:0.0020308672853459444\n",
            "train loss:0.0001025646171143594\n",
            "train loss:0.0004413210731482459\n",
            "train loss:0.0014366962876621206\n",
            "train loss:0.0008783650748283507\n",
            "train loss:0.00036843324385454445\n",
            "train loss:0.0039610967876119625\n",
            "train loss:4.76161564548245e-05\n",
            "train loss:0.0015865427193481685\n",
            "train loss:9.378885911033808e-05\n",
            "train loss:0.00418985077968339\n",
            "train loss:0.0007137893704765847\n",
            "train loss:0.0006478419832610726\n",
            "train loss:0.002951894021468522\n",
            "train loss:2.8164888568220576e-05\n",
            "train loss:0.00013787942236827562\n",
            "train loss:0.0003824879923096242\n",
            "train loss:0.0004094963273594425\n",
            "train loss:0.0002824789295224627\n",
            "train loss:0.00024551468824718993\n",
            "train loss:5.2105264893218834e-05\n",
            "train loss:0.0005579110145685166\n",
            "train loss:0.0017741666720071647\n",
            "train loss:0.0003482520878864045\n",
            "train loss:0.00015143912889090312\n",
            "train loss:0.00041243301218514246\n",
            "train loss:0.002531599203305734\n",
            "train loss:0.0008231200446706503\n",
            "train loss:0.0030933113244360866\n",
            "train loss:0.0038498596559245163\n",
            "train loss:0.0014333952370782987\n",
            "train loss:0.001313945015222163\n",
            "train loss:0.0009687130577979043\n",
            "train loss:0.00029880377407498216\n",
            "train loss:0.00031637579771115666\n",
            "train loss:7.518398055115025e-05\n",
            "train loss:6.990510030627974e-05\n",
            "train loss:0.0008956283022683298\n",
            "train loss:0.0001534169383648466\n",
            "train loss:9.076022425844987e-05\n",
            "train loss:0.0007350057578943664\n",
            "train loss:0.0006533804947438213\n",
            "train loss:0.0018353085330960342\n",
            "train loss:0.0016401656111126855\n",
            "train loss:0.0020521976982541246\n",
            "train loss:0.0018111320277242552\n",
            "train loss:0.0008327293771222116\n",
            "train loss:0.0003644566351669952\n",
            "train loss:0.0007231328891596084\n",
            "train loss:0.0014261292753055574\n",
            "train loss:0.00010396457289689702\n",
            "train loss:0.006472641626190802\n",
            "train loss:0.00027069347275208\n",
            "train loss:0.003303929305258661\n",
            "train loss:0.0019622315735808027\n",
            "train loss:0.00010443234824340418\n",
            "train loss:0.0027313456710226125\n",
            "train loss:0.0001630188355783126\n",
            "train loss:0.0011355264375664301\n",
            "train loss:0.0009651424342444968\n",
            "train loss:0.0012592029141152668\n",
            "train loss:0.0031499256105952177\n",
            "train loss:0.0006820948143350541\n",
            "train loss:2.7860509790126895e-05\n",
            "train loss:0.00021047829411687295\n",
            "train loss:0.007265424154167882\n",
            "train loss:0.0029078386178003155\n",
            "train loss:0.0001848302354246001\n",
            "train loss:0.0009534116570277173\n",
            "train loss:0.002083769895147544\n",
            "train loss:0.0010841057166868352\n",
            "train loss:0.002165155786274335\n",
            "train loss:0.0018634403287571\n",
            "train loss:0.0036699591890566436\n",
            "train loss:0.00035241681998060006\n",
            "train loss:6.398874509355902e-05\n",
            "train loss:0.0028731578041016636\n",
            "train loss:0.00012678054746927627\n",
            "train loss:0.00012413246804869865\n",
            "train loss:0.000747191582315952\n",
            "train loss:0.002271803995852675\n",
            "train loss:0.0063344624922324275\n",
            "train loss:2.1740807416590034e-05\n",
            "train loss:0.028913141873171747\n",
            "train loss:0.0004560603295057206\n",
            "train loss:0.000584729708036435\n",
            "train loss:0.0005266538710593853\n",
            "train loss:0.00010204017464889275\n",
            "train loss:0.0023238748656585987\n",
            "train loss:0.0011022908288520553\n",
            "train loss:0.005961309148753439\n",
            "train loss:0.008246575748250598\n",
            "train loss:0.0002757484051365234\n",
            "train loss:0.0005936156133608213\n",
            "train loss:0.00046493230467218655\n",
            "train loss:0.0005396397065233052\n",
            "train loss:0.0017235488599020073\n",
            "train loss:0.003129087081409489\n",
            "train loss:0.001013019942013699\n",
            "train loss:0.00020984385467202497\n",
            "train loss:0.00023899555656627905\n",
            "train loss:0.0002259784166274792\n",
            "train loss:0.0033168792117709007\n",
            "train loss:0.0016863667156930386\n",
            "train loss:0.0024661764546307446\n",
            "train loss:0.0003438191578639387\n",
            "train loss:0.0026588938417756213\n",
            "train loss:9.32643136956372e-05\n",
            "train loss:1.4509347479791419e-05\n",
            "train loss:0.00018782126963127818\n",
            "train loss:0.0010311914514082774\n",
            "train loss:0.0012926688350182645\n",
            "train loss:0.0009271588863918708\n",
            "train loss:8.97958509561596e-05\n",
            "train loss:0.00013402564840149606\n",
            "train loss:0.0136429131069328\n",
            "train loss:0.0006839404384359183\n",
            "train loss:0.00038335606356628793\n",
            "train loss:0.0005431781523694739\n",
            "train loss:5.948777631699701e-05\n",
            "train loss:0.006914478658472535\n",
            "train loss:0.0021046572909222804\n",
            "train loss:0.0003571623404877336\n",
            "train loss:0.0027317640911593056\n",
            "train loss:8.960326683517081e-05\n",
            "train loss:7.46854277358738e-05\n",
            "train loss:0.0028012458280102896\n",
            "train loss:0.0005645474071051063\n",
            "train loss:0.0015248935262312474\n",
            "train loss:0.00048495513745864334\n",
            "train loss:0.0005133608286179206\n",
            "train loss:1.5923584769652477e-05\n",
            "train loss:0.0006557552817092844\n",
            "train loss:0.006218103674023295\n",
            "train loss:6.475810602184451e-05\n",
            "train loss:0.00164039161069315\n",
            "train loss:0.00013148772869295015\n",
            "train loss:0.0016413440679395432\n",
            "train loss:0.001654006050911006\n",
            "train loss:0.000839348313549689\n",
            "train loss:0.0005142234689600687\n",
            "train loss:0.006890482406253194\n",
            "train loss:0.0019315999938674727\n",
            "train loss:0.001333825954939368\n",
            "train loss:0.0028755189096614188\n",
            "train loss:0.0010264116709191597\n",
            "train loss:0.0006790277339788343\n",
            "train loss:0.00019543837251848075\n",
            "train loss:0.020523778108199223\n",
            "train loss:0.0038698191867244745\n",
            "train loss:0.0005150231721113771\n",
            "train loss:0.0006798124115860872\n",
            "train loss:0.0005769270192291177\n",
            "train loss:0.0005160954791723497\n",
            "train loss:0.006119314324543838\n",
            "train loss:0.0008891818074033038\n",
            "train loss:0.004299606490553078\n",
            "train loss:8.947751321755217e-05\n",
            "train loss:0.0016526299027092705\n",
            "train loss:0.0004646804242669144\n",
            "train loss:0.0019350482961979735\n",
            "train loss:0.00048570500166743617\n",
            "train loss:0.0019734136698924716\n",
            "train loss:0.002355742514678087\n",
            "train loss:0.0008284493804540064\n",
            "train loss:0.0004292276851641963\n",
            "train loss:0.0002559950351129421\n",
            "train loss:0.00014100487493640557\n",
            "train loss:0.0008626159914612515\n",
            "train loss:0.003010667466972556\n",
            "train loss:0.0009864703949428082\n",
            "train loss:0.005668740190296803\n",
            "train loss:0.0008660385255616612\n",
            "train loss:0.0004645502284132338\n",
            "train loss:0.0014862896889764202\n",
            "train loss:0.0004881637184702022\n",
            "train loss:0.00019875929848669517\n",
            "train loss:0.0005096966044836914\n",
            "train loss:7.085583341473497e-05\n",
            "train loss:8.747636653167688e-05\n",
            "train loss:0.002123787695653236\n",
            "train loss:0.002981228539920039\n",
            "train loss:0.0009958575066839124\n",
            "train loss:2.6768043061207075e-05\n",
            "train loss:0.00045387603736413655\n",
            "train loss:0.009548324307409616\n",
            "train loss:0.0016532749935719807\n",
            "train loss:7.024972081323268e-05\n",
            "train loss:0.00047184333536682123\n",
            "train loss:0.0013782657050793754\n",
            "train loss:0.0017685708073003157\n",
            "train loss:0.0016317206282305825\n",
            "train loss:0.001777066803745376\n",
            "train loss:0.0008979485827508244\n",
            "train loss:0.0005769790860646098\n",
            "train loss:0.0010494244665534514\n",
            "train loss:0.0005701766249387397\n",
            "train loss:0.0009313637037611347\n",
            "train loss:0.0009303078338408271\n",
            "train loss:0.00014380963672557003\n",
            "train loss:3.319241362398232e-05\n",
            "train loss:0.0030721798331078748\n",
            "train loss:0.0008273834148340209\n",
            "train loss:0.037386106800150004\n",
            "train loss:0.00023805877126018612\n",
            "train loss:0.0001579535274357644\n",
            "train loss:0.0039688319748889134\n",
            "train loss:0.0067291337281529815\n",
            "train loss:8.269619760700488e-05\n",
            "train loss:0.0004340000988709819\n",
            "train loss:0.0005364759077040825\n",
            "train loss:0.0027717105031790455\n",
            "train loss:0.000397071933283702\n",
            "train loss:7.414277866881299e-05\n",
            "train loss:0.0001818196217365144\n",
            "train loss:0.0015482279815076732\n",
            "train loss:0.0035470036509094357\n",
            "train loss:5.2291861375986134e-05\n",
            "train loss:0.0003976583951273726\n",
            "train loss:0.0008649746007648615\n",
            "train loss:0.0006340466407193171\n",
            "train loss:0.0003866278416072874\n",
            "train loss:0.002551909337661937\n",
            "train loss:0.002311007270115246\n",
            "train loss:0.0011071465755723437\n",
            "train loss:0.00012060320792401172\n",
            "train loss:0.0012162638378046772\n",
            "train loss:0.00022084497317158579\n",
            "train loss:0.0005943709768860162\n",
            "train loss:0.0009903894493801082\n",
            "train loss:0.00023762183049043001\n",
            "train loss:0.0008081988411829527\n",
            "train loss:0.009095781622592055\n",
            "train loss:0.0010554344287747664\n",
            "train loss:0.00018667488325246106\n",
            "train loss:0.0003309832870204879\n",
            "train loss:0.00389094135673388\n",
            "train loss:0.0013006506643041826\n",
            "train loss:0.001605983721632931\n",
            "train loss:0.0433708494529004\n",
            "train loss:0.0035748270741642646\n",
            "train loss:0.00037879637932415383\n",
            "train loss:0.003117523666136327\n",
            "train loss:0.0003633948794883643\n",
            "train loss:0.003836402091536849\n",
            "train loss:0.0023601646690986143\n",
            "train loss:0.0017206677497154576\n",
            "train loss:0.010197487538958161\n",
            "train loss:0.00033825172938173407\n",
            "train loss:0.0012702670028094986\n",
            "train loss:0.002648718996464332\n",
            "train loss:0.0011938173537623619\n",
            "train loss:0.000611466239780686\n",
            "train loss:9.156846906866432e-05\n",
            "train loss:0.0009667246658433322\n",
            "train loss:1.78960520743549e-05\n",
            "train loss:0.00043061253353593134\n",
            "train loss:0.0032661452084633506\n",
            "train loss:0.00031306225690000156\n",
            "train loss:8.519752615430752e-05\n",
            "train loss:0.0014660131314776486\n",
            "train loss:0.0003988971997198959\n",
            "train loss:0.00034483519948627444\n",
            "train loss:0.0024783080970375514\n",
            "train loss:0.0008073328858854152\n",
            "train loss:9.54846016239423e-05\n",
            "train loss:7.956850228174206e-06\n",
            "train loss:6.201633148831157e-05\n",
            "train loss:0.0003274938008254953\n",
            "=== epoch:20, train acc:0.999, test acc:0.987 ===\n",
            "train loss:0.0004834991409850613\n",
            "train loss:0.0003891120678924338\n",
            "train loss:0.0010479327543219785\n",
            "train loss:0.001423671960303238\n",
            "train loss:0.0004827983841663761\n",
            "train loss:3.598343866018202e-05\n",
            "train loss:0.0031954191925830816\n",
            "train loss:0.0005550207313407436\n",
            "train loss:1.0213366586879957e-05\n",
            "train loss:8.447245639405684e-06\n",
            "train loss:9.465296565715958e-05\n",
            "train loss:0.0009062843345283895\n",
            "train loss:0.0018835807333503532\n",
            "train loss:0.0020977243125750223\n",
            "train loss:0.001511160016551336\n",
            "train loss:0.00044231745892244563\n",
            "train loss:0.0014707090501400782\n",
            "train loss:0.0018774068343625897\n",
            "train loss:0.0008886767217750853\n",
            "train loss:0.004104673567290303\n",
            "train loss:0.0011274081360602712\n",
            "train loss:0.00021477072147031218\n",
            "train loss:0.0013069302557676618\n",
            "train loss:0.0008490371209813098\n",
            "train loss:0.0023771409651732077\n",
            "train loss:0.00020595491935480753\n",
            "train loss:0.0007219112877606217\n",
            "train loss:1.9264052586500527e-05\n",
            "train loss:0.015901718363960517\n",
            "train loss:0.0005282375887962381\n",
            "train loss:0.00022358653985674604\n",
            "train loss:0.00023830750688133166\n",
            "train loss:0.0018678692676182035\n",
            "train loss:0.0043017663573295265\n",
            "train loss:0.001473826303617479\n",
            "train loss:0.0002990749559348442\n",
            "train loss:0.0031256886144452305\n",
            "train loss:0.00010636364322412156\n",
            "train loss:0.003104872408072445\n",
            "train loss:7.818056238167942e-05\n",
            "train loss:6.772346358177792e-06\n",
            "train loss:0.0010069130317278672\n",
            "train loss:0.0007997229532608326\n",
            "train loss:0.0008909263790352042\n",
            "train loss:0.000519000791284851\n",
            "train loss:0.0006062092349043935\n",
            "train loss:0.0020334880645344593\n",
            "train loss:0.0021940753019334554\n",
            "train loss:0.0010157857281272622\n",
            "train loss:0.0012804764469573717\n",
            "train loss:8.452350608227639e-05\n",
            "train loss:0.0004934254877941943\n",
            "train loss:0.0005599439797676285\n",
            "train loss:0.0026488501862339973\n",
            "train loss:0.0009175244269846128\n",
            "train loss:0.00014383094318223052\n",
            "train loss:0.0004590651955621147\n",
            "train loss:5.637276696025005e-05\n",
            "train loss:9.634837289012987e-05\n",
            "train loss:0.0014672886518051276\n",
            "train loss:0.0015357628479720437\n",
            "train loss:0.0007227548955516593\n",
            "train loss:0.00029445314561750934\n",
            "train loss:0.0001038178996755177\n",
            "train loss:0.000331199945407592\n",
            "train loss:0.0013019420212750008\n",
            "train loss:0.0024003896910696015\n",
            "train loss:0.00011383244963453326\n",
            "train loss:0.00042818939451296574\n",
            "train loss:1.0369022840387998e-05\n",
            "train loss:0.00015409707362238815\n",
            "train loss:0.0005496271244343072\n",
            "train loss:0.00022891247830955159\n",
            "train loss:0.00036942639086060144\n",
            "train loss:0.00034554955863687684\n",
            "train loss:0.00047196294869173637\n",
            "train loss:0.0001697536932124406\n",
            "train loss:0.00029068249924915103\n",
            "train loss:0.00048435298803819327\n",
            "train loss:0.00025802766228545155\n",
            "train loss:0.0024776836450685165\n",
            "train loss:0.0001230936553888599\n",
            "train loss:0.0014596963851468859\n",
            "train loss:0.0004923030706632075\n",
            "train loss:0.0007793125096665729\n",
            "train loss:0.002733683464960495\n",
            "train loss:0.0003786079705330246\n",
            "train loss:0.00030383761507279846\n",
            "train loss:0.0006715834617464272\n",
            "train loss:0.00021846973970827713\n",
            "train loss:0.0003392200537962103\n",
            "train loss:0.0005950627051629256\n",
            "train loss:0.00026721851800392985\n",
            "train loss:0.0021587935127812207\n",
            "train loss:0.0022649665975918876\n",
            "train loss:0.002307223798647331\n",
            "train loss:0.00024246356800311384\n",
            "train loss:0.0010692165188559172\n",
            "train loss:0.0006182361609195591\n",
            "train loss:4.43999462852236e-05\n",
            "train loss:0.00014204867788719262\n",
            "train loss:0.0009763113477803283\n",
            "train loss:0.00023344968328775797\n",
            "train loss:0.0029226500184022343\n",
            "train loss:0.000616949344326065\n",
            "train loss:0.00020052930447255133\n",
            "train loss:9.359519712998879e-05\n",
            "train loss:0.0022210197299057143\n",
            "train loss:4.829761767827955e-05\n",
            "train loss:0.0006691175185214608\n",
            "train loss:0.0006023855537603926\n",
            "train loss:0.0006215256345776249\n",
            "train loss:0.0003464339013418233\n",
            "train loss:0.0005988678298292211\n",
            "train loss:0.0012153306813291543\n",
            "train loss:0.00040376623840532804\n",
            "train loss:0.0016836399273433952\n",
            "train loss:0.0022885814133664652\n",
            "train loss:3.782481406388405e-06\n",
            "train loss:0.0025384690718886983\n",
            "train loss:0.010492668561407468\n",
            "train loss:0.007460927084199169\n",
            "train loss:0.0013618404487839271\n",
            "train loss:0.0006058214704001342\n",
            "train loss:0.0007551520482094037\n",
            "train loss:0.00021815566787941447\n",
            "train loss:0.0018453045327565156\n",
            "train loss:0.001356375089398252\n",
            "train loss:0.00019197391762182446\n",
            "train loss:0.0019253116780104376\n",
            "train loss:0.0003398472720900351\n",
            "train loss:0.0003631130852627832\n",
            "train loss:0.0014895431914741608\n",
            "train loss:0.0002775889221349303\n",
            "train loss:0.0005147840509614808\n",
            "train loss:0.000953815054809581\n",
            "train loss:0.0008742182450788937\n",
            "train loss:0.0014608287821492065\n",
            "train loss:7.797741340166097e-05\n",
            "train loss:0.0014369852868746735\n",
            "train loss:0.0006579786089618008\n",
            "train loss:0.0012293229696162436\n",
            "train loss:5.486533289131337e-05\n",
            "train loss:0.0006767288212570291\n",
            "train loss:0.000396837079826682\n",
            "train loss:0.0007941571911150611\n",
            "train loss:0.0029727102479575407\n",
            "train loss:0.0008363483863464744\n",
            "train loss:0.00040272364091081516\n",
            "train loss:1.0923571144441434e-05\n",
            "train loss:1.9616282023017197e-05\n",
            "train loss:0.0014287381914689092\n",
            "train loss:0.0019381329089374056\n",
            "train loss:0.0001466063537295828\n",
            "train loss:0.0010729514874021273\n",
            "train loss:0.0016685428940693158\n",
            "train loss:0.0019210788628693804\n",
            "train loss:0.0015851989549237548\n",
            "train loss:0.000329045140038066\n",
            "train loss:0.0014001237943465404\n",
            "train loss:5.0566124229314434e-05\n",
            "train loss:0.010175769840388331\n",
            "train loss:0.000593697288109332\n",
            "train loss:0.0026941613702626305\n",
            "train loss:0.00016984828291652235\n",
            "train loss:0.0032774738229730455\n",
            "train loss:6.152202713684657e-05\n",
            "train loss:0.0004376525291523834\n",
            "train loss:0.0006173547725442308\n",
            "train loss:0.00016734131093889542\n",
            "train loss:0.003412933631200395\n",
            "train loss:0.0007383371188481283\n",
            "train loss:0.0005159292803787299\n",
            "train loss:0.003936060054464291\n",
            "train loss:0.0016477583696862075\n",
            "train loss:0.0003579017838140315\n",
            "train loss:5.582389784954028e-05\n",
            "train loss:0.0015353856239111875\n",
            "train loss:6.901507022588295e-05\n",
            "train loss:0.00027740399389451607\n",
            "train loss:0.0022537601320333153\n",
            "train loss:0.0005985024868908695\n",
            "train loss:0.0031157760522633527\n",
            "train loss:0.0003218310256737407\n",
            "train loss:0.0010541461023775697\n",
            "train loss:0.001079489957051069\n",
            "train loss:0.0012441094445937704\n",
            "train loss:0.0003177601833903386\n",
            "train loss:0.0011293104904362035\n",
            "train loss:0.002221446176400445\n",
            "train loss:0.0006043501393958397\n",
            "train loss:0.0007681710259694944\n",
            "train loss:8.382727941416067e-05\n",
            "train loss:0.0022309526959170727\n",
            "train loss:0.0005690004148690625\n",
            "train loss:0.001170450067511687\n",
            "train loss:4.9214100759323824e-05\n",
            "train loss:0.005004209653842151\n",
            "train loss:0.0001458861296257801\n",
            "train loss:0.0015926733478479123\n",
            "train loss:0.00012185126224183527\n",
            "train loss:0.0022164614415307645\n",
            "train loss:0.00011326307691323396\n",
            "train loss:0.006191624851386015\n",
            "train loss:0.0019685037964474613\n",
            "train loss:0.00322901702163663\n",
            "train loss:0.0016770206451287406\n",
            "train loss:0.00011808177288073274\n",
            "train loss:0.0016115560628071277\n",
            "train loss:0.00022931653205799813\n",
            "train loss:0.00018270725522239045\n",
            "train loss:0.0025962752760459395\n",
            "train loss:0.0026017081183226094\n",
            "train loss:0.0010020607146736978\n",
            "train loss:0.0027408077754824314\n",
            "train loss:0.005103438442229918\n",
            "train loss:0.0031930533298688375\n",
            "train loss:7.153243439520331e-05\n",
            "train loss:0.002034570374614495\n",
            "train loss:0.0006485201313365285\n",
            "train loss:0.00012995277265595085\n",
            "train loss:0.001691789448688429\n",
            "train loss:0.00246523784177888\n",
            "train loss:6.075570963941534e-05\n",
            "train loss:0.0019931419463896285\n",
            "train loss:0.004779341011504994\n",
            "train loss:0.0009634043634536535\n",
            "train loss:0.012678832939171854\n",
            "train loss:0.00259788431371667\n",
            "train loss:0.001624708743570331\n",
            "train loss:0.0006825562611581466\n",
            "train loss:7.991212143240724e-05\n",
            "train loss:0.0036190177366716364\n",
            "train loss:8.514474470054874e-05\n",
            "train loss:0.0004477500816231643\n",
            "train loss:0.0007541976737674351\n",
            "train loss:0.0015698640010661413\n",
            "train loss:0.00010732063203275683\n",
            "train loss:0.0009434105189946373\n",
            "train loss:0.0003992791222065798\n",
            "train loss:0.0004082155700923218\n",
            "train loss:0.0003529610803652325\n",
            "train loss:0.0003281700533905887\n",
            "train loss:0.0024400212952974186\n",
            "train loss:0.004581157110479565\n",
            "train loss:0.0030986745577302214\n",
            "train loss:8.727544462890635e-05\n",
            "train loss:0.0011337034339919093\n",
            "train loss:1.044297926248543e-05\n",
            "train loss:0.0006690377357780166\n",
            "train loss:0.0025582441544970158\n",
            "train loss:0.0012772533039858045\n",
            "train loss:0.0003008121091659734\n",
            "train loss:0.00048091874451439045\n",
            "train loss:4.7803459109810387e-05\n",
            "train loss:0.0014148217503582897\n",
            "train loss:0.002207885534750567\n",
            "train loss:8.624075497221479e-05\n",
            "train loss:0.0033379214058313796\n",
            "train loss:8.005819838559719e-05\n",
            "train loss:4.044142349369231e-05\n",
            "train loss:0.001404958010247261\n",
            "train loss:9.115506430481822e-05\n",
            "train loss:0.0002619046707294081\n",
            "train loss:0.0002005286766935005\n",
            "train loss:0.0011371802046056744\n",
            "train loss:0.0014961490647329046\n",
            "train loss:0.0014194290109977089\n",
            "train loss:0.0005848210359081007\n",
            "train loss:0.0003672548430951745\n",
            "train loss:0.0030512906044175016\n",
            "train loss:0.0003473863901536687\n",
            "train loss:0.0005921045304006417\n",
            "train loss:0.0005117889016716629\n",
            "train loss:0.0013266089261739491\n",
            "train loss:1.8910588733779108e-05\n",
            "train loss:0.0007686714290248722\n",
            "train loss:0.0037846260620416584\n",
            "train loss:0.0007074491843600791\n",
            "train loss:0.0011036126477704411\n",
            "train loss:0.0007837965549157273\n",
            "train loss:0.002647424712145848\n",
            "train loss:0.0009561170908198376\n",
            "train loss:0.0024872348306689738\n",
            "train loss:0.0002275378386109835\n",
            "train loss:0.00032903499689779106\n",
            "train loss:0.0002812226216177234\n",
            "train loss:0.0010944874626324667\n",
            "train loss:0.0006639841299122808\n",
            "train loss:0.0001968477600914709\n",
            "train loss:0.001192517159100329\n",
            "train loss:4.9114261303659474e-05\n",
            "train loss:0.0001276771912039706\n",
            "train loss:0.0008893546684573067\n",
            "train loss:0.00021820536108277364\n",
            "train loss:0.0007237321456870793\n",
            "train loss:0.0006799677609675634\n",
            "train loss:0.0001751243198027985\n",
            "train loss:0.0014029405067711761\n",
            "train loss:0.000560983411195954\n",
            "train loss:0.0005972987997418115\n",
            "train loss:0.00032131484487962894\n",
            "train loss:0.00042555775609692895\n",
            "train loss:0.0005575895478610945\n",
            "train loss:0.002249420846606689\n",
            "train loss:0.0002980014302707416\n",
            "train loss:0.0023823106206806973\n",
            "train loss:0.0022489998003514715\n",
            "train loss:1.4555022809736029e-05\n",
            "train loss:9.185557354090644e-05\n",
            "train loss:0.00012501269777303556\n",
            "train loss:0.00028025278637141415\n",
            "train loss:0.0020360908114693235\n",
            "train loss:8.070498732293117e-05\n",
            "train loss:1.9932353371446055e-05\n",
            "train loss:0.00011825566371189969\n",
            "train loss:0.004095105834269753\n",
            "train loss:0.0018272311712702277\n",
            "train loss:0.0013462476773546917\n",
            "train loss:0.0004944348214859664\n",
            "train loss:0.0003939759482590266\n",
            "train loss:0.015601321071928874\n",
            "train loss:0.0014536981300111136\n",
            "train loss:4.075959261790034e-05\n",
            "train loss:0.0009112888730253893\n",
            "train loss:0.00469344662832913\n",
            "train loss:0.00014028935742024546\n",
            "train loss:0.00020275281978143764\n",
            "train loss:0.001371356652407183\n",
            "train loss:0.00025078722437769517\n",
            "train loss:0.0005289891170185804\n",
            "train loss:0.00030328747619911607\n",
            "train loss:4.9712959853249726e-05\n",
            "train loss:0.00025171590646377363\n",
            "train loss:0.0004109249530517437\n",
            "train loss:0.00042239608839023416\n",
            "train loss:0.00013241319722795132\n",
            "train loss:0.00189786021775019\n",
            "train loss:0.00012531579224574609\n",
            "train loss:0.0004205549062171891\n",
            "train loss:2.0355281406904138e-05\n",
            "train loss:0.00017438686663242973\n",
            "train loss:0.0011658077259182264\n",
            "train loss:0.002107894307387279\n",
            "train loss:0.00013781297192147466\n",
            "train loss:0.002214220846329156\n",
            "train loss:0.0027799732219314804\n",
            "train loss:0.0006298337529732975\n",
            "train loss:9.359613253373793e-05\n",
            "train loss:0.00014772425167006197\n",
            "train loss:0.0005709588087236574\n",
            "train loss:0.002771058803115034\n",
            "train loss:9.197212754973415e-05\n",
            "train loss:0.0002680153465015283\n",
            "train loss:0.0012840541781799378\n",
            "train loss:0.003818981829489093\n",
            "train loss:0.0003705676231787695\n",
            "train loss:0.0018363234023961968\n",
            "train loss:0.00015051217591911676\n",
            "train loss:8.980867695379363e-05\n",
            "train loss:0.0029513512543549976\n",
            "train loss:0.0013486181710561601\n",
            "train loss:0.0002526079496509423\n",
            "train loss:0.007918651732514742\n",
            "train loss:0.0007404714273970013\n",
            "train loss:0.00013108348396046542\n",
            "train loss:0.0006817791848248265\n",
            "train loss:0.0005986327379466182\n",
            "train loss:0.00016435605418332636\n",
            "train loss:0.00018495137636488754\n",
            "train loss:0.000103059400250093\n",
            "train loss:0.0001903872706102784\n",
            "train loss:0.001272203852968967\n",
            "train loss:3.782153080531854e-05\n",
            "train loss:0.000522727001753362\n",
            "train loss:9.729796777376816e-05\n",
            "train loss:0.0008563804043217041\n",
            "train loss:0.00013733255316997068\n",
            "train loss:0.0005556635566304612\n",
            "train loss:0.002223511130452384\n",
            "train loss:0.0006240220551144158\n",
            "train loss:0.009547869315885022\n",
            "train loss:0.0009280796449012727\n",
            "train loss:0.013906936323152113\n",
            "train loss:0.0022627917218478624\n",
            "train loss:0.0013726100274694941\n",
            "train loss:0.0022583753957639867\n",
            "train loss:0.0014498310584821427\n",
            "train loss:0.00040831769838633005\n",
            "train loss:0.0005304769862204517\n",
            "train loss:0.02739955891841227\n",
            "train loss:0.0021845985749249512\n",
            "train loss:0.0022835142770828844\n",
            "train loss:0.00038468135165750227\n",
            "train loss:7.036216439340636e-05\n",
            "train loss:0.0006625180507519582\n",
            "train loss:0.0018650472903050863\n",
            "train loss:0.00011454242283972729\n",
            "train loss:0.005849052483404964\n",
            "train loss:0.0006538870233477671\n",
            "train loss:0.001227118225029076\n",
            "train loss:0.0010558452071927473\n",
            "train loss:0.00022362709012811946\n",
            "train loss:0.0025971820553756498\n",
            "train loss:0.0012302859654124022\n",
            "train loss:0.011193871825553156\n",
            "train loss:0.0006762890018584375\n",
            "train loss:0.0007186622787687034\n",
            "train loss:0.0014713141058807122\n",
            "train loss:0.0015930112305406784\n",
            "train loss:0.0016367920616308504\n",
            "train loss:0.00013178909100096045\n",
            "train loss:0.020064538341271372\n",
            "train loss:0.0033528872003329995\n",
            "train loss:0.0005087376926624051\n",
            "train loss:0.0016155433154856574\n",
            "train loss:3.76348224348869e-05\n",
            "train loss:0.0013125867889013404\n",
            "train loss:0.0010030234114293088\n",
            "train loss:0.00019947335198553288\n",
            "train loss:0.0013075029975795902\n",
            "train loss:0.00017805163403256105\n",
            "train loss:0.0015109327025236655\n",
            "train loss:0.00038868247529209247\n",
            "train loss:0.0009839318872758305\n",
            "train loss:0.0006515020157858809\n",
            "train loss:0.0004248796909226879\n",
            "train loss:0.0018976826891192414\n",
            "train loss:0.0002005600270169045\n",
            "train loss:0.0020839016953572527\n",
            "train loss:0.0002996017464281295\n",
            "train loss:0.00045023624216996314\n",
            "train loss:0.0004867874900585958\n",
            "train loss:0.00020294127682362085\n",
            "train loss:0.0014098128357166136\n",
            "train loss:0.0004936614013982305\n",
            "train loss:0.003754593999274358\n",
            "train loss:0.0008705698301119432\n",
            "train loss:0.0029714534296630545\n",
            "train loss:0.0004828796343431093\n",
            "train loss:0.0018695204536075297\n",
            "train loss:0.0036734727684361256\n",
            "train loss:4.699978890732269e-05\n",
            "train loss:0.0010528687335302036\n",
            "train loss:8.330883716378539e-05\n",
            "train loss:0.0007962455494260304\n",
            "train loss:0.005649764426870878\n",
            "train loss:0.006666778928062575\n",
            "train loss:0.002712223101276925\n",
            "train loss:0.0021612780443779826\n",
            "train loss:0.0005725014433783657\n",
            "train loss:0.0027189963402310803\n",
            "train loss:0.0026193632424526088\n",
            "train loss:2.4843369761676826e-05\n",
            "train loss:0.00015006006586151447\n",
            "train loss:3.8887625201361204e-05\n",
            "train loss:0.004159577265624071\n",
            "train loss:0.00037649455506400176\n",
            "train loss:0.010331009423087255\n",
            "train loss:0.0006713354460193555\n",
            "train loss:0.0012448873672869947\n",
            "train loss:0.0003468578303455703\n",
            "train loss:0.00025725895247383863\n",
            "train loss:0.0005428768744786765\n",
            "train loss:0.0007014028347489499\n",
            "train loss:0.001068470055556896\n",
            "train loss:0.00010159871828702994\n",
            "train loss:0.0004715225334586671\n",
            "train loss:0.0016626139854569854\n",
            "train loss:7.34949052232642e-05\n",
            "train loss:3.5470826717346806e-05\n",
            "train loss:0.0011543985764496398\n",
            "train loss:0.002005800222376022\n",
            "train loss:0.0005765076004219336\n",
            "train loss:0.0005031801483501602\n",
            "train loss:2.5473041786940663e-05\n",
            "train loss:4.934148904146221e-05\n",
            "train loss:0.00801117494002423\n",
            "train loss:0.003568074600822632\n",
            "train loss:0.001314633640130796\n",
            "train loss:0.0009496034706538226\n",
            "train loss:7.494817971432433e-05\n",
            "train loss:0.0001201799056921651\n",
            "train loss:6.149918231725415e-05\n",
            "train loss:0.0014458538264701083\n",
            "train loss:0.002462980819452028\n",
            "train loss:0.00030361505644378663\n",
            "train loss:0.002391829215909637\n",
            "train loss:0.0008334139537917272\n",
            "train loss:7.833064176465885e-05\n",
            "train loss:0.0006507054514447556\n",
            "train loss:0.001735892327905669\n",
            "train loss:0.0029060826053565326\n",
            "train loss:0.0003970191063423166\n",
            "train loss:0.0002987925027309344\n",
            "train loss:0.002943524870112961\n",
            "train loss:9.800659452392648e-05\n",
            "train loss:2.2979446236175986e-05\n",
            "train loss:0.0006163926822728378\n",
            "train loss:0.0021309845834015266\n",
            "train loss:0.005123491890294242\n",
            "train loss:0.0012232696532717428\n",
            "train loss:6.962592385505189e-05\n",
            "train loss:0.0034244377680336517\n",
            "train loss:3.677959611601902e-05\n",
            "train loss:0.0010844467625983766\n",
            "train loss:0.0004632152776016356\n",
            "train loss:0.0013700316660765694\n",
            "train loss:0.0001616037186499068\n",
            "train loss:0.002136019533755867\n",
            "train loss:0.0013600387415507306\n",
            "train loss:0.00011835270368349004\n",
            "train loss:0.0003781561356124793\n",
            "train loss:0.0008600676182447339\n",
            "train loss:0.0009698055396933893\n",
            "train loss:0.007605896207607068\n",
            "train loss:0.0001918175479674788\n",
            "train loss:0.00038900944868268535\n",
            "train loss:0.0011213738543704404\n",
            "train loss:0.0002624657709338734\n",
            "train loss:4.9625387528750775e-05\n",
            "train loss:0.0003031068078231451\n",
            "train loss:0.008965262779797563\n",
            "train loss:0.0014090734735605257\n",
            "train loss:0.00043749795946872026\n",
            "train loss:6.580593910156627e-05\n",
            "train loss:0.0033622394111862287\n",
            "train loss:0.00030280588183980816\n",
            "train loss:0.00023753670250291247\n",
            "train loss:0.006388052771263964\n",
            "train loss:0.0002132521121146433\n",
            "train loss:0.002037993583014762\n",
            "train loss:0.00010242098647129125\n",
            "train loss:0.0018327130458177928\n",
            "train loss:0.0005101001100288996\n",
            "train loss:0.010372856491416716\n",
            "train loss:0.00013944816162236344\n",
            "train loss:0.00014987384248981716\n",
            "train loss:7.553091290386171e-05\n",
            "train loss:0.0014499635758303544\n",
            "train loss:0.0008201365255961928\n",
            "train loss:0.00036826493680215027\n",
            "train loss:0.0003490319730531434\n",
            "train loss:0.00085809765957524\n",
            "train loss:0.0009954874463487086\n",
            "train loss:0.0008973820044312585\n",
            "train loss:0.0012914404903479574\n",
            "train loss:0.0018868002802138165\n",
            "train loss:0.00022697203764457906\n",
            "train loss:1.0711004031201542e-05\n",
            "train loss:0.0008805220169204636\n",
            "train loss:0.0006844541582199064\n",
            "train loss:0.0012256330949330589\n",
            "train loss:0.0008616293126980736\n",
            "train loss:0.0006625628882045629\n",
            "train loss:0.03045020933209509\n",
            "train loss:0.00015024992769848347\n",
            "train loss:0.0020025285680987923\n",
            "train loss:0.0010465525309197089\n",
            "train loss:0.00011848314633513213\n",
            "train loss:8.671444027749371e-05\n",
            "train loss:0.00278873673818119\n",
            "train loss:0.0005481405204779387\n",
            "train loss:0.0018790395242602909\n",
            "train loss:0.00015120719991391682\n",
            "train loss:1.7017526050592982e-05\n",
            "train loss:0.0035362381593791975\n",
            "train loss:0.0009175238104968653\n",
            "train loss:0.0013967967484592627\n",
            "train loss:0.0006911461526701679\n",
            "train loss:7.744973857955477e-05\n",
            "train loss:0.004462921697342738\n",
            "train loss:0.002723224370507139\n",
            "train loss:0.00024292267775196354\n",
            "train loss:0.001782333578949202\n",
            "train loss:0.0008207874441534803\n",
            "train loss:0.0005725085296364472\n",
            "train loss:0.0006593056904342999\n",
            "train loss:0.015051447758997813\n",
            "train loss:0.0004344241824064367\n",
            "train loss:0.0027343111825958726\n",
            "train loss:0.0011144473462879262\n",
            "train loss:0.004666630601820163\n",
            "train loss:0.0002137390707566357\n",
            "train loss:0.0009731747030203153\n",
            "train loss:5.2887236786671e-05\n",
            "train loss:0.0024648395130162167\n",
            "train loss:0.0028154721639898194\n",
            "train loss:0.00045544152825369963\n",
            "train loss:0.0009922342637229448\n",
            "train loss:0.0045410019040072495\n",
            "train loss:0.003444061518299216\n",
            "train loss:5.469987439663611e-05\n",
            "train loss:0.00031962474233665\n",
            "train loss:0.0017144497992486265\n",
            "train loss:0.000764907066062359\n",
            "train loss:0.005576052921188291\n",
            "train loss:0.00039550151578088\n",
            "train loss:0.0032901598392928253\n",
            "=============== Final Test Accuracy ===============\n",
            "test acc:0.9877\n",
            "Saved Network Parameters!\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- 합성곱 계층과 풀링 계층은 이미지 인식에 필수적인 모듈이다\n",
        "\n",
        "## 7.6 CNN 시각화 하기"
      ],
      "metadata": {
        "id": "Z24ljvSql6ih"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# coding: utf-8\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def filter_show(filters, nx=8, margin=3, scale=10):\n",
        "    \"\"\"\n",
        "    c.f. https://gist.github.com/aidiary/07d530d5e08011832b12#file-draw_weight-py\n",
        "    \"\"\"\n",
        "    FN, C, FH, FW = filters.shape\n",
        "    ny = int(np.ceil(FN / nx))\n",
        "\n",
        "    fig = plt.figure()\n",
        "    fig.subplots_adjust(left=0, right=1, bottom=0, top=1, hspace=0.05, wspace=0.05)\n",
        "\n",
        "    for i in range(FN):\n",
        "        ax = fig.add_subplot(ny, nx, i+1, xticks=[], yticks=[])\n",
        "        ax.imshow(filters[i, 0], cmap=plt.cm.gray_r, interpolation='nearest')\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "network = SimpleConvNet()\n",
        "# 무작위(랜덤) 초기화 후의 가중치\n",
        "filter_show(network.params['W1'])\n",
        "\n",
        "# 학습된 가중치\n",
        "network.load_params(\"params.pkl\")\n",
        "filter_show(network.params['W1'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 937
        },
        "id": "gB4Xd9NEmKN9",
        "outputId": "5e4715b6-dcfb-4d4d-becc-1cd54d63f7ac"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 30 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 30 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 7.6.1 1번째 층의 가중치 시각화하기\n",
        "- 학습 전 필터는 무작위고 규칙성이 없었는데, 학습 후에는 규칙성있는 이미지가 되었다\n",
        "- 학습된 필터에서 필터1은 세로 에지에 필터2는 가로에지에 반응한다.\n",
        "- 이처럼 합성곱 계층의 필터는 에지나 블롭 등의 원시적인 정보를 추출할 수 있다. 이런 정보가 뒷단 계층에 절달되는 것이 앞서 구현한 CNN에서 일어나는 일이다.\n",
        "\n",
        "### 7.6.2 층 깊이에 따른 추출 정보 변화\n",
        "- 연구 결과에 따르면 계층이 깊어질수록 추출되는 정보는 더 추상화 된다.\n",
        "- 층이 깊어지면서 뉴런이 반응하는 대상이 단수한 모양에서 고급 정보로 변화되고, 의미를 이해하도록 변화 된다.\n",
        "\n",
        "## 7.7 대표적인 CNN\n",
        "### 7.7.1 LeNet\n",
        "- LeNet은 손글씨 숫자를 인식하는 네트워크이다.\n",
        "- 현재의 CNN과 비교하면 활성화 함수의 차이가 있다.\n",
        "\n",
        "### 7.7.2 AlextNet\n",
        "- 활성화 함수로 ReLU함수 사용\n",
        "- LRN이라는 국소적 정규화를 실시하는 계층 이용\n",
        "- 드롭아웃을 사용한다."
      ],
      "metadata": {
        "id": "1B28_PwGpcTZ"
      }
    }
  ]
}